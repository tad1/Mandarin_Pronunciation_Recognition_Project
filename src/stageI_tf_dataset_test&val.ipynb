{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:58:00.286450Z",
     "start_time": "2025-08-07T13:57:52.348755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "import src.models.SimplifiedLightweightCNN as model_module\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport src.models.SimplifiedLightweightCNN\n",
    "from data.source.pg_experiment import get_pg_experiment_dataset, AUDIO_PATH\n",
    "from models.SimpleCNN_v2 import PronunciationDataset, collate_fn, train, evaluate\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from path import RESULT_DIRECTORY\n",
    "import wandb\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ],
   "id": "1904a7bc680edc9e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:58:01.098558Z",
     "start_time": "2025-08-07T13:58:00.929957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_pron, _ = get_pg_experiment_dataset(\".wav\")\n",
    "\n",
    "# Word filtering\n",
    "# word = \"a0\"  # The first word - zero\n",
    "# df_pron = df_pron.filter(pl.col(\"word_id\").eq(word))\n",
    "\n",
    "df_stageI_polish = df_pron.filter(pl.col(\"word_id\").str.starts_with(\"a\") & pl.col(\"mother\")\n",
    "                                  .str.strip_chars()\n",
    "                                  .str.to_lowercase()\n",
    "                                  .is_in([\"polish\", \"polski\"]))\n",
    "\n",
    "df_other_lang = df_pron.filter(\n",
    "    pl.col(\"word_id\").str.starts_with(\"a\") &\n",
    "    (~pl.col(\"mother\")\n",
    "     .str.strip_chars()\n",
    "     .str.to_lowercase()\n",
    "     .is_in([\"polish\", \"polski\"]))\n",
    ")\n"
   ],
   "id": "8b5c1fb07b58069d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:58:01.117410Z",
     "start_time": "2025-08-07T13:58:01.108820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "np.shape(df_stageI_polish)"
   ],
   "id": "a4f11b9d8b57be9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5732, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "2ecc72f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:58:01.289132Z",
     "start_time": "2025-08-07T13:58:01.137329Z"
    }
   },
   "source": [
    "count_missing = 0\n",
    "rec_paths = df_other_lang.select(\"rec_path\").to_series()\n",
    "\n",
    "for path in rec_paths:\n",
    "    full_path = os.path.normpath(os.path.join(AUDIO_PATH, path))\n",
    "    if not os.path.exists(full_path):\n",
    "        count_missing += 1\n",
    "\n",
    "count_missing_pl = 0\n",
    "rec_paths = df_stageI_polish.select(\"rec_path\").to_series()\n",
    "\n",
    "for path in rec_paths:\n",
    "    full_path = os.path.normpath(os.path.join(AUDIO_PATH, path))\n",
    "    if not os.path.exists(full_path):\n",
    "        count_missing_pl += 1\n",
    "\n",
    "print(f\"Missing files in mother language Polish: {count_missing_pl}\")\n",
    "print(f\"Missing files in mother language other than Polish: {count_missing}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files in mother language Polish: 0\n",
      "Missing files in mother language other than Polish: 2\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "a57adc8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:58:01.325603Z",
     "start_time": "2025-08-07T13:58:01.309571Z"
    }
   },
   "source": [
    "# Find indices of rows with missing files\n",
    "rec_paths = df_other_lang.select(\"rec_path\").to_series()\n",
    "missing_indices = []\n",
    "\n",
    "for i, path in enumerate(rec_paths):\n",
    "    full_path = os.path.normpath(os.path.join(AUDIO_PATH, path))\n",
    "    if not os.path.exists(full_path):\n",
    "        missing_indices.append(i)\n",
    "\n",
    "# Print the deleted rows\n",
    "deleted_rows = df_other_lang[missing_indices]\n",
    "print(deleted_rows)\n",
    "\n",
    "df_other_lang = df_other_lang.filter(\n",
    "    ~pl.Series(range(len(df_other_lang))).is_in(pl.Series(missing_indices).implode())\n",
    ")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 7)\n",
      "┌────────────┬───────┬─────────┬───────────────────┬───────────┬────────┬─────────┐\n",
      "│ id_student ┆ value ┆ word_id ┆ rec_path          ┆ univ      ┆ gender ┆ mother  │\n",
      "│ ---        ┆ ---   ┆ ---     ┆ ---               ┆ ---       ┆ ---    ┆ ---     │\n",
      "│ i64        ┆ i64   ┆ str     ┆ str               ┆ str       ┆ str    ┆ str     │\n",
      "╞════════════╪═══════╪═════════╪═══════════════════╪═══════════╪════════╪═════════╡\n",
      "│ 757        ┆ 1     ┆ a0      ┆ stageI\\757\\a0.wav ┆ CLES_UMK2 ┆ f      ┆ chinese │\n",
      "│ 757        ┆ 1     ┆ a1      ┆ stageI\\757\\a1.wav ┆ CLES_UMK2 ┆ f      ┆ chinese │\n",
      "└────────────┴───────┴─────────┴───────────────────┴───────────┴────────┴─────────┘\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "4a95dcb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:58:01.355326Z",
     "start_time": "2025-08-07T13:58:01.344257Z"
    }
   },
   "source": [
    "import polars as pl\n",
    "\n",
    "train_split = 0.6\n",
    "val_split = 0.2\n",
    "test_split = 1 - train_split - val_split\n",
    "\n",
    "\n",
    "def stratified_split(df: pl.DataFrame, label_col: str, train_frac=train_split, val_frac=val_split, seed=42):\n",
    "    classes = df.select(label_col).unique().to_series()\n",
    "    train_rows, val_rows, test_rows = [], [], []\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    for cls in classes:\n",
    "        class_df = df.filter(pl.col(label_col) == cls)\n",
    "        n = class_df.height\n",
    "        indices = rng.permutation(n)\n",
    "\n",
    "        train_end = int(train_frac * n)\n",
    "        val_end = int((train_frac + val_frac) * n)\n",
    "\n",
    "        train_rows.append(class_df[indices[:train_end]])\n",
    "        val_rows.append(class_df[indices[train_end:val_end]])\n",
    "        test_rows.append(class_df[indices[val_end:]])\n",
    "\n",
    "    train_df = pl.concat(train_rows)\n",
    "    val_df = pl.concat(val_rows)\n",
    "    test_df = pl.concat(test_rows)\n",
    "\n",
    "    return train_df, val_df, test_df\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "37258aa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:58:01.376641Z",
     "start_time": "2025-08-07T13:58:01.368016Z"
    }
   },
   "source": [
    "# dataset = PronunciationDataset(df_stageI_polish, base_dir=AUDIO_PATH)\n",
    "dataset_other = PronunciationDataset(df_other_lang, base_dir=AUDIO_PATH)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "fd331ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:58:01.400467Z",
     "start_time": "2025-08-07T13:58:01.388874Z"
    }
   },
   "source": [
    "train_pl, val_pl, test_pl = stratified_split(df_stageI_polish, label_col=\"value\", train_frac=0.6, val_frac=0.2)\n",
    "\n",
    "dataset_train = PronunciationDataset(train_pl, base_dir=AUDIO_PATH)\n",
    "dataset_val = PronunciationDataset(val_pl, base_dir=AUDIO_PATH)\n",
    "dataset_test = PronunciationDataset(test_pl, base_dir=AUDIO_PATH)\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "bf890d68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:58:53.224312Z",
     "start_time": "2025-08-07T13:58:01.413373Z"
    }
   },
   "source": [
    "from pytorch_dataloader import MemoryLoadedDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=8)\n",
    "val_loader = DataLoader(dataset_val, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=8)\n",
    "test_loader = DataLoader(dataset_test, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=8)\n",
    "\n",
    "train_loader = MemoryLoadedDataLoader(train_loader, device=device)\n",
    "print(\"Loaded train loader into memory\")\n",
    "val_loader = MemoryLoadedDataLoader(val_loader, device=device)\n",
    "print(\"Loaded validation loader into memory\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train loader into memory\n",
      "Loaded validation loader into memory\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "f03a3a47",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-07T14:23:36.799784Z"
    }
   },
   "source": [
    "# Model variables definition.\n",
    "dropout_rate = 0.40\n",
    "model = model_module.SimplifiedLightweightCNN(input_channels=1, num_classes=1, dropout_rate=dropout_rate)\n",
    "print(model.eval())\n",
    "name = \"StageI-SimplifiedLightweightCNN\"\n",
    "lr = 1e-4  # Reduce from 1e-3\n",
    "epochs = 50\n",
    "0\n",
    "model = model.to(device)\n",
    "# weight_decay = 1e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# Add L2 regularization\n",
    "# Add learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # name of the run\n",
    "    name=name,\n",
    "    config={\n",
    "        \"Name\": name,\n",
    "        \"learning_rate\": lr,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"criterion\": \"BCELoss\",\n",
    "        \"dataset\": \"BothStages-only-polish\",\n",
    "        \"train_val_test(%)\": f'{train_split}-{val_split}-{test_split}',\n",
    "        \"epochs\": epochs,\n",
    "        \"classifier_dropout_rate\": dropout_rate\n",
    "    },\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    # Logging the metadata for each epoch so that the charts can be generated on the dashboard\n",
    "    run.log({\"train_acc\": train_acc, \"train_loss\": train_loss, \"val_acc\": val_acc, \"val_loss\": val_loss, })\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f},\"\n",
    "        f\"Train acc: {train_acc * 100:.2f}%, Val Acc: {val_acc * 100:.2f}%\")\n",
    "\n",
    "run.log({\"model_eval\": model.eval()})\n",
    "# Saving the model to pth and adding it to the artifacts of the run, there is 5GB of memory on wandb, so we should be fine.\n",
    "torch.save(model.state_dict(), os.path.join(RESULT_DIRECTORY, f'{name}.pth'))\n",
    "artifact = wandb.Artifact(name, type=\"model\")\n",
    "artifact.add_file(os.path.join(RESULT_DIRECTORY, f'{name}.pth'))\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "# Finish the run so it gets sent to the remote. You can discover the run right after that on the dashboard.\n",
    "run.finish()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimplifiedLightweightCNN(\n",
      "  (block1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 3), stride=(1, 1), padding=(2, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SiLU()\n",
      "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): SiLU()\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout2d(p=0.1, inplace=False)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SiLU()\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): SiLU()\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout2d(p=0.15, inplace=False)\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "    (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): SiLU()\n",
      "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): SiLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Dropout2d(p=0.15, inplace=False)\n",
      "  )\n",
      "  (block4): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SiLU()\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): SiLU()\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout2d(p=0.2, inplace=False)\n",
      "  )\n",
      "  (attention): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=1)\n",
      "    (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): SiLU()\n",
      "    (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): Sigmoid()\n",
      "  )\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): Dropout(p=0.4, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\fisch\\Documents\\studiaMagisterskie\\sem1\\zespolowyProjektBadawczy\\repo\\Mandarin_Pronunciation_Recognition_Project\\src\\wandb\\run-20250807_162336-vpy9kdpl</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src_src/runs/vpy9kdpl' target=\"_blank\">StageI-SimplifiedLightweightCNN</a></strong> to <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src_src' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src_src' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src_src</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src_src/runs/vpy9kdpl' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src_src/runs/vpy9kdpl</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14a3f8c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T12:23:55.260789Z",
     "start_time": "2025-08-06T12:23:55.256522Z"
    }
   },
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#\n",
    "# model_saved = LightweightCNN()\n",
    "# model_saved.load_state_dict(torch.load(os.path.join(RESULT_DIRECTORY, \"initial_model_t&val_080101split.pth\")))\n",
    "# model_saved.to(device)\n",
    "# model_saved.eval()"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T12:24:18.971483Z",
     "start_time": "2025-08-06T12:23:55.327160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ],
   "id": "4807df96674d796e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6589, Test Acc: 0.6142\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T12:24:23.353763Z",
     "start_time": "2025-08-06T12:24:19.018639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_other_lang_loader = DataLoader(dataset_other, batch_size=16,\n",
    "                                  shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "df_other_lang_loader = MemoryLoadedDataLoader(df_other_lang_loader, device=device)\n",
    "print(\"df_other_lang_loader into memory\")"
   ],
   "id": "95afdb8cd70e36d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_other_lang_loader into memory\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T12:24:23.748668Z",
     "start_time": "2025-08-06T12:24:23.415756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "other_lang_loss, other_lang_acc = evaluate(model, df_other_lang_loader, criterion, device)\n",
    "print(f\"Other mother language Loss: {other_lang_loss:.4f}, Other mother language Acc: {other_lang_acc:.4f}\")"
   ],
   "id": "240583388f0c08d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other mother language Loss: 0.6213, Other mother language Acc: 0.6866\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
