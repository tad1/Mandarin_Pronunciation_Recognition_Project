{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12f8129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:06:49.748097Z",
     "start_time": "2025-07-29T09:06:49.733744Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from data.source.pg_experiment import get_pg_experiment_dataframe\n",
    "import polars as pl\n",
    "\n",
    "from models.SimplifiedLightweightCNN import SimplifiedLightweightCNN\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport models.SimplifiedLightweightCNN\n",
    "from models.SimpleCNN_v2 import train, evaluate\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from path import RESULT_DIRECTORY\n",
    "import wandb\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a57adc8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:04:37.391376Z",
     "start_time": "2025-07-29T09:04:37.356941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_pg_experiment_dataset(): WARNING, Dropped 2 rows with missing files\n",
      "get_pg_experiment_dataset(): WARNING, Dropped 2 rows with missing files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<sys>:0: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "<sys>:0: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (518, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id_student</th><th>value</th><th>word_id</th><th>rec_path</th><th>stage</th><th>univ</th><th>gender</th><th>mother</th></tr><tr><td>i64</td><td>i64</td><td>u32</td><td>str</td><td>i32</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>8</td><td>0</td><td>1</td><td>&quot;/home/tad1/Projects/AI/Mandari…</td><td>1</td><td>&quot;PG_CS_MA_1&quot;</td><td>&quot;m&quot;</td><td>&quot;polish&quot;</td></tr><tr><td>9</td><td>1</td><td>1</td><td>&quot;/home/tad1/Projects/AI/Mandari…</td><td>1</td><td>&quot;PG_CS_MA_1&quot;</td><td>&quot;f&quot;</td><td>&quot;polish&quot;</td></tr><tr><td>10</td><td>0</td><td>1</td><td>&quot;/home/tad1/Projects/AI/Mandari…</td><td>1</td><td>&quot;PG_CS_MA_1&quot;</td><td>&quot;m&quot;</td><td>&quot;polish&quot;</td></tr><tr><td>11</td><td>1</td><td>1</td><td>&quot;/home/tad1/Projects/AI/Mandari…</td><td>1</td><td>&quot;PG_CS_MA_1&quot;</td><td>&quot;f&quot;</td><td>&quot;polish&quot;</td></tr><tr><td>21</td><td>0</td><td>1</td><td>&quot;/home/tad1/Projects/AI/Mandari…</td><td>1</td><td>&quot;PG_CS_MA_1&quot;</td><td>&quot;f&quot;</td><td>&quot;polish&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1537</td><td>1</td><td>1</td><td>&quot;/home/tad1/Projects/AI/Mandari…</td><td>1</td><td>&quot;CLES_UMK5&quot;</td><td>&quot;f&quot;</td><td>&quot;polish&quot;</td></tr><tr><td>1539</td><td>1</td><td>1</td><td>&quot;/home/tad1/Projects/AI/Mandari…</td><td>1</td><td>&quot;CLES_UMK5&quot;</td><td>&quot;m&quot;</td><td>&quot;polish&quot;</td></tr><tr><td>1542</td><td>1</td><td>1</td><td>&quot;/home/tad1/Projects/AI/Mandari…</td><td>1</td><td>&quot;CLES_UMK5&quot;</td><td>&quot;m&quot;</td><td>&quot;polish&quot;</td></tr><tr><td>1543</td><td>1</td><td>1</td><td>&quot;/home/tad1/Projects/AI/Mandari…</td><td>1</td><td>&quot;CLES_UMK5&quot;</td><td>&quot;f&quot;</td><td>&quot;polish&quot;</td></tr><tr><td>1544</td><td>0</td><td>1</td><td>&quot;/home/tad1/Projects/AI/Mandari…</td><td>1</td><td>&quot;CLES_UMK5&quot;</td><td>&quot;m&quot;</td><td>&quot;polish&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (518, 8)\n",
       "┌────────────┬───────┬─────────┬────────────────────────────┬───────┬────────────┬────────┬────────┐\n",
       "│ id_student ┆ value ┆ word_id ┆ rec_path                   ┆ stage ┆ univ       ┆ gender ┆ mother │\n",
       "│ ---        ┆ ---   ┆ ---     ┆ ---                        ┆ ---   ┆ ---        ┆ ---    ┆ ---    │\n",
       "│ i64        ┆ i64   ┆ u32     ┆ str                        ┆ i32   ┆ str        ┆ str    ┆ str    │\n",
       "╞════════════╪═══════╪═════════╪════════════════════════════╪═══════╪════════════╪════════╪════════╡\n",
       "│ 8          ┆ 0     ┆ 1       ┆ /home/tad1/Projects/AI/Man ┆ 1     ┆ PG_CS_MA_1 ┆ m      ┆ polish │\n",
       "│            ┆       ┆         ┆ dari…                      ┆       ┆            ┆        ┆        │\n",
       "│ 9          ┆ 1     ┆ 1       ┆ /home/tad1/Projects/AI/Man ┆ 1     ┆ PG_CS_MA_1 ┆ f      ┆ polish │\n",
       "│            ┆       ┆         ┆ dari…                      ┆       ┆            ┆        ┆        │\n",
       "│ 10         ┆ 0     ┆ 1       ┆ /home/tad1/Projects/AI/Man ┆ 1     ┆ PG_CS_MA_1 ┆ m      ┆ polish │\n",
       "│            ┆       ┆         ┆ dari…                      ┆       ┆            ┆        ┆        │\n",
       "│ 11         ┆ 1     ┆ 1       ┆ /home/tad1/Projects/AI/Man ┆ 1     ┆ PG_CS_MA_1 ┆ f      ┆ polish │\n",
       "│            ┆       ┆         ┆ dari…                      ┆       ┆            ┆        ┆        │\n",
       "│ 21         ┆ 0     ┆ 1       ┆ /home/tad1/Projects/AI/Man ┆ 1     ┆ PG_CS_MA_1 ┆ f      ┆ polish │\n",
       "│            ┆       ┆         ┆ dari…                      ┆       ┆            ┆        ┆        │\n",
       "│ …          ┆ …     ┆ …       ┆ …                          ┆ …     ┆ …          ┆ …      ┆ …      │\n",
       "│ 1537       ┆ 1     ┆ 1       ┆ /home/tad1/Projects/AI/Man ┆ 1     ┆ CLES_UMK5  ┆ f      ┆ polish │\n",
       "│            ┆       ┆         ┆ dari…                      ┆       ┆            ┆        ┆        │\n",
       "│ 1539       ┆ 1     ┆ 1       ┆ /home/tad1/Projects/AI/Man ┆ 1     ┆ CLES_UMK5  ┆ m      ┆ polish │\n",
       "│            ┆       ┆         ┆ dari…                      ┆       ┆            ┆        ┆        │\n",
       "│ 1542       ┆ 1     ┆ 1       ┆ /home/tad1/Projects/AI/Man ┆ 1     ┆ CLES_UMK5  ┆ m      ┆ polish │\n",
       "│            ┆       ┆         ┆ dari…                      ┆       ┆            ┆        ┆        │\n",
       "│ 1543       ┆ 1     ┆ 1       ┆ /home/tad1/Projects/AI/Man ┆ 1     ┆ CLES_UMK5  ┆ f      ┆ polish │\n",
       "│            ┆       ┆         ┆ dari…                      ┆       ┆            ┆        ┆        │\n",
       "│ 1544       ┆ 0     ┆ 1       ┆ /home/tad1/Projects/AI/Man ┆ 1     ┆ CLES_UMK5  ┆ m      ┆ polish │\n",
       "│            ┆       ┆         ┆ dari…                      ┆       ┆            ┆        ┆        │\n",
       "└────────────┴───────┴─────────┴────────────────────────────┴───────┴────────────┴────────┴────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pron, df_tone = get_pg_experiment_dataframe(\".ogg\")\n",
    "# df_stageI_polish = df_pron.filter(pl.col(\"mother\") == \"polish\")\n",
    "df_stageI_polish = df_pron.with_columns(word_id = pl.struct(\"word_id\").rank(\"dense\"))\n",
    "df_stageI_polish = df_stageI_polish.filter(pl.col(\"word_id\") == 1)\n",
    "df_stageI_polish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a95dcb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:04:37.442136Z",
     "start_time": "2025-07-29T09:04:37.416770Z"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "def stratified_split(df: pl.DataFrame, label_col: str, train_frac=0.8, val_frac=0.1, seed=42) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n",
    "    classes = df.select(label_col).unique().to_series()\n",
    "    train_rows, val_rows, test_rows = [], [], []\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    for cls in classes:\n",
    "        class_df = df.filter(pl.col(label_col) == cls)\n",
    "        n = class_df.height\n",
    "        indices = rng.permutation(n)\n",
    "\n",
    "        train_end = int(train_frac * n)\n",
    "        val_end = int((train_frac + val_frac) * n)\n",
    "\n",
    "        train_rows.append(class_df[indices[:train_end]])\n",
    "        val_rows.append(class_df[indices[train_end:val_end]])\n",
    "        test_rows.append(class_df[indices[val_end:]])\n",
    "\n",
    "    train_df = pl.concat(train_rows)\n",
    "    val_df = pl.concat(val_rows)\n",
    "    test_df = pl.concat(test_rows)\n",
    "\n",
    "    return train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd331ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:04:37.500687Z",
     "start_time": "2025-07-29T09:04:37.486859Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import Cast, TorchDataset\n",
    "from develop import reload_function, reload_module\n",
    "import pytorch_dataloader\n",
    "reload_module(pytorch_dataloader)\n",
    "from pytorch_dataloader import ReshapeCollate, build_collate_fn, PaddingCollate, DefaultCollate\n",
    "from functools import partial\n",
    "\n",
    "from transformation import Channels, RMSEnergy, TorchVadLogMelSpec, TorchVadMFCC, ZeroCrossingRate\n",
    "\n",
    "reload_function(TorchVadMFCC)\n",
    "\n",
    "TRAIN_SPLIT = 0.6\n",
    "VAL_SPLIT = 0.2\n",
    "TEST_SPLIT = 1 - TRAIN_SPLIT - VAL_SPLIT\n",
    "train_pl, val_pl, test_pl = stratified_split(df_stageI_polish, label_col=\"value\", train_frac=TRAIN_SPLIT, val_frac=VAL_SPLIT)\n",
    "\n",
    "to_dataset = lambda dataframe: TorchDataset(\n",
    "    Cast(dataframe.get_column(\"rec_path\"), Channels(\"stack\",\"multiply\")(\n",
    "            TorchVadMFCC(delta=0),\n",
    "            TorchVadMFCC(delta=1),\n",
    "            TorchVadMFCC(delta=2),\n",
    "        )),\n",
    "    Cast(dataframe.get_column(\"rec_path\"), Channels(\"cat\",\"multiply\")(\n",
    "            ZeroCrossingRate(),\n",
    "            RMSEnergy(),\n",
    "        )),\n",
    "    Cast(dataframe.get_column(\"value\"), lambda x: torch.tensor(x).float()),\n",
    ")\n",
    "\n",
    "collate_fn = build_collate_fn(\n",
    "    PaddingCollate(mode=\"SET_MAX_LEN\", max_len=80, pad_dim=2),\n",
    "    PaddingCollate(mode=\"SET_MAX_LEN\", max_len=160, pad_dim=1),\n",
    "    DefaultCollate(),\n",
    ")\n",
    "dataset_train = to_dataset(train_pl)\n",
    "dataset_val = to_dataset(val_pl)\n",
    "dataset_test = to_dataset(test_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be889768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/1297/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/1297/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/1297/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/772/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/772/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/772/a0.ogg has no speech segments, using full waveform\n",
      "torch.Size([16, 3, 40, 80]) torch.Size([16, 2, 160]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "from pytorch_dataloader import MemoryLoadedDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "val_loader = DataLoader(dataset_val, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "test_loader = DataLoader(dataset_test, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "input2d, input1d, label  = next(iter(train_loader))\n",
    "print(input2d.shape, input1d.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf890d68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:05:23.032498Z",
     "start_time": "2025-07-29T09:04:37.522932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/1536/a0.ogg/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/1293/a0.ogg has no speech segments, using full waveform \n",
      "has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/1536/a0.ogg /home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/1293/a0.ogghas no speech segments, using full waveform has no speech segments, using full waveform\n",
      "\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/1293/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/1536/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/612/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/612/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/612/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/633/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/633/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/633/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/484/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/484/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/484/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/761/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/761/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/761/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/1297/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/1297/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/1297/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/772/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/772/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/772/a0.ogg has no speech segments, using full waveform\n",
      "Loaded train loader into memory\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/944/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/944/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/944/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/550/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/550/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/550/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/345/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/345/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/345/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/668/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/668/a0.ogg has no speech segments, using full waveform\n",
      "/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/data/source/pg_dataset/recordings/stageI/668/a0.ogg has no speech segments, using full waveform\n",
      "Loaded validation loader into memory\n"
     ]
    }
   ],
   "source": [
    "train_loader = MemoryLoadedDataLoader(train_loader, device=device)\n",
    "print(\"Loaded train loader into memory\")\n",
    "val_loader = MemoryLoadedDataLoader(val_loader, device=device)\n",
    "print(\"Loaded validation loader into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c91ba076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.FusionCNN import FusionCNN\n",
    "reload_function(FusionCNN)\n",
    "model = FusionCNN(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f03a3a47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:25:42.755943Z",
     "start_time": "2025-07-29T09:22:19.522018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/src/wandb/run-20250809_001046-io1x1cxc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/io1x1cxc' target=\"_blank\">Hypothesis-5 FusionCNN</a></strong> to <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/io1x1cxc' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/io1x1cxc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6910, Train Acc: 0.5290, Val Loss: 0.6904, Val Acc: 0.5631\n",
      "Epoch 2, Train Loss: 0.6888, Train Acc: 0.5677, Val Loss: 0.6840, Val Acc: 0.5922\n",
      "Epoch 3, Train Loss: 0.6800, Train Acc: 0.5839, Val Loss: 0.6795, Val Acc: 0.5922\n",
      "Epoch 4, Train Loss: 0.6774, Train Acc: 0.5742, Val Loss: 0.6764, Val Acc: 0.5922\n",
      "Epoch 5, Train Loss: 0.6691, Train Acc: 0.6000, Val Loss: 0.6732, Val Acc: 0.5922\n",
      "Epoch 6, Train Loss: 0.6811, Train Acc: 0.5839, Val Loss: 0.6716, Val Acc: 0.5922\n",
      "Epoch 7, Train Loss: 0.6702, Train Acc: 0.6032, Val Loss: 0.6692, Val Acc: 0.5922\n",
      "Epoch 8, Train Loss: 0.6747, Train Acc: 0.5935, Val Loss: 0.6676, Val Acc: 0.5922\n",
      "Epoch 9, Train Loss: 0.6722, Train Acc: 0.5903, Val Loss: 0.6664, Val Acc: 0.5922\n",
      "Epoch 10, Train Loss: 0.6720, Train Acc: 0.5968, Val Loss: 0.6655, Val Acc: 0.5922\n",
      "Epoch 11, Train Loss: 0.6697, Train Acc: 0.5935, Val Loss: 0.6653, Val Acc: 0.5922\n",
      "Epoch 12, Train Loss: 0.6728, Train Acc: 0.6032, Val Loss: 0.6647, Val Acc: 0.5922\n",
      "Epoch 13, Train Loss: 0.6668, Train Acc: 0.5903, Val Loss: 0.6636, Val Acc: 0.5922\n",
      "Epoch 14, Train Loss: 0.6648, Train Acc: 0.5968, Val Loss: 0.6627, Val Acc: 0.5922\n",
      "Epoch 15, Train Loss: 0.6752, Train Acc: 0.5968, Val Loss: 0.6618, Val Acc: 0.5922\n",
      "Epoch 16, Train Loss: 0.6726, Train Acc: 0.6000, Val Loss: 0.6615, Val Acc: 0.5922\n",
      "Epoch 17, Train Loss: 0.6651, Train Acc: 0.6032, Val Loss: 0.6601, Val Acc: 0.5922\n",
      "Epoch 18, Train Loss: 0.6637, Train Acc: 0.5968, Val Loss: 0.6581, Val Acc: 0.5922\n",
      "Epoch 19, Train Loss: 0.6573, Train Acc: 0.6032, Val Loss: 0.6559, Val Acc: 0.5922\n",
      "Epoch 20, Train Loss: 0.6542, Train Acc: 0.6065, Val Loss: 0.6543, Val Acc: 0.6019\n",
      "Epoch 21, Train Loss: 0.6545, Train Acc: 0.5903, Val Loss: 0.6530, Val Acc: 0.6019\n",
      "Epoch 22, Train Loss: 0.6674, Train Acc: 0.5806, Val Loss: 0.6525, Val Acc: 0.6019\n",
      "Epoch 23, Train Loss: 0.6590, Train Acc: 0.5903, Val Loss: 0.6497, Val Acc: 0.6019\n",
      "Epoch 24, Train Loss: 0.6495, Train Acc: 0.6000, Val Loss: 0.6489, Val Acc: 0.6311\n",
      "Epoch 25, Train Loss: 0.6434, Train Acc: 0.6129, Val Loss: 0.6480, Val Acc: 0.6117\n",
      "Epoch 26, Train Loss: 0.6601, Train Acc: 0.5968, Val Loss: 0.6449, Val Acc: 0.6311\n",
      "Epoch 27, Train Loss: 0.6513, Train Acc: 0.6161, Val Loss: 0.6389, Val Acc: 0.6311\n",
      "Epoch 28, Train Loss: 0.6506, Train Acc: 0.6226, Val Loss: 0.6388, Val Acc: 0.6408\n",
      "Epoch 29, Train Loss: 0.6413, Train Acc: 0.6452, Val Loss: 0.6394, Val Acc: 0.6214\n",
      "Epoch 30, Train Loss: 0.6503, Train Acc: 0.6000, Val Loss: 0.6365, Val Acc: 0.6117\n",
      "Epoch 31, Train Loss: 0.6408, Train Acc: 0.6355, Val Loss: 0.6340, Val Acc: 0.6214\n",
      "Epoch 32, Train Loss: 0.6523, Train Acc: 0.6226, Val Loss: 0.6330, Val Acc: 0.6214\n",
      "Epoch 33, Train Loss: 0.6327, Train Acc: 0.6323, Val Loss: 0.6298, Val Acc: 0.6117\n",
      "Epoch 34, Train Loss: 0.6286, Train Acc: 0.6387, Val Loss: 0.6282, Val Acc: 0.6311\n",
      "Epoch 35, Train Loss: 0.6293, Train Acc: 0.6226, Val Loss: 0.6254, Val Acc: 0.6408\n",
      "Epoch 36, Train Loss: 0.6507, Train Acc: 0.5968, Val Loss: 0.6224, Val Acc: 0.6505\n",
      "Epoch 37, Train Loss: 0.6247, Train Acc: 0.6355, Val Loss: 0.6197, Val Acc: 0.6505\n",
      "Epoch 38, Train Loss: 0.6311, Train Acc: 0.6226, Val Loss: 0.6147, Val Acc: 0.6505\n",
      "Epoch 39, Train Loss: 0.6434, Train Acc: 0.6032, Val Loss: 0.6106, Val Acc: 0.6408\n",
      "Epoch 40, Train Loss: 0.6261, Train Acc: 0.6065, Val Loss: 0.6083, Val Acc: 0.6505\n",
      "Epoch 41, Train Loss: 0.6153, Train Acc: 0.6871, Val Loss: 0.6060, Val Acc: 0.6699\n",
      "Epoch 42, Train Loss: 0.6311, Train Acc: 0.6194, Val Loss: 0.6047, Val Acc: 0.6893\n",
      "Epoch 43, Train Loss: 0.6269, Train Acc: 0.6355, Val Loss: 0.6033, Val Acc: 0.7184\n",
      "Epoch 44, Train Loss: 0.6062, Train Acc: 0.6935, Val Loss: 0.5962, Val Acc: 0.6699\n",
      "Epoch 45, Train Loss: 0.6033, Train Acc: 0.6806, Val Loss: 0.5970, Val Acc: 0.7379\n",
      "Epoch 46, Train Loss: 0.6079, Train Acc: 0.6645, Val Loss: 0.5908, Val Acc: 0.6893\n",
      "Epoch 47, Train Loss: 0.6212, Train Acc: 0.6419, Val Loss: 0.5939, Val Acc: 0.6990\n",
      "Epoch 48, Train Loss: 0.6123, Train Acc: 0.6484, Val Loss: 0.5948, Val Acc: 0.7184\n",
      "Epoch 49, Train Loss: 0.6171, Train Acc: 0.6548, Val Loss: 0.5845, Val Acc: 0.7379\n",
      "Epoch 50, Train Loss: 0.6173, Train Acc: 0.6161, Val Loss: 0.5860, Val Acc: 0.7282\n",
      "Epoch 51, Train Loss: 0.6008, Train Acc: 0.6484, Val Loss: 0.5811, Val Acc: 0.7282\n",
      "Epoch 52, Train Loss: 0.6032, Train Acc: 0.6742, Val Loss: 0.5774, Val Acc: 0.7379\n",
      "Epoch 53, Train Loss: 0.6185, Train Acc: 0.6516, Val Loss: 0.5739, Val Acc: 0.7282\n",
      "Epoch 54, Train Loss: 0.6025, Train Acc: 0.6677, Val Loss: 0.5782, Val Acc: 0.7282\n",
      "Epoch 55, Train Loss: 0.5820, Train Acc: 0.7000, Val Loss: 0.5680, Val Acc: 0.6990\n",
      "Epoch 56, Train Loss: 0.5842, Train Acc: 0.7000, Val Loss: 0.5642, Val Acc: 0.7379\n",
      "Epoch 57, Train Loss: 0.5828, Train Acc: 0.6871, Val Loss: 0.5642, Val Acc: 0.7184\n",
      "Epoch 58, Train Loss: 0.5867, Train Acc: 0.6839, Val Loss: 0.5640, Val Acc: 0.7087\n",
      "Epoch 59, Train Loss: 0.5611, Train Acc: 0.6935, Val Loss: 0.5601, Val Acc: 0.7184\n",
      "Epoch 60, Train Loss: 0.5851, Train Acc: 0.6774, Val Loss: 0.5584, Val Acc: 0.7087\n",
      "Epoch 61, Train Loss: 0.5837, Train Acc: 0.6871, Val Loss: 0.5576, Val Acc: 0.7379\n",
      "Epoch 62, Train Loss: 0.5611, Train Acc: 0.7258, Val Loss: 0.5547, Val Acc: 0.7476\n",
      "Epoch 63, Train Loss: 0.5573, Train Acc: 0.7097, Val Loss: 0.5516, Val Acc: 0.7282\n",
      "Epoch 64, Train Loss: 0.5679, Train Acc: 0.7097, Val Loss: 0.5455, Val Acc: 0.7379\n",
      "Epoch 65, Train Loss: 0.5834, Train Acc: 0.7000, Val Loss: 0.5424, Val Acc: 0.7476\n",
      "Epoch 66, Train Loss: 0.5780, Train Acc: 0.6871, Val Loss: 0.5498, Val Acc: 0.7379\n",
      "Epoch 67, Train Loss: 0.5979, Train Acc: 0.6774, Val Loss: 0.5453, Val Acc: 0.7476\n",
      "Epoch 68, Train Loss: 0.5586, Train Acc: 0.7000, Val Loss: 0.5535, Val Acc: 0.7379\n",
      "Epoch 69, Train Loss: 0.5478, Train Acc: 0.7258, Val Loss: 0.5377, Val Acc: 0.7379\n",
      "Epoch 70, Train Loss: 0.5568, Train Acc: 0.7065, Val Loss: 0.5483, Val Acc: 0.7573\n",
      "Epoch 71, Train Loss: 0.5779, Train Acc: 0.6548, Val Loss: 0.5346, Val Acc: 0.7476\n",
      "Epoch 72, Train Loss: 0.5786, Train Acc: 0.6839, Val Loss: 0.5312, Val Acc: 0.7379\n",
      "Epoch 73, Train Loss: 0.5725, Train Acc: 0.7032, Val Loss: 0.5427, Val Acc: 0.7379\n",
      "Epoch 74, Train Loss: 0.5566, Train Acc: 0.7129, Val Loss: 0.5293, Val Acc: 0.7476\n",
      "Epoch 75, Train Loss: 0.5518, Train Acc: 0.7484, Val Loss: 0.5364, Val Acc: 0.7282\n",
      "Epoch 76, Train Loss: 0.5576, Train Acc: 0.7097, Val Loss: 0.5274, Val Acc: 0.7282\n",
      "Epoch 77, Train Loss: 0.5383, Train Acc: 0.7226, Val Loss: 0.5329, Val Acc: 0.7282\n",
      "Epoch 78, Train Loss: 0.5662, Train Acc: 0.6839, Val Loss: 0.5322, Val Acc: 0.7379\n",
      "Epoch 79, Train Loss: 0.5742, Train Acc: 0.6968, Val Loss: 0.5284, Val Acc: 0.7379\n",
      "Epoch 80, Train Loss: 0.5453, Train Acc: 0.7129, Val Loss: 0.5247, Val Acc: 0.7476\n",
      "Epoch 81, Train Loss: 0.5352, Train Acc: 0.7161, Val Loss: 0.5212, Val Acc: 0.7476\n",
      "Epoch 82, Train Loss: 0.5421, Train Acc: 0.6871, Val Loss: 0.5236, Val Acc: 0.7670\n",
      "Epoch 83, Train Loss: 0.5349, Train Acc: 0.7226, Val Loss: 0.5182, Val Acc: 0.7573\n",
      "Epoch 84, Train Loss: 0.5494, Train Acc: 0.7129, Val Loss: 0.5178, Val Acc: 0.7573\n",
      "Epoch 85, Train Loss: 0.5197, Train Acc: 0.7226, Val Loss: 0.5205, Val Acc: 0.7670\n",
      "Epoch 86, Train Loss: 0.5607, Train Acc: 0.7129, Val Loss: 0.5147, Val Acc: 0.7573\n",
      "Epoch 87, Train Loss: 0.5320, Train Acc: 0.7258, Val Loss: 0.5198, Val Acc: 0.7767\n",
      "Epoch 88, Train Loss: 0.5478, Train Acc: 0.6871, Val Loss: 0.5130, Val Acc: 0.7670\n",
      "Epoch 89, Train Loss: 0.5249, Train Acc: 0.7452, Val Loss: 0.5153, Val Acc: 0.7573\n",
      "Epoch 90, Train Loss: 0.5280, Train Acc: 0.7516, Val Loss: 0.5140, Val Acc: 0.7573\n",
      "Epoch 91, Train Loss: 0.5183, Train Acc: 0.7355, Val Loss: 0.5083, Val Acc: 0.7670\n",
      "Epoch 92, Train Loss: 0.5167, Train Acc: 0.7516, Val Loss: 0.5097, Val Acc: 0.7670\n",
      "Epoch 93, Train Loss: 0.5165, Train Acc: 0.7387, Val Loss: 0.5063, Val Acc: 0.7573\n",
      "Epoch 94, Train Loss: 0.5111, Train Acc: 0.7516, Val Loss: 0.5096, Val Acc: 0.7670\n",
      "Epoch 95, Train Loss: 0.5164, Train Acc: 0.7581, Val Loss: 0.5104, Val Acc: 0.7767\n",
      "Epoch 96, Train Loss: 0.5232, Train Acc: 0.7452, Val Loss: 0.5010, Val Acc: 0.7767\n",
      "Epoch 97, Train Loss: 0.5172, Train Acc: 0.7581, Val Loss: 0.5001, Val Acc: 0.7670\n",
      "Epoch 98, Train Loss: 0.5142, Train Acc: 0.7290, Val Loss: 0.5002, Val Acc: 0.7767\n",
      "Epoch 99, Train Loss: 0.5109, Train Acc: 0.7484, Val Loss: 0.4978, Val Acc: 0.7864\n",
      "Epoch 100, Train Loss: 0.5268, Train Acc: 0.7161, Val Loss: 0.4964, Val Acc: 0.7767\n",
      "Epoch 101, Train Loss: 0.5241, Train Acc: 0.7581, Val Loss: 0.5064, Val Acc: 0.7573\n",
      "Epoch 102, Train Loss: 0.4821, Train Acc: 0.7839, Val Loss: 0.4957, Val Acc: 0.7767\n",
      "Epoch 103, Train Loss: 0.4867, Train Acc: 0.7903, Val Loss: 0.4936, Val Acc: 0.7864\n",
      "Epoch 104, Train Loss: 0.5139, Train Acc: 0.7387, Val Loss: 0.4980, Val Acc: 0.7670\n",
      "Epoch 105, Train Loss: 0.5048, Train Acc: 0.7452, Val Loss: 0.4900, Val Acc: 0.7864\n",
      "Epoch 106, Train Loss: 0.4806, Train Acc: 0.7548, Val Loss: 0.4920, Val Acc: 0.7864\n",
      "Epoch 107, Train Loss: 0.5037, Train Acc: 0.7645, Val Loss: 0.4917, Val Acc: 0.7864\n",
      "Epoch 108, Train Loss: 0.5139, Train Acc: 0.7419, Val Loss: 0.4957, Val Acc: 0.7670\n",
      "Epoch 109, Train Loss: 0.4867, Train Acc: 0.7613, Val Loss: 0.4867, Val Acc: 0.7864\n",
      "Epoch 110, Train Loss: 0.4771, Train Acc: 0.7903, Val Loss: 0.4930, Val Acc: 0.7767\n",
      "Epoch 111, Train Loss: 0.4802, Train Acc: 0.7677, Val Loss: 0.4889, Val Acc: 0.7961\n",
      "Epoch 112, Train Loss: 0.4875, Train Acc: 0.7677, Val Loss: 0.4866, Val Acc: 0.7864\n",
      "Epoch 113, Train Loss: 0.4930, Train Acc: 0.7581, Val Loss: 0.4846, Val Acc: 0.7961\n",
      "Epoch 114, Train Loss: 0.4920, Train Acc: 0.7581, Val Loss: 0.4877, Val Acc: 0.7961\n",
      "Epoch 115, Train Loss: 0.4934, Train Acc: 0.7581, Val Loss: 0.4830, Val Acc: 0.7767\n",
      "Epoch 116, Train Loss: 0.4818, Train Acc: 0.7839, Val Loss: 0.4995, Val Acc: 0.7864\n",
      "Epoch 117, Train Loss: 0.4750, Train Acc: 0.7806, Val Loss: 0.4850, Val Acc: 0.7767\n",
      "Epoch 118, Train Loss: 0.4783, Train Acc: 0.7677, Val Loss: 0.4972, Val Acc: 0.7864\n",
      "Epoch 119, Train Loss: 0.4633, Train Acc: 0.7742, Val Loss: 0.4792, Val Acc: 0.7767\n",
      "Epoch 120, Train Loss: 0.4503, Train Acc: 0.7742, Val Loss: 0.4801, Val Acc: 0.7961\n",
      "Epoch 121, Train Loss: 0.4575, Train Acc: 0.7839, Val Loss: 0.4745, Val Acc: 0.7864\n",
      "Epoch 122, Train Loss: 0.4436, Train Acc: 0.8000, Val Loss: 0.4840, Val Acc: 0.7864\n",
      "Epoch 123, Train Loss: 0.4523, Train Acc: 0.7806, Val Loss: 0.4828, Val Acc: 0.7767\n",
      "Epoch 124, Train Loss: 0.4356, Train Acc: 0.8194, Val Loss: 0.4780, Val Acc: 0.7864\n",
      "Epoch 125, Train Loss: 0.4502, Train Acc: 0.8161, Val Loss: 0.4879, Val Acc: 0.7767\n",
      "Epoch 126, Train Loss: 0.4768, Train Acc: 0.7742, Val Loss: 0.4765, Val Acc: 0.7961\n",
      "Epoch 127, Train Loss: 0.4502, Train Acc: 0.7968, Val Loss: 0.4811, Val Acc: 0.7961\n",
      "Epoch 128, Train Loss: 0.4252, Train Acc: 0.8323, Val Loss: 0.4848, Val Acc: 0.7864\n",
      "Epoch 129, Train Loss: 0.4427, Train Acc: 0.7871, Val Loss: 0.4772, Val Acc: 0.7961\n",
      "Epoch 130, Train Loss: 0.4531, Train Acc: 0.7677, Val Loss: 0.4810, Val Acc: 0.7961\n",
      "Epoch 131, Train Loss: 0.4598, Train Acc: 0.7871, Val Loss: 0.4808, Val Acc: 0.7864\n",
      "Epoch 132, Train Loss: 0.4257, Train Acc: 0.8258, Val Loss: 0.4793, Val Acc: 0.7864\n",
      "Epoch 133, Train Loss: 0.4394, Train Acc: 0.8065, Val Loss: 0.4774, Val Acc: 0.7864\n",
      "Epoch 134, Train Loss: 0.4463, Train Acc: 0.8129, Val Loss: 0.4864, Val Acc: 0.7767\n",
      "Epoch 135, Train Loss: 0.4092, Train Acc: 0.7903, Val Loss: 0.4828, Val Acc: 0.7961\n",
      "Epoch 136, Train Loss: 0.4370, Train Acc: 0.8129, Val Loss: 0.4779, Val Acc: 0.7864\n",
      "Epoch 137, Train Loss: 0.4245, Train Acc: 0.8129, Val Loss: 0.4777, Val Acc: 0.7961\n",
      "Epoch 138, Train Loss: 0.4556, Train Acc: 0.7452, Val Loss: 0.4816, Val Acc: 0.7864\n",
      "Epoch 139, Train Loss: 0.4326, Train Acc: 0.8065, Val Loss: 0.4751, Val Acc: 0.7961\n",
      "Epoch 140, Train Loss: 0.4196, Train Acc: 0.8000, Val Loss: 0.4763, Val Acc: 0.7864\n",
      "Epoch 141, Train Loss: 0.4153, Train Acc: 0.8258, Val Loss: 0.4767, Val Acc: 0.7864\n",
      "Epoch 142, Train Loss: 0.4178, Train Acc: 0.8161, Val Loss: 0.4758, Val Acc: 0.7864\n",
      "Epoch 143, Train Loss: 0.4349, Train Acc: 0.8000, Val Loss: 0.4757, Val Acc: 0.7961\n",
      "Epoch 144, Train Loss: 0.4687, Train Acc: 0.7645, Val Loss: 0.4737, Val Acc: 0.7961\n",
      "Epoch 145, Train Loss: 0.4607, Train Acc: 0.7839, Val Loss: 0.4724, Val Acc: 0.7961\n",
      "Epoch 146, Train Loss: 0.4087, Train Acc: 0.8419, Val Loss: 0.4776, Val Acc: 0.7864\n",
      "Epoch 147, Train Loss: 0.4322, Train Acc: 0.8194, Val Loss: 0.4773, Val Acc: 0.7864\n",
      "Epoch 148, Train Loss: 0.4127, Train Acc: 0.8226, Val Loss: 0.4762, Val Acc: 0.7864\n",
      "Epoch 149, Train Loss: 0.4371, Train Acc: 0.7806, Val Loss: 0.4773, Val Acc: 0.7864\n",
      "Epoch 150, Train Loss: 0.4415, Train Acc: 0.8065, Val Loss: 0.4758, Val Acc: 0.7864\n",
      "Epoch 151, Train Loss: 0.4218, Train Acc: 0.8290, Val Loss: 0.4752, Val Acc: 0.7864\n",
      "Epoch 152, Train Loss: 0.4296, Train Acc: 0.8323, Val Loss: 0.4754, Val Acc: 0.7864\n",
      "Epoch 153, Train Loss: 0.4152, Train Acc: 0.8323, Val Loss: 0.4765, Val Acc: 0.7864\n",
      "Epoch 154, Train Loss: 0.4303, Train Acc: 0.8129, Val Loss: 0.4775, Val Acc: 0.7864\n",
      "Epoch 155, Train Loss: 0.4106, Train Acc: 0.8097, Val Loss: 0.4724, Val Acc: 0.8058\n",
      "Epoch 156, Train Loss: 0.4555, Train Acc: 0.7903, Val Loss: 0.4772, Val Acc: 0.7864\n",
      "Epoch 157, Train Loss: 0.3986, Train Acc: 0.8161, Val Loss: 0.4786, Val Acc: 0.7864\n",
      "Epoch 158, Train Loss: 0.4510, Train Acc: 0.7935, Val Loss: 0.4785, Val Acc: 0.7864\n",
      "Epoch 159, Train Loss: 0.3945, Train Acc: 0.8355, Val Loss: 0.4769, Val Acc: 0.7864\n",
      "Epoch 160, Train Loss: 0.4261, Train Acc: 0.8226, Val Loss: 0.4767, Val Acc: 0.7864\n",
      "Epoch 161, Train Loss: 0.4476, Train Acc: 0.8097, Val Loss: 0.4733, Val Acc: 0.8058\n",
      "Epoch 162, Train Loss: 0.4286, Train Acc: 0.8032, Val Loss: 0.4762, Val Acc: 0.7864\n",
      "Epoch 163, Train Loss: 0.4088, Train Acc: 0.8161, Val Loss: 0.4813, Val Acc: 0.7767\n",
      "Epoch 164, Train Loss: 0.4262, Train Acc: 0.8129, Val Loss: 0.4748, Val Acc: 0.7864\n",
      "Epoch 165, Train Loss: 0.4128, Train Acc: 0.8097, Val Loss: 0.4786, Val Acc: 0.7864\n",
      "Epoch 166, Train Loss: 0.4250, Train Acc: 0.8323, Val Loss: 0.4740, Val Acc: 0.7961\n",
      "Epoch 167, Train Loss: 0.4241, Train Acc: 0.8032, Val Loss: 0.4748, Val Acc: 0.7864\n",
      "Epoch 168, Train Loss: 0.4301, Train Acc: 0.8355, Val Loss: 0.4725, Val Acc: 0.8058\n",
      "Epoch 169, Train Loss: 0.3860, Train Acc: 0.8419, Val Loss: 0.4770, Val Acc: 0.7864\n",
      "Epoch 170, Train Loss: 0.4064, Train Acc: 0.8194, Val Loss: 0.4763, Val Acc: 0.7864\n",
      "Epoch 171, Train Loss: 0.4344, Train Acc: 0.8000, Val Loss: 0.4742, Val Acc: 0.7961\n",
      "Epoch 172, Train Loss: 0.4021, Train Acc: 0.8323, Val Loss: 0.4773, Val Acc: 0.7864\n",
      "Epoch 173, Train Loss: 0.4132, Train Acc: 0.8065, Val Loss: 0.4745, Val Acc: 0.7961\n",
      "Epoch 174, Train Loss: 0.4181, Train Acc: 0.7968, Val Loss: 0.4777, Val Acc: 0.7864\n",
      "Epoch 175, Train Loss: 0.4084, Train Acc: 0.8226, Val Loss: 0.4759, Val Acc: 0.7864\n",
      "Epoch 176, Train Loss: 0.4233, Train Acc: 0.7968, Val Loss: 0.4741, Val Acc: 0.7961\n",
      "Epoch 177, Train Loss: 0.4108, Train Acc: 0.8226, Val Loss: 0.4762, Val Acc: 0.7864\n",
      "Epoch 178, Train Loss: 0.4346, Train Acc: 0.8226, Val Loss: 0.4772, Val Acc: 0.7864\n",
      "Epoch 179, Train Loss: 0.4471, Train Acc: 0.7903, Val Loss: 0.4742, Val Acc: 0.7961\n",
      "Epoch 180, Train Loss: 0.4288, Train Acc: 0.8161, Val Loss: 0.4745, Val Acc: 0.7961\n",
      "Epoch 181, Train Loss: 0.3983, Train Acc: 0.8065, Val Loss: 0.4753, Val Acc: 0.7864\n",
      "Epoch 182, Train Loss: 0.4040, Train Acc: 0.8258, Val Loss: 0.4776, Val Acc: 0.7864\n",
      "Epoch 183, Train Loss: 0.4160, Train Acc: 0.8032, Val Loss: 0.4801, Val Acc: 0.7767\n",
      "Epoch 184, Train Loss: 0.4246, Train Acc: 0.8097, Val Loss: 0.4778, Val Acc: 0.7864\n",
      "Epoch 185, Train Loss: 0.3972, Train Acc: 0.8290, Val Loss: 0.4764, Val Acc: 0.7864\n",
      "Epoch 186, Train Loss: 0.4167, Train Acc: 0.8097, Val Loss: 0.4759, Val Acc: 0.7864\n",
      "Epoch 187, Train Loss: 0.3948, Train Acc: 0.8290, Val Loss: 0.4756, Val Acc: 0.7864\n",
      "Epoch 188, Train Loss: 0.4158, Train Acc: 0.8290, Val Loss: 0.4797, Val Acc: 0.7767\n",
      "Epoch 189, Train Loss: 0.4426, Train Acc: 0.8161, Val Loss: 0.4774, Val Acc: 0.7864\n",
      "Epoch 190, Train Loss: 0.4080, Train Acc: 0.8129, Val Loss: 0.4756, Val Acc: 0.7864\n",
      "Epoch 191, Train Loss: 0.4160, Train Acc: 0.8290, Val Loss: 0.4732, Val Acc: 0.7961\n",
      "Epoch 192, Train Loss: 0.4229, Train Acc: 0.8129, Val Loss: 0.4752, Val Acc: 0.7864\n",
      "Epoch 193, Train Loss: 0.4138, Train Acc: 0.8097, Val Loss: 0.4752, Val Acc: 0.7864\n",
      "Epoch 194, Train Loss: 0.4336, Train Acc: 0.8032, Val Loss: 0.4746, Val Acc: 0.7961\n",
      "Epoch 195, Train Loss: 0.4326, Train Acc: 0.7871, Val Loss: 0.4765, Val Acc: 0.7864\n",
      "Epoch 196, Train Loss: 0.4130, Train Acc: 0.8065, Val Loss: 0.4766, Val Acc: 0.7864\n",
      "Epoch 197, Train Loss: 0.4041, Train Acc: 0.8226, Val Loss: 0.4761, Val Acc: 0.7864\n",
      "Epoch 198, Train Loss: 0.4097, Train Acc: 0.8226, Val Loss: 0.4751, Val Acc: 0.7864\n",
      "Epoch 199, Train Loss: 0.4079, Train Acc: 0.8258, Val Loss: 0.4805, Val Acc: 0.7767\n",
      "Epoch 200, Train Loss: 0.4137, Train Acc: 0.8355, Val Loss: 0.4745, Val Acc: 0.7961\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▄▃▂▄▄▅▄▄▅▄▄▅▅▅▇▆▆▇▆▆█▇▆▇█▇▇▇██▇██</td></tr><tr><td>train_loss</td><td>█▇██▇▇▇▇▇▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▃▁▂▁▂▂▂▁▁▂▁▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▃▃▆▅▆▆▅▇▆▇▇▇▇▇▇██▇█▇▇███████████████▇</td></tr><tr><td>val_loss</td><td>████▇▇▇▇▆▆▆▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>model_eval</td><td>FusionCNN(<br>  (featur...</td></tr><tr><td>train_acc</td><td>0.83548</td></tr><tr><td>train_loss</td><td>0.41374</td></tr><tr><td>val_acc</td><td>0.79612</td></tr><tr><td>val_loss</td><td>0.47453</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Hypothesis-5 FusionCNN</strong> at: <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/io1x1cxc' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/io1x1cxc</a><br> View project at: <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250809_001046-io1x1cxc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model variables definition.\n",
    "pth = \"SimplifiedLightweightCNN.pth\"\n",
    "lr = 1e-4  # Reduce from 1e-3\n",
    "epochs = 200\n",
    "model = model.to(device)\n",
    "reload_function(train)\n",
    "reload_function(evaluate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)  # Add L2 regularization\n",
    "# Add learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # name of the run\n",
    "    name=\"Hypothesis-5 FusionCNN\",\n",
    "    config={\n",
    "        \"Name\": 'SimplifiedLightweightCNN',\n",
    "        \"learning_rate\": lr,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"criterion\": \"BCELoss\",\n",
    "        \"architecture\": \"SimplifiedLightweightCNN\",\n",
    "        \"architecture_details\": str(model),\n",
    "        \"dataset\": \"Stage-I-only-polish\",\n",
    "        \"train_val_test(%)\": f'{TRAIN_SPLIT}-{VAL_SPLIT}-{TEST_SPLIT}',\n",
    "        \"epochs\": epochs,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    # Logging the metadata for each epoch so that the charts can be generated on the dashboard\n",
    "    run.log({\"train_acc\": train_acc, \"train_loss\": train_loss, \"val_acc\": val_acc, \"val_loss\": val_loss, })\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "run.log({\"model_eval\": model.eval()})\n",
    "# Saving the model to pth and adding it to the artifacts of the run, there is 5GB of memory on wandb, so we should be fine.\n",
    "torch.save(model.state_dict(), os.path.join(RESULT_DIRECTORY, pth))\n",
    "artifact = wandb.Artifact(\"SimplifiedLightweightCNN-model\", type=\"model\")\n",
    "artifact.add_file(os.path.join(RESULT_DIRECTORY, pth))\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "# Finish the run so it gets sent to the remote. You can discover the run right after that on the dashboard.\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408b6df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
