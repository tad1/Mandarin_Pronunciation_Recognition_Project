{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12f8129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:06:49.748097Z",
     "start_time": "2025-07-29T09:06:49.733744Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from data.source.pg_experiment import get_pg_experiment_dataframe\n",
    "import polars as pl\n",
    "\n",
    "from models.SimplifiedLightweightCNN import SimplifiedLightweightCNN\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport models.SimplifiedLightweightCNN\n",
    "from models.SimpleCNN_v2 import train, evaluate\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from path import RESULT_DIRECTORY\n",
    "import wandb\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a57adc8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:04:37.391376Z",
     "start_time": "2025-07-29T09:04:37.356941Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<sys>:0: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "<sys>:0: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_pg_experiment_dataset(): WARNING, Dropped 2 rows with missing files\n",
      "get_pg_experiment_dataset(): WARNING, Dropped 2 rows with missing files\n",
      "Number of unique words: 12\n",
      "Number of samples: 6016\n"
     ]
    }
   ],
   "source": [
    "df_pron, df_tone = get_pg_experiment_dataframe(\".ogg\")\n",
    "\n",
    "dataframe = df_pron.with_columns(word_id = pl.struct(\"word_id\").rank(\"dense\"))\n",
    "# dataframe = dataframe.filter(pl.col(\"mother\") == \"polish\")\n",
    "dataframe = dataframe.filter(pl.col(\"stage\") == 1)\n",
    "# df_stageI_polish = df_stageI_polish.filter(pl.col(\"word_id\") == 1)\n",
    "\n",
    "N_WORDS = dataframe.select(pl.col(\"word_id\").n_unique()).to_numpy()[0][0]\n",
    "print(f\"Number of unique words: {N_WORDS}\")\n",
    "print(f\"Number of samples: {dataframe.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a95dcb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:04:37.442136Z",
     "start_time": "2025-07-29T09:04:37.416770Z"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "def stratified_split(df: pl.DataFrame, label_col: str, second_label: str, train_frac=0.8, val_frac=0.1, seed=42) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n",
    "    classes = df.select(label_col).unique().to_series()\n",
    "    sub_classes = df.select(second_label).unique().to_series()\n",
    "    train_rows, val_rows, test_rows = [], [], []\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "    # ensure for training set that all classes have equal representation for each value\n",
    "    max_population = min(\n",
    "        df.filter((pl.col(label_col) == cls) & (pl.col(second_label) == subcls)).height\n",
    "        for cls in classes\n",
    "        for subcls in sub_classes\n",
    "    )\n",
    "\n",
    "    for cls in classes:\n",
    "        class_df = df.filter(pl.col(label_col) == cls)\n",
    "        for subcls in sub_classes:\n",
    "            subclass_df = class_df.filter(pl.col(second_label) == subcls)\n",
    "            n = subclass_df.height\n",
    "            indices = rng.permutation(n)\n",
    "\n",
    "            train_end = int(train_frac * max_population)\n",
    "            val_end = int((train_frac + val_frac) * n)\n",
    "\n",
    "            train_rows.append(subclass_df[indices[:train_end]])\n",
    "            val_rows.append(subclass_df[indices[train_end:val_end]])\n",
    "            test_rows.append(subclass_df[indices[val_end:]])\n",
    "\n",
    "    train_df = pl.concat(train_rows)\n",
    "    val_df = pl.concat(val_rows)\n",
    "    test_df = pl.concat(test_rows)\n",
    "\n",
    "    return train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd331ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:04:37.500687Z",
     "start_time": "2025-07-29T09:04:37.486859Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "from polars import DataFrame\n",
    "from dataset import Cast, TorchDataset\n",
    "from develop import reload_function, reload_module\n",
    "import pytorch_dataloader\n",
    "reload_module(pytorch_dataloader)\n",
    "from pytorch_dataloader import ReshapeCollate, build_collate_fn, PaddingCollate, DefaultCollate\n",
    "from functools import partial\n",
    "\n",
    "from transformation import Channels, RMSEnergy, TorchVadLogMelSpec, TorchVadMFCC, ZeroCrossingRate\n",
    "\n",
    "reload_function(TorchVadMFCC)\n",
    "\n",
    "TRAIN_SPLIT = 0.6\n",
    "VAL_SPLIT = 0.2\n",
    "TEST_SPLIT = 1 - TRAIN_SPLIT - VAL_SPLIT\n",
    "train_pl, val_pl, test_pl = stratified_split(dataframe, label_col=\"word_id\", second_label=\"value\", train_frac=TRAIN_SPLIT, val_frac=VAL_SPLIT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a0e8553",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set is perfectly balanced.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15253/3089078463.py:2: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  word_sizes = train_pl.group_by(\"word_id\").agg(pl.count().alias(\"size\"))\n"
     ]
    }
   ],
   "source": [
    "word_means = train_pl.group_by(\"word_id\").agg(pl.col(\"value\").mean().alias(\"mean\"))\n",
    "word_sizes = train_pl.group_by(\"word_id\").agg(pl.count().alias(\"size\"))\n",
    "\n",
    "assert all(mean == word_means[\"mean\"][0] for mean in word_means[\"mean\"]), \"Not all word_id mean values are the same.\"\n",
    "assert all(size == word_sizes[\"size\"][0] for size in word_sizes[\"size\"]), \"Not all word_id groups have equal size.\"\n",
    "\n",
    "print(\"Train set is perfectly balanced.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5cddf77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio import PROJECT_SAMPLING_RATE\n",
    "from transformation import TorchVadLogMelSpec, TorchVadWav2Vec2, VadAudio\n",
    "\n",
    "\n",
    "to_dataset: Callable[[DataFrame], TorchDataset] = lambda dataframe: TorchDataset(\n",
    "    # Cast(dataframe.get_column(\"rec_path\"), Channels(\"stack\",\"multiply\")(\n",
    "    #         TorchVadMFCC(),\n",
    "    #     )),\n",
    "    # Cast(dataframe.get_column(\"rec_path\"), VadAudio()),\n",
    "    Cast(dataframe.get_column(\"word_id\"), lambda x: torch.tensor(x-1, dtype=torch.long)),\n",
    "    Cast(dataframe.get_column(\"value\"), lambda x: torch.tensor(x).float()),\n",
    ")\n",
    "\n",
    "collate_fn = build_collate_fn(\n",
    "    # PaddingCollate(mode=\"SET_MAX_LEN\", max_len=80, pad_dim=2),\n",
    "    # PaddingCollate(mode=\"SET_MAX_LEN\", max_len=PROJECT_SAMPLING_RATE, pad_dim=0),\n",
    "    DefaultCollate(),\n",
    "    DefaultCollate(),\n",
    ")\n",
    "dataset_train = to_dataset(train_pl)\n",
    "dataset_val = to_dataset(val_pl)\n",
    "dataset_test = to_dataset(test_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be889768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "from pytorch_dataloader import MemoryLoadedDataLoader\n",
    "from os import name\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#note, if you are using Windows you MUST set `num_workers=0` - TL;DT multithreading DON'T work in notebooks because Windows DON'T have `fork()`\n",
    "num_workers = 0 if name == \"nt\" else 4\n",
    "train_loader = DataLoader(dataset_train, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=num_workers)\n",
    "val_loader = DataLoader(dataset_val, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
    "test_loader = DataLoader(dataset_test, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
    "\n",
    "for x in next(iter(train_loader)):\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf890d68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:05:23.032498Z",
     "start_time": "2025-07-29T09:04:37.522932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train loader into memory\n",
      "Loaded validation loader into memory\n"
     ]
    }
   ],
   "source": [
    "train_loader = MemoryLoadedDataLoader(train_loader, device=device)\n",
    "print(\"Loaded train loader into memory\")\n",
    "val_loader = MemoryLoadedDataLoader(val_loader, device=device)\n",
    "print(\"Loaded validation loader into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c91ba076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Guesser import ContextGuesser\n",
    "\n",
    "reload_function(ContextGuesser)\n",
    "model = ContextGuesser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a3a47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:25:42.755943Z",
     "start_time": "2025-07-29T09:22:19.522018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "updating run metadata (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">OldSimpleCNN</strong> at: <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/fooy5cle' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/fooy5cle</a><br> View project at: <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260114_003832-fooy5cle/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/src/wandb/run-20251021_172046-7c3zmj0x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/7c3zmj0x' target=\"_blank\">Tones ContextFusionCNN(16) 1L-NN</a></strong> to <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/7c3zmj0x' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/7c3zmj0x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6860, Train Acc: 0.5744, Val Loss: 0.6797, Val Acc: 0.6080\n",
      "Epoch 2, Train Loss: 0.6803, Train Acc: 0.5855, Val Loss: 0.6730, Val Acc: 0.6157\n",
      "Epoch 3, Train Loss: 0.6759, Train Acc: 0.5944, Val Loss: 0.6671, Val Acc: 0.6132\n",
      "Epoch 4, Train Loss: 0.6731, Train Acc: 0.5915, Val Loss: 0.6633, Val Acc: 0.6149\n",
      "Epoch 5, Train Loss: 0.6712, Train Acc: 0.5986, Val Loss: 0.6600, Val Acc: 0.6149\n",
      "Epoch 6, Train Loss: 0.6687, Train Acc: 0.5992, Val Loss: 0.6580, Val Acc: 0.6166\n",
      "Epoch 7, Train Loss: 0.6663, Train Acc: 0.5998, Val Loss: 0.6554, Val Acc: 0.6140\n",
      "Epoch 8, Train Loss: 0.6675, Train Acc: 0.6001, Val Loss: 0.6533, Val Acc: 0.6166\n",
      "Epoch 9, Train Loss: 0.6639, Train Acc: 0.6078, Val Loss: 0.6516, Val Acc: 0.6183\n",
      "Epoch 10, Train Loss: 0.6649, Train Acc: 0.6001, Val Loss: 0.6506, Val Acc: 0.6183\n",
      "Epoch 11, Train Loss: 0.6613, Train Acc: 0.6115, Val Loss: 0.6488, Val Acc: 0.6200\n",
      "Epoch 12, Train Loss: 0.6584, Train Acc: 0.6075, Val Loss: 0.6468, Val Acc: 0.6277\n",
      "Epoch 13, Train Loss: 0.6583, Train Acc: 0.6049, Val Loss: 0.6455, Val Acc: 0.6268\n",
      "Epoch 14, Train Loss: 0.6556, Train Acc: 0.6089, Val Loss: 0.6441, Val Acc: 0.6302\n",
      "Epoch 15, Train Loss: 0.6514, Train Acc: 0.6155, Val Loss: 0.6422, Val Acc: 0.6379\n",
      "Epoch 16, Train Loss: 0.6551, Train Acc: 0.6157, Val Loss: 0.6410, Val Acc: 0.6277\n",
      "Epoch 17, Train Loss: 0.6528, Train Acc: 0.6146, Val Loss: 0.6395, Val Acc: 0.6413\n",
      "Epoch 18, Train Loss: 0.6498, Train Acc: 0.6209, Val Loss: 0.6379, Val Acc: 0.6354\n",
      "Epoch 19, Train Loss: 0.6439, Train Acc: 0.6177, Val Loss: 0.6351, Val Acc: 0.6371\n",
      "Epoch 20, Train Loss: 0.6486, Train Acc: 0.6197, Val Loss: 0.6349, Val Acc: 0.6379\n",
      "Epoch 21, Train Loss: 0.6478, Train Acc: 0.6172, Val Loss: 0.6345, Val Acc: 0.6345\n",
      "Epoch 22, Train Loss: 0.6400, Train Acc: 0.6212, Val Loss: 0.6314, Val Acc: 0.6396\n",
      "Epoch 23, Train Loss: 0.6447, Train Acc: 0.6246, Val Loss: 0.6307, Val Acc: 0.6422\n",
      "Epoch 24, Train Loss: 0.6406, Train Acc: 0.6331, Val Loss: 0.6295, Val Acc: 0.6371\n",
      "Epoch 25, Train Loss: 0.6436, Train Acc: 0.6203, Val Loss: 0.6278, Val Acc: 0.6405\n",
      "Epoch 26, Train Loss: 0.6318, Train Acc: 0.6371, Val Loss: 0.6270, Val Acc: 0.6422\n",
      "Epoch 27, Train Loss: 0.6331, Train Acc: 0.6394, Val Loss: 0.6220, Val Acc: 0.6388\n",
      "Epoch 28, Train Loss: 0.6315, Train Acc: 0.6414, Val Loss: 0.6225, Val Acc: 0.6473\n",
      "Epoch 29, Train Loss: 0.6348, Train Acc: 0.6354, Val Loss: 0.6207, Val Acc: 0.6456\n",
      "Epoch 30, Train Loss: 0.6284, Train Acc: 0.6414, Val Loss: 0.6189, Val Acc: 0.6371\n",
      "Epoch 31, Train Loss: 0.6291, Train Acc: 0.6448, Val Loss: 0.6187, Val Acc: 0.6447\n",
      "Epoch 32, Train Loss: 0.6263, Train Acc: 0.6422, Val Loss: 0.6172, Val Acc: 0.6499\n",
      "Epoch 33, Train Loss: 0.6219, Train Acc: 0.6431, Val Loss: 0.6162, Val Acc: 0.6533\n",
      "Epoch 34, Train Loss: 0.6273, Train Acc: 0.6460, Val Loss: 0.6133, Val Acc: 0.6558\n",
      "Epoch 35, Train Loss: 0.6220, Train Acc: 0.6522, Val Loss: 0.6114, Val Acc: 0.6507\n",
      "Epoch 36, Train Loss: 0.6269, Train Acc: 0.6417, Val Loss: 0.6114, Val Acc: 0.6516\n",
      "Epoch 37, Train Loss: 0.6192, Train Acc: 0.6497, Val Loss: 0.6112, Val Acc: 0.6473\n",
      "Epoch 38, Train Loss: 0.6201, Train Acc: 0.6422, Val Loss: 0.6102, Val Acc: 0.6447\n",
      "Epoch 39, Train Loss: 0.6175, Train Acc: 0.6599, Val Loss: 0.6063, Val Acc: 0.6465\n",
      "Epoch 40, Train Loss: 0.6163, Train Acc: 0.6474, Val Loss: 0.6052, Val Acc: 0.6490\n",
      "Epoch 41, Train Loss: 0.6216, Train Acc: 0.6559, Val Loss: 0.6072, Val Acc: 0.6490\n",
      "Epoch 42, Train Loss: 0.6141, Train Acc: 0.6465, Val Loss: 0.6066, Val Acc: 0.6533\n",
      "Epoch 43, Train Loss: 0.6100, Train Acc: 0.6482, Val Loss: 0.6021, Val Acc: 0.6430\n",
      "Epoch 44, Train Loss: 0.6082, Train Acc: 0.6682, Val Loss: 0.6011, Val Acc: 0.6456\n",
      "Epoch 45, Train Loss: 0.6106, Train Acc: 0.6465, Val Loss: 0.5984, Val Acc: 0.6533\n",
      "Epoch 46, Train Loss: 0.6035, Train Acc: 0.6670, Val Loss: 0.5988, Val Acc: 0.6584\n",
      "Epoch 47, Train Loss: 0.6096, Train Acc: 0.6613, Val Loss: 0.5960, Val Acc: 0.6576\n",
      "Epoch 48, Train Loss: 0.5963, Train Acc: 0.6770, Val Loss: 0.5935, Val Acc: 0.6627\n",
      "Epoch 49, Train Loss: 0.5931, Train Acc: 0.6747, Val Loss: 0.5916, Val Acc: 0.6678\n",
      "Epoch 50, Train Loss: 0.6076, Train Acc: 0.6705, Val Loss: 0.5898, Val Acc: 0.6687\n",
      "Epoch 51, Train Loss: 0.5987, Train Acc: 0.6645, Val Loss: 0.5905, Val Acc: 0.6644\n",
      "Epoch 52, Train Loss: 0.5982, Train Acc: 0.6625, Val Loss: 0.5917, Val Acc: 0.6678\n",
      "Epoch 53, Train Loss: 0.5969, Train Acc: 0.6773, Val Loss: 0.5907, Val Acc: 0.6678\n",
      "Epoch 54, Train Loss: 0.5942, Train Acc: 0.6856, Val Loss: 0.5898, Val Acc: 0.6670\n",
      "Epoch 55, Train Loss: 0.6005, Train Acc: 0.6670, Val Loss: 0.5872, Val Acc: 0.6772\n",
      "Epoch 56, Train Loss: 0.5922, Train Acc: 0.6773, Val Loss: 0.5850, Val Acc: 0.6721\n",
      "Epoch 57, Train Loss: 0.5819, Train Acc: 0.6847, Val Loss: 0.5873, Val Acc: 0.6661\n",
      "Epoch 58, Train Loss: 0.5919, Train Acc: 0.6810, Val Loss: 0.5818, Val Acc: 0.6815\n",
      "Epoch 59, Train Loss: 0.5810, Train Acc: 0.6890, Val Loss: 0.5821, Val Acc: 0.6755\n",
      "Epoch 60, Train Loss: 0.5896, Train Acc: 0.6930, Val Loss: 0.5895, Val Acc: 0.6729\n",
      "Epoch 61, Train Loss: 0.5872, Train Acc: 0.6861, Val Loss: 0.5841, Val Acc: 0.6661\n",
      "Epoch 62, Train Loss: 0.5876, Train Acc: 0.6904, Val Loss: 0.5802, Val Acc: 0.6712\n",
      "Epoch 63, Train Loss: 0.5792, Train Acc: 0.6853, Val Loss: 0.5757, Val Acc: 0.6798\n",
      "Epoch 64, Train Loss: 0.5781, Train Acc: 0.6987, Val Loss: 0.5807, Val Acc: 0.6789\n",
      "Epoch 65, Train Loss: 0.5778, Train Acc: 0.6978, Val Loss: 0.5752, Val Acc: 0.6746\n",
      "Epoch 66, Train Loss: 0.5831, Train Acc: 0.6956, Val Loss: 0.5722, Val Acc: 0.6917\n",
      "Epoch 67, Train Loss: 0.5731, Train Acc: 0.6995, Val Loss: 0.5703, Val Acc: 0.6832\n",
      "Epoch 68, Train Loss: 0.5692, Train Acc: 0.7032, Val Loss: 0.5789, Val Acc: 0.6652\n",
      "Epoch 69, Train Loss: 0.5756, Train Acc: 0.6893, Val Loss: 0.5719, Val Acc: 0.6798\n",
      "Epoch 70, Train Loss: 0.5679, Train Acc: 0.7018, Val Loss: 0.5744, Val Acc: 0.6746\n",
      "Epoch 71, Train Loss: 0.5640, Train Acc: 0.7098, Val Loss: 0.5702, Val Acc: 0.6746\n",
      "Epoch 72, Train Loss: 0.5695, Train Acc: 0.6995, Val Loss: 0.5694, Val Acc: 0.6866\n",
      "Epoch 73, Train Loss: 0.5719, Train Acc: 0.7015, Val Loss: 0.5679, Val Acc: 0.6866\n",
      "Epoch 74, Train Loss: 0.5699, Train Acc: 0.7050, Val Loss: 0.5669, Val Acc: 0.6951\n",
      "Epoch 75, Train Loss: 0.5651, Train Acc: 0.7058, Val Loss: 0.5670, Val Acc: 0.6789\n",
      "Epoch 76, Train Loss: 0.5701, Train Acc: 0.7058, Val Loss: 0.5692, Val Acc: 0.6849\n",
      "Epoch 77, Train Loss: 0.5628, Train Acc: 0.6984, Val Loss: 0.5699, Val Acc: 0.6823\n",
      "Epoch 78, Train Loss: 0.5588, Train Acc: 0.7141, Val Loss: 0.5698, Val Acc: 0.6840\n",
      "Epoch 79, Train Loss: 0.5615, Train Acc: 0.6970, Val Loss: 0.5661, Val Acc: 0.6849\n",
      "Epoch 80, Train Loss: 0.5433, Train Acc: 0.7189, Val Loss: 0.5705, Val Acc: 0.6857\n",
      "Epoch 81, Train Loss: 0.5533, Train Acc: 0.7166, Val Loss: 0.5583, Val Acc: 0.6994\n",
      "Epoch 82, Train Loss: 0.5576, Train Acc: 0.7158, Val Loss: 0.5679, Val Acc: 0.6892\n",
      "Epoch 83, Train Loss: 0.5528, Train Acc: 0.7121, Val Loss: 0.5698, Val Acc: 0.6934\n",
      "Epoch 84, Train Loss: 0.5459, Train Acc: 0.7266, Val Loss: 0.5674, Val Acc: 0.6951\n",
      "Epoch 85, Train Loss: 0.5455, Train Acc: 0.7272, Val Loss: 0.5716, Val Acc: 0.6883\n",
      "Epoch 86, Train Loss: 0.5462, Train Acc: 0.7189, Val Loss: 0.5607, Val Acc: 0.6977\n",
      "Epoch 87, Train Loss: 0.5390, Train Acc: 0.7229, Val Loss: 0.5608, Val Acc: 0.6951\n",
      "Epoch 88, Train Loss: 0.5375, Train Acc: 0.7218, Val Loss: 0.5578, Val Acc: 0.6960\n",
      "Epoch 89, Train Loss: 0.5350, Train Acc: 0.7269, Val Loss: 0.5577, Val Acc: 0.6934\n",
      "Epoch 90, Train Loss: 0.5355, Train Acc: 0.7246, Val Loss: 0.5531, Val Acc: 0.7079\n",
      "Epoch 91, Train Loss: 0.5316, Train Acc: 0.7278, Val Loss: 0.5549, Val Acc: 0.7003\n",
      "Epoch 92, Train Loss: 0.5330, Train Acc: 0.7303, Val Loss: 0.5591, Val Acc: 0.6934\n",
      "Epoch 93, Train Loss: 0.5348, Train Acc: 0.7298, Val Loss: 0.5628, Val Acc: 0.6951\n",
      "Epoch 94, Train Loss: 0.5284, Train Acc: 0.7412, Val Loss: 0.5567, Val Acc: 0.6951\n",
      "Epoch 95, Train Loss: 0.5275, Train Acc: 0.7406, Val Loss: 0.5581, Val Acc: 0.6951\n",
      "Epoch 96, Train Loss: 0.5312, Train Acc: 0.7403, Val Loss: 0.5623, Val Acc: 0.6994\n",
      "Epoch 97, Train Loss: 0.5197, Train Acc: 0.7360, Val Loss: 0.5546, Val Acc: 0.7003\n",
      "Epoch 98, Train Loss: 0.5276, Train Acc: 0.7434, Val Loss: 0.5573, Val Acc: 0.7037\n",
      "Epoch 99, Train Loss: 0.5187, Train Acc: 0.7457, Val Loss: 0.5552, Val Acc: 0.7011\n",
      "Epoch 100, Train Loss: 0.5265, Train Acc: 0.7395, Val Loss: 0.5533, Val Acc: 0.7011\n",
      "Epoch 101, Train Loss: 0.5240, Train Acc: 0.7389, Val Loss: 0.5570, Val Acc: 0.6934\n",
      "Epoch 102, Train Loss: 0.5166, Train Acc: 0.7420, Val Loss: 0.5556, Val Acc: 0.6968\n",
      "Epoch 103, Train Loss: 0.5195, Train Acc: 0.7392, Val Loss: 0.5550, Val Acc: 0.7020\n",
      "Epoch 104, Train Loss: 0.5212, Train Acc: 0.7466, Val Loss: 0.5552, Val Acc: 0.7028\n",
      "Epoch 105, Train Loss: 0.5252, Train Acc: 0.7429, Val Loss: 0.5538, Val Acc: 0.7028\n",
      "Epoch 106, Train Loss: 0.5231, Train Acc: 0.7449, Val Loss: 0.5549, Val Acc: 0.7003\n",
      "Epoch 107, Train Loss: 0.5256, Train Acc: 0.7369, Val Loss: 0.5550, Val Acc: 0.7003\n",
      "Epoch 108, Train Loss: 0.5252, Train Acc: 0.7389, Val Loss: 0.5548, Val Acc: 0.6943\n",
      "Epoch 109, Train Loss: 0.5160, Train Acc: 0.7457, Val Loss: 0.5552, Val Acc: 0.6977\n",
      "Epoch 110, Train Loss: 0.5172, Train Acc: 0.7511, Val Loss: 0.5555, Val Acc: 0.7003\n",
      "Epoch 111, Train Loss: 0.5126, Train Acc: 0.7497, Val Loss: 0.5569, Val Acc: 0.6985\n",
      "Epoch 112, Train Loss: 0.5171, Train Acc: 0.7503, Val Loss: 0.5554, Val Acc: 0.7011\n",
      "Epoch 113, Train Loss: 0.5201, Train Acc: 0.7432, Val Loss: 0.5554, Val Acc: 0.6977\n",
      "Epoch 114, Train Loss: 0.5228, Train Acc: 0.7372, Val Loss: 0.5544, Val Acc: 0.6994\n",
      "Epoch 115, Train Loss: 0.5195, Train Acc: 0.7469, Val Loss: 0.5542, Val Acc: 0.7011\n",
      "Epoch 116, Train Loss: 0.5184, Train Acc: 0.7420, Val Loss: 0.5551, Val Acc: 0.6977\n",
      "Epoch 117, Train Loss: 0.5099, Train Acc: 0.7503, Val Loss: 0.5545, Val Acc: 0.6977\n",
      "Epoch 118, Train Loss: 0.5089, Train Acc: 0.7489, Val Loss: 0.5548, Val Acc: 0.6968\n",
      "Epoch 119, Train Loss: 0.5141, Train Acc: 0.7469, Val Loss: 0.5548, Val Acc: 0.6994\n",
      "Epoch 120, Train Loss: 0.5042, Train Acc: 0.7583, Val Loss: 0.5551, Val Acc: 0.6968\n",
      "Epoch 121, Train Loss: 0.5176, Train Acc: 0.7395, Val Loss: 0.5549, Val Acc: 0.6977\n",
      "Epoch 122, Train Loss: 0.5099, Train Acc: 0.7457, Val Loss: 0.5557, Val Acc: 0.6994\n",
      "Epoch 123, Train Loss: 0.5063, Train Acc: 0.7529, Val Loss: 0.5546, Val Acc: 0.6994\n",
      "Epoch 124, Train Loss: 0.5115, Train Acc: 0.7534, Val Loss: 0.5553, Val Acc: 0.6968\n",
      "Epoch 125, Train Loss: 0.5131, Train Acc: 0.7437, Val Loss: 0.5563, Val Acc: 0.6960\n",
      "Epoch 126, Train Loss: 0.5099, Train Acc: 0.7466, Val Loss: 0.5552, Val Acc: 0.6968\n",
      "Epoch 127, Train Loss: 0.5059, Train Acc: 0.7560, Val Loss: 0.5550, Val Acc: 0.6994\n",
      "Epoch 128, Train Loss: 0.5161, Train Acc: 0.7466, Val Loss: 0.5553, Val Acc: 0.6985\n",
      "Epoch 129, Train Loss: 0.5020, Train Acc: 0.7597, Val Loss: 0.5558, Val Acc: 0.6985\n",
      "Epoch 130, Train Loss: 0.5074, Train Acc: 0.7583, Val Loss: 0.5553, Val Acc: 0.7003\n",
      "Epoch 131, Train Loss: 0.5123, Train Acc: 0.7554, Val Loss: 0.5548, Val Acc: 0.7037\n",
      "Epoch 132, Train Loss: 0.5091, Train Acc: 0.7586, Val Loss: 0.5553, Val Acc: 0.6994\n",
      "Epoch 133, Train Loss: 0.5202, Train Acc: 0.7474, Val Loss: 0.5561, Val Acc: 0.6943\n",
      "Epoch 134, Train Loss: 0.5199, Train Acc: 0.7420, Val Loss: 0.5558, Val Acc: 0.6985\n",
      "Epoch 135, Train Loss: 0.5231, Train Acc: 0.7449, Val Loss: 0.5552, Val Acc: 0.6994\n",
      "Epoch 136, Train Loss: 0.5169, Train Acc: 0.7474, Val Loss: 0.5556, Val Acc: 0.6977\n",
      "Epoch 137, Train Loss: 0.5079, Train Acc: 0.7531, Val Loss: 0.5556, Val Acc: 0.6994\n",
      "Epoch 138, Train Loss: 0.5177, Train Acc: 0.7460, Val Loss: 0.5548, Val Acc: 0.6977\n",
      "Epoch 139, Train Loss: 0.5103, Train Acc: 0.7500, Val Loss: 0.5562, Val Acc: 0.6968\n",
      "Epoch 140, Train Loss: 0.5109, Train Acc: 0.7554, Val Loss: 0.5555, Val Acc: 0.6977\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▅▅▇▇▇███████</td></tr><tr><td>val_loss</td><td>█▇▆▅▅▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>model_eval</td><td>ContextFusionCNN(<br>  ...</td></tr><tr><td>train_acc</td><td>0.68682</td></tr><tr><td>train_loss</td><td>1.20786</td></tr><tr><td>val_acc</td><td>0.65004</td></tr><tr><td>val_loss</td><td>1.22477</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Tones ContextFusionCNN(16) 1L-NN</strong> at: <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/7c3zmj0x' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/7c3zmj0x</a><br> View project at: <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_172046-7c3zmj0x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model variables definition.\n",
    "pth = \"SimplifiedLightweightCNN.pth\"\n",
    "lr = 1e-4  # Reduce from 1e-3\n",
    "epochs = 140\n",
    "model = model.to(device)\n",
    "reload_function(train)\n",
    "reload_function(evaluate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)  # Add L2 regularization\n",
    "# Add learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # name of the run\n",
    "    name=\"Guesser on perfectly balanced train\",\n",
    "    config={\n",
    "        \"Name\": 'SimplifiedLightweightCNN',\n",
    "        \"learning_rate\": lr,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"criterion\": \"BCELoss\",\n",
    "        \"architecture\": \"SimplifiedLightweightCNN\",\n",
    "        \"architecture_details\": str(model),\n",
    "        \"dataset\": \"Stage-I\",\n",
    "        \"train_val_test(%)\": f'{TRAIN_SPLIT}-{VAL_SPLIT}-{TEST_SPLIT}',\n",
    "        \"epochs\": epochs,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    # Logging the metadata for each epoch so that the charts can be generated on the dashboard\n",
    "    run.log({\"train_acc\": train_acc, \"train_loss\": train_loss, \"val_acc\": val_acc, \"val_loss\": val_loss, })\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "run.log({\"model_eval\": model.eval()})\n",
    "# Saving the model to pth and adding it to the artifacts of the run, there is 5GB of memory on wandb, so we should be fine.\n",
    "torch.save(model.state_dict(), os.path.join(RESULT_DIRECTORY, pth))\n",
    "artifact = wandb.Artifact(\"SimplifiedLightweightCNN-model\", type=\"model\")\n",
    "artifact.add_file(os.path.join(RESULT_DIRECTORY, pth))\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "# Finish the run so it gets sent to the remote. You can discover the run right after that on the dashboard.\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408b6df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
