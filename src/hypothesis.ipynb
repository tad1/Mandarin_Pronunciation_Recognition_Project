{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f8129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:06:49.748097Z",
     "start_time": "2025-07-29T09:06:49.733744Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from data.source.pg_experiment import get_pg_experiment_dataframe\n",
    "import polars as pl\n",
    "\n",
    "from models.SimplifiedLightweightCNN import SimplifiedLightweightCNN\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport models.SimplifiedLightweightCNN\n",
    "from models.SimpleCNN_v2 import train, evaluate\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from path import RESULT_DIRECTORY\n",
    "import wandb\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57adc8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:04:37.391376Z",
     "start_time": "2025-07-29T09:04:37.356941Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pronv1, _ = get_pg_experiment_dataframe(\".ogg\", assesment_version=\"v1\")\n",
    "df_pronv2, _ = get_pg_experiment_dataframe(\".ogg\", assesment_version=\"v2\")\n",
    "\n",
    "dataframe = df_pronv1.join(df_pronv2, on=[\"id_student\", \"word_id\"], how=\"inner\", suffix=\"_v2\")\n",
    "dataframe = dataframe.rename({\"value\": \"v1_value\", \"value_v2\": \"v2_value\"})\n",
    "dataframe = dataframe.with_columns(word_id = pl.struct(\"word_id\").rank(\"dense\"))\n",
    "dataframe = dataframe.filter(pl.col(\"stage\") == 1)\n",
    "\n",
    "df_outer = dataframe.filter(pl.col(\"v1_value\") != pl.col(\"v2_value\"))\n",
    "df_inner = dataframe.filter(pl.col(\"v1_value\") == pl.col(\"v2_value\"))\n",
    "\n",
    "\n",
    "\n",
    "N_WORDS = dataframe.select(pl.col(\"word_id\").n_unique()).to_numpy()[0][0]\n",
    "print(f\"Number of unique words: {N_WORDS}\")\n",
    "print(f\"Number of samples: {dataframe.shape[0]}\")\n",
    "print(f\"Samples with v1_value != v2_value: {df_outer.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95dcb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:04:37.442136Z",
     "start_time": "2025-07-29T09:04:37.416770Z"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "def split(df_outer: pl.DataFrame, df_inner: pl.DataFrame, label_col: str, train_frac=0.8, val_frac=0.1, seed=42) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n",
    "    classes = df_outer.select(label_col).unique().to_series()\n",
    "    train_rows, val_rows, test_rows = [], [], []\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    # test_rows, get random from df_inner of size df_outer * (1 - train_frac - val_frac)\n",
    "    \n",
    "    n_subset = df_inner.height\n",
    "    n_test = int((1 - train_frac - val_frac) * df_outer.height)\n",
    "    indices_subset = rng.permutation(n_subset)\n",
    "    test_rows.append(df_inner[indices_subset[:n_test]])\n",
    "    \n",
    "    df = pl.concat([df_outer, df_inner[indices_subset[n_test:]]])\n",
    "    \n",
    "    # update fractions\n",
    "    total_frac = train_frac + val_frac\n",
    "    train_frac = train_frac / total_frac\n",
    "    val_frac = val_frac / total_frac\n",
    "\n",
    "\n",
    "    for cls in classes:\n",
    "        class_df = df.filter(pl.col(label_col) == cls)\n",
    "        n = class_df.height\n",
    "        indices = rng.permutation(n)\n",
    "\n",
    "        train_end = int(train_frac * n)\n",
    "        val_end = int((train_frac + val_frac) * n)\n",
    "\n",
    "        train_rows.append(class_df[indices[:train_end]])\n",
    "        val_rows.append(class_df[indices[train_end:val_end]])\n",
    "\n",
    "    train_df = pl.concat(train_rows)\n",
    "    val_df = pl.concat(val_rows)\n",
    "    test_df = pl.concat(test_rows)\n",
    "\n",
    "    return train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd331ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:04:37.500687Z",
     "start_time": "2025-07-29T09:04:37.486859Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "from polars import DataFrame\n",
    "from dataset import Cast, TorchDataset\n",
    "from develop import reload_function, reload_module\n",
    "import pytorch_dataloader\n",
    "reload_module(pytorch_dataloader)\n",
    "from pytorch_dataloader import ReshapeCollate, build_collate_fn, PaddingCollate, DefaultCollate\n",
    "from functools import partial\n",
    "\n",
    "from transformation import Channels, RMSEnergy, TorchVadLogMelSpec, TorchVadMFCC, ZeroCrossingRate\n",
    "\n",
    "reload_function(TorchVadMFCC)\n",
    "\n",
    "TRAIN_SPLIT = 0.6\n",
    "VAL_SPLIT = 0.2\n",
    "TEST_SPLIT = 1 - TRAIN_SPLIT - VAL_SPLIT\n",
    "train_pl, val_pl, test_pl = split(df_outer=df_outer, df_inner=df_inner, label_col=\"word_id\", train_frac=TRAIN_SPLIT, val_frac=VAL_SPLIT)\n",
    "val_pl = val_pl.filter(pl.col(\"v1_value\") == pl.col(\"v2_value\"))\n",
    "test_pl = test_pl.filter(pl.col(\"v1_value\") == pl.col(\"v2_value\"))\n",
    "\n",
    "to_dataset: Callable[[DataFrame], TorchDataset] = lambda dataframe: TorchDataset(\n",
    "    Cast(dataframe.get_column(\"rec_path\"), Channels(\"stack\",\"multiply\")(\n",
    "            TorchVadMFCC(delta=0),\n",
    "        )),\n",
    "    Cast(dataframe.get_column(\"word_id\"), lambda x: torch.tensor(x-1, dtype=torch.long)),\n",
    "    Cast(dataframe.get_column(\"v1_value\"), lambda x: torch.tensor(x).float()),\n",
    "    Cast(dataframe.get_column(\"v2_value\"), lambda x: torch.tensor(x).float()),\n",
    ")\n",
    "\n",
    "collate_fn = build_collate_fn(\n",
    "    PaddingCollate(mode=\"SET_MAX_LEN\", max_len=80, pad_dim=2),\n",
    "    DefaultCollate(),\n",
    "    DefaultCollate(),\n",
    "    DefaultCollate(),\n",
    ")\n",
    "dataset_train = to_dataset(train_pl)\n",
    "dataset_val = to_dataset(val_pl)\n",
    "dataset_test = to_dataset(test_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be889768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_dataloader import MemoryLoadedDataLoader\n",
    "from os import name\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#note, if you are using Windows you MUST set `num_workers=0` - TL;DT multithreading DON'T work in notebooks because Windows DON'T have `fork()`\n",
    "num_workers = 0 if name == \"nt\" else 4\n",
    "train_loader = DataLoader(dataset_train, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=num_workers)\n",
    "val_loader = DataLoader(dataset_val, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
    "test_loader = DataLoader(dataset_test, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
    "\n",
    "for x in next(iter(train_loader)):\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf890d68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:05:23.032498Z",
     "start_time": "2025-07-29T09:04:37.522932Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = MemoryLoadedDataLoader(train_loader, device=device)\n",
    "print(\"Loaded train loader into memory\")\n",
    "val_loader = MemoryLoadedDataLoader(val_loader, device=device)\n",
    "print(\"Loaded validation loader into memory\")\n",
    "test_loader = MemoryLoadedDataLoader(test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ba076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.FusionCNN import ContextFusionCNN\n",
    "reload_function(ContextFusionCNN)\n",
    "model = ContextFusionCNN(1, num_words=N_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a3a47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:25:42.755943Z",
     "start_time": "2025-07-29T09:22:19.522018Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model variables definition.\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "TRAIN_MODE : Literal[\"zerov1\", \"zerov2\", \"double_zerov1\", \"double_zerov2\", \"sequential\", \"interleave\"] = \"interleave\"\n",
    "pth = \"SimplifiedLightweightCNN.pth\"\n",
    "lr = 1e-4  # Reduce from 1e-3\n",
    "epochs = 140\n",
    "model = model.to(device)\n",
    "reload_function(train)\n",
    "reload_function(evaluate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)  # Add L2 regularization\n",
    "# Add learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # name of the run\n",
    "    name=f\"hypothesis 14 - {TRAIN_MODE}\",\n",
    "    config={\n",
    "        \"Name\": 'SimplifiedLightweightCNN',\n",
    "        \"learning_rate\": lr,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"criterion\": \"BCELoss\",\n",
    "        \"architecture\": \"SimplifiedLightweightCNN\",\n",
    "        \"architecture_details\": str(model),\n",
    "        \"dataset\": \"Stage-I\",\n",
    "        \"train_val_test(%)\": f'{TRAIN_SPLIT}-{VAL_SPLIT}-{TEST_SPLIT}',\n",
    "        \"epochs\": epochs,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "configs  = {\n",
    "    \"interleave\": ([\"v1\",\"v2\"], True),\n",
    "    \"sequential\": ([\"v1\",\"v2\"], False),\n",
    "    \"zerov1\": ([\"v1\",\"v1\"], False),\n",
    "    \"zerov2\": ([\"v2\",\"v2\"], False),\n",
    "    \"double_zerov1\": ([\"v1\",\"v1\"], False),\n",
    "    \"double_zerov2\": ([\"v2\",\"v2\"], False),\n",
    "}\n",
    "label_versions, interleave_labels = configs[TRAIN_MODE]\n",
    "for epoch in range(epochs):\n",
    "    for label_version in label_versions:\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device, label_version=label_version, interleave_labels=interleave_labels)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device, label_version=label_version, interleave_labels=interleave_labels)\n",
    "        scheduler.step(val_loss)\n",
    "        # Logging the metadata for each epoch so that the charts can be generated on the dashboard\n",
    "    run.log({\"train_acc\": train_acc, \"train_loss\": train_loss, \"val_acc\": val_acc, \"val_loss\": val_loss, })\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "run.log({\"model_eval\": model.eval()})\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "run.log({\"test_acc\": test_acc, \"test_loss\": test_loss})\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "# Saving the model to pth and adding it to the artifacts of the run, there is 5GB of memory on wandb, so we should be fine.\n",
    "torch.save(model.state_dict(), os.path.join(RESULT_DIRECTORY, pth))\n",
    "artifact = wandb.Artifact(\"SimplifiedLightweightCNN-model\", type=\"model\")\n",
    "artifact.add_file(os.path.join(RESULT_DIRECTORY, pth))\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "# Finish the run so it gets sent to the remote. You can discover the run right after that on the dashboard.\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408b6df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mandarin_Pronunciation_Recognition_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
