{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12f8129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:06:49.748097Z",
     "start_time": "2025-07-29T09:06:49.733744Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from data.source.pg_experiment import get_pg_experiment_dataframe\n",
    "import polars as pl\n",
    "\n",
    "from models.SimplifiedLightweightCNN import SimplifiedLightweightCNN\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport models.SimplifiedLightweightCNN\n",
    "from models.SimpleCNN_v2 import train, evaluate\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from path import RESULT_DIRECTORY\n",
    "import wandb\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a57adc8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:04:37.391376Z",
     "start_time": "2025-07-29T09:04:37.356941Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<sys>:0: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "<sys>:0: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_pg_experiment_dataset(): WARNING, Dropped 2 rows with missing files\n",
      "get_pg_experiment_dataset(): WARNING, Dropped 2 rows with missing files\n",
      "Number of unique words: 12\n",
      "Number of samples: 6016\n"
     ]
    }
   ],
   "source": [
    "df_pron, df_tone = get_pg_experiment_dataframe(\".ogg\")\n",
    "\n",
    "dataframe = df_pron.with_columns(word_id = pl.struct(\"word_id\").rank(\"dense\"))\n",
    "# dataframe = dataframe.filter(pl.col(\"mother\") == \"polish\")\n",
    "dataframe = dataframe.filter(pl.col(\"stage\") == 1)\n",
    "# df_stageI_polish = df_stageI_polish.filter(pl.col(\"word_id\") == 1)\n",
    "\n",
    "N_WORDS = dataframe.select(pl.col(\"word_id\").n_unique()).to_numpy()[0][0]\n",
    "print(f\"Number of unique words: {N_WORDS}\")\n",
    "print(f\"Number of samples: {dataframe.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a95dcb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:04:37.442136Z",
     "start_time": "2025-07-29T09:04:37.416770Z"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "def stratified_split(df: pl.DataFrame, label_col: str, second_label: str, train_frac=0.8, val_frac=0.1, seed=42) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n",
    "    classes = df.select(label_col).unique().to_series()\n",
    "    sub_classes = df.select(second_label).unique().to_series()\n",
    "    train_rows, val_rows, test_rows = [], [], []\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "    # ensure for training set that all classes have equal representation for each value\n",
    "    max_population = min(\n",
    "        df.filter((pl.col(label_col) == cls) & (pl.col(second_label) == subcls)).height\n",
    "        for cls in classes\n",
    "        for subcls in sub_classes\n",
    "    )\n",
    "\n",
    "    for cls in classes:\n",
    "        class_df = df.filter(pl.col(label_col) == cls)\n",
    "        for subcls in sub_classes:\n",
    "            subclass_df = class_df.filter(pl.col(second_label) == subcls)\n",
    "            n = subclass_df.height\n",
    "            indices = rng.permutation(n)\n",
    "\n",
    "            train_end = int(train_frac * max_population)\n",
    "            val_end = int((train_frac + val_frac) * n)\n",
    "\n",
    "            train_rows.append(subclass_df[indices[:train_end]])\n",
    "            val_rows.append(subclass_df[indices[train_end:val_end]])\n",
    "            test_rows.append(subclass_df[indices[val_end:]])\n",
    "\n",
    "    train_df = pl.concat(train_rows)\n",
    "    val_df = pl.concat(val_rows)\n",
    "    test_df = pl.concat(test_rows)\n",
    "\n",
    "    return train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd331ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:04:37.500687Z",
     "start_time": "2025-07-29T09:04:37.486859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1032\n",
      "Validation dataset size: 3640\n",
      "Test dataset size: 1183\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "from polars import DataFrame\n",
    "from dataset import Cast, TorchDataset\n",
    "from develop import reload_function, reload_module\n",
    "import pytorch_dataloader\n",
    "reload_module(pytorch_dataloader)\n",
    "from pytorch_dataloader import ReshapeCollate, build_collate_fn, PaddingCollate, DefaultCollate\n",
    "from functools import partial\n",
    "\n",
    "from transformation import Channels, RMSEnergy, TorchVadLogMelSpec, TorchVadMFCC, ZeroCrossingRate\n",
    "\n",
    "reload_function(TorchVadMFCC)\n",
    "\n",
    "TRAIN_SPLIT = 0.6\n",
    "VAL_SPLIT = 0.2\n",
    "TEST_SPLIT = 1 - TRAIN_SPLIT - VAL_SPLIT\n",
    "train_pl, val_pl, test_pl = stratified_split(dataframe, label_col=\"word_id\", second_label=\"value\", train_frac=TRAIN_SPLIT, val_frac=VAL_SPLIT)\n",
    "\n",
    "print(f\"Dataframe size: {dataframe.shape[0]}\")\n",
    "print(f\"Train dataset size: {train_pl.shape[0]}\")\n",
    "print(f\"Validation dataset size: {val_pl.shape[0]}\")\n",
    "print(f\"Test dataset size: {test_pl.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a0e8553",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set is perfectly balanced.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15253/3089078463.py:2: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  word_sizes = train_pl.group_by(\"word_id\").agg(pl.count().alias(\"size\"))\n"
     ]
    }
   ],
   "source": [
    "word_means = train_pl.group_by(\"word_id\").agg(pl.col(\"value\").mean().alias(\"mean\"))\n",
    "word_sizes = train_pl.group_by(\"word_id\").agg(pl.count().alias(\"size\"))\n",
    "\n",
    "assert all(mean == word_means[\"mean\"][0] for mean in word_means[\"mean\"]), \"Not all word_id mean values are the same.\"\n",
    "assert all(size == word_sizes[\"size\"][0] for size in word_sizes[\"size\"]), \"Not all word_id groups have equal size.\"\n",
    "\n",
    "print(\"Train set is perfectly balanced.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5cddf77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio import PROJECT_SAMPLING_RATE\n",
    "from transformation import TorchVadLogMelSpec, TorchVadWav2Vec2, VadAudio\n",
    "\n",
    "\n",
    "to_dataset: Callable[[DataFrame], TorchDataset] = lambda dataframe: TorchDataset(\n",
    "    Cast(dataframe.get_column(\"rec_path\"), Channels(\"stack\",\"multiply\")(\n",
    "            TorchVadMFCC(delta=0),\n",
    "        )),\n",
    "    Cast(dataframe.get_column(\"rec_path\"), Channels(\"cat\",\"multiply\")(\n",
    "            ZeroCrossingRate(),\n",
    "            RMSEnergy(),\n",
    "        )),\n",
    "    Cast(dataframe.get_column(\"word_id\"), lambda x: torch.tensor(x-1, dtype=torch.long)),\n",
    "    Cast(dataframe.get_column(\"value\"), lambda x: torch.tensor(x).float()),\n",
    ")\n",
    "\n",
    "collate_fn = build_collate_fn(\n",
    "    PaddingCollate(mode=\"SET_MAX_LEN\", max_len=80, pad_dim=2),\n",
    "    PaddingCollate(mode=\"SET_MAX_LEN\", max_len=80, pad_dim=1),\n",
    "    DefaultCollate(),\n",
    "    DefaultCollate(),\n",
    ")\n",
    "dataset_train = to_dataset(train_pl)\n",
    "dataset_val = to_dataset(val_pl)\n",
    "dataset_test = to_dataset(test_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be889768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "from pytorch_dataloader import MemoryLoadedDataLoader\n",
    "from os import name\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#note, if you are using Windows you MUST set `num_workers=0` - TL;DT multithreading DON'T work in notebooks because Windows DON'T have `fork()`\n",
    "num_workers = 0 if name == \"nt\" else 4\n",
    "train_loader = DataLoader(dataset_train, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=num_workers)\n",
    "val_loader = DataLoader(dataset_val, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
    "test_loader = DataLoader(dataset_test, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
    "\n",
    "for x in next(iter(train_loader)):\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf890d68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:05:23.032498Z",
     "start_time": "2025-07-29T09:04:37.522932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train loader into memory\n",
      "Loaded validation loader into memory\n"
     ]
    }
   ],
   "source": [
    "train_loader = MemoryLoadedDataLoader(train_loader, device=device)\n",
    "print(\"Loaded train loader into memory\")\n",
    "val_loader = MemoryLoadedDataLoader(val_loader, device=device)\n",
    "print(\"Loaded validation loader into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c91ba076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.FusionCNN import ContextFusionCNN\n",
    "reload_function(ContextFusionCNN)\n",
    "model = ContextFusionCNN(1,2, num_words=N_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a3a47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:25:42.755943Z",
     "start_time": "2025-07-29T09:22:19.522018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tad1/Projects/AI/Mandarin_Pronunciation_Recognition_Project/src/wandb/run-20251021_172046-7c3zmj0x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/7c3zmj0x' target=\"_blank\">Tones ContextFusionCNN(16) 1L-NN</a></strong> to <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/7c3zmj0x' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/7c3zmj0x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.7095, Train Acc: 0.4971, Val Loss: 0.7375, Val Acc: 0.3918\n",
      "Epoch 2, Train Loss: 0.7106, Train Acc: 0.4942, Val Loss: 0.7373, Val Acc: 0.3918\n",
      "Epoch 3, Train Loss: 0.7056, Train Acc: 0.4971, Val Loss: 0.7366, Val Acc: 0.3918\n",
      "Epoch 4, Train Loss: 0.7063, Train Acc: 0.4981, Val Loss: 0.7359, Val Acc: 0.3918\n",
      "Epoch 5, Train Loss: 0.7049, Train Acc: 0.5010, Val Loss: 0.7350, Val Acc: 0.3918\n",
      "Epoch 6, Train Loss: 0.7065, Train Acc: 0.4971, Val Loss: 0.7346, Val Acc: 0.3918\n",
      "Epoch 7, Train Loss: 0.7053, Train Acc: 0.4990, Val Loss: 0.7336, Val Acc: 0.3918\n",
      "Epoch 8, Train Loss: 0.7010, Train Acc: 0.5039, Val Loss: 0.7330, Val Acc: 0.3918\n",
      "Epoch 9, Train Loss: 0.7040, Train Acc: 0.4981, Val Loss: 0.7321, Val Acc: 0.3918\n",
      "Epoch 10, Train Loss: 0.7027, Train Acc: 0.5019, Val Loss: 0.7315, Val Acc: 0.3918\n",
      "Epoch 11, Train Loss: 0.7043, Train Acc: 0.5048, Val Loss: 0.7309, Val Acc: 0.3918\n",
      "Epoch 12, Train Loss: 0.7023, Train Acc: 0.4981, Val Loss: 0.7303, Val Acc: 0.3918\n",
      "Epoch 13, Train Loss: 0.7024, Train Acc: 0.5019, Val Loss: 0.7299, Val Acc: 0.3918\n",
      "Epoch 14, Train Loss: 0.7029, Train Acc: 0.4961, Val Loss: 0.7294, Val Acc: 0.3918\n",
      "Epoch 15, Train Loss: 0.7019, Train Acc: 0.5068, Val Loss: 0.7289, Val Acc: 0.3918\n",
      "Epoch 16, Train Loss: 0.7011, Train Acc: 0.5019, Val Loss: 0.7281, Val Acc: 0.3918\n",
      "Epoch 17, Train Loss: 0.6991, Train Acc: 0.5010, Val Loss: 0.7276, Val Acc: 0.3918\n",
      "Epoch 18, Train Loss: 0.7013, Train Acc: 0.5000, Val Loss: 0.7268, Val Acc: 0.3918\n",
      "Epoch 19, Train Loss: 0.6985, Train Acc: 0.5068, Val Loss: 0.7263, Val Acc: 0.3918\n",
      "Epoch 20, Train Loss: 0.7011, Train Acc: 0.5010, Val Loss: 0.7254, Val Acc: 0.3918\n",
      "Epoch 21, Train Loss: 0.7042, Train Acc: 0.4903, Val Loss: 0.7253, Val Acc: 0.3918\n",
      "Epoch 22, Train Loss: 0.7008, Train Acc: 0.4981, Val Loss: 0.7245, Val Acc: 0.3918\n",
      "Epoch 23, Train Loss: 0.7000, Train Acc: 0.5039, Val Loss: 0.7238, Val Acc: 0.3918\n",
      "Epoch 24, Train Loss: 0.7011, Train Acc: 0.5019, Val Loss: 0.7234, Val Acc: 0.3918\n",
      "Epoch 25, Train Loss: 0.6990, Train Acc: 0.5010, Val Loss: 0.7229, Val Acc: 0.3918\n",
      "Epoch 26, Train Loss: 0.7010, Train Acc: 0.5010, Val Loss: 0.7222, Val Acc: 0.3918\n",
      "Epoch 27, Train Loss: 0.7034, Train Acc: 0.4816, Val Loss: 0.7219, Val Acc: 0.3918\n",
      "Epoch 28, Train Loss: 0.6957, Train Acc: 0.5068, Val Loss: 0.7213, Val Acc: 0.3918\n",
      "Epoch 29, Train Loss: 0.6978, Train Acc: 0.5058, Val Loss: 0.7206, Val Acc: 0.3918\n",
      "Epoch 30, Train Loss: 0.6999, Train Acc: 0.4932, Val Loss: 0.7204, Val Acc: 0.3918\n",
      "Epoch 31, Train Loss: 0.6991, Train Acc: 0.5048, Val Loss: 0.7195, Val Acc: 0.3918\n",
      "Epoch 32, Train Loss: 0.6970, Train Acc: 0.5029, Val Loss: 0.7189, Val Acc: 0.3918\n",
      "Epoch 33, Train Loss: 0.6998, Train Acc: 0.5068, Val Loss: 0.7185, Val Acc: 0.3918\n",
      "Epoch 34, Train Loss: 0.6981, Train Acc: 0.5107, Val Loss: 0.7182, Val Acc: 0.3918\n",
      "Epoch 35, Train Loss: 0.6981, Train Acc: 0.5039, Val Loss: 0.7175, Val Acc: 0.3918\n",
      "Epoch 36, Train Loss: 0.6978, Train Acc: 0.5097, Val Loss: 0.7171, Val Acc: 0.3918\n",
      "Epoch 37, Train Loss: 0.6966, Train Acc: 0.5136, Val Loss: 0.7164, Val Acc: 0.3918\n",
      "Epoch 38, Train Loss: 0.6966, Train Acc: 0.5029, Val Loss: 0.7163, Val Acc: 0.3918\n",
      "Epoch 39, Train Loss: 0.6947, Train Acc: 0.5155, Val Loss: 0.7156, Val Acc: 0.3918\n",
      "Epoch 40, Train Loss: 0.6959, Train Acc: 0.5107, Val Loss: 0.7150, Val Acc: 0.3918\n",
      "Epoch 41, Train Loss: 0.6954, Train Acc: 0.5107, Val Loss: 0.7145, Val Acc: 0.3918\n",
      "Epoch 42, Train Loss: 0.6972, Train Acc: 0.4981, Val Loss: 0.7139, Val Acc: 0.3918\n",
      "Epoch 43, Train Loss: 0.6976, Train Acc: 0.5116, Val Loss: 0.7134, Val Acc: 0.3918\n",
      "Epoch 44, Train Loss: 0.6977, Train Acc: 0.5029, Val Loss: 0.7127, Val Acc: 0.3918\n",
      "Epoch 45, Train Loss: 0.6947, Train Acc: 0.5039, Val Loss: 0.7127, Val Acc: 0.3918\n",
      "Epoch 46, Train Loss: 0.6958, Train Acc: 0.4971, Val Loss: 0.7121, Val Acc: 0.3918\n",
      "Epoch 47, Train Loss: 0.6969, Train Acc: 0.5107, Val Loss: 0.7114, Val Acc: 0.3918\n",
      "Epoch 48, Train Loss: 0.6923, Train Acc: 0.5078, Val Loss: 0.7109, Val Acc: 0.3918\n",
      "Epoch 49, Train Loss: 0.6967, Train Acc: 0.4903, Val Loss: 0.7104, Val Acc: 0.3918\n",
      "Epoch 50, Train Loss: 0.6946, Train Acc: 0.5242, Val Loss: 0.7101, Val Acc: 0.3918\n",
      "Epoch 51, Train Loss: 0.6949, Train Acc: 0.5136, Val Loss: 0.7096, Val Acc: 0.3918\n",
      "Epoch 52, Train Loss: 0.6985, Train Acc: 0.4855, Val Loss: 0.7097, Val Acc: 0.3918\n",
      "Epoch 53, Train Loss: 0.6927, Train Acc: 0.5097, Val Loss: 0.7091, Val Acc: 0.3918\n",
      "Epoch 54, Train Loss: 0.6951, Train Acc: 0.5087, Val Loss: 0.7081, Val Acc: 0.3918\n",
      "Epoch 55, Train Loss: 0.6952, Train Acc: 0.5039, Val Loss: 0.7076, Val Acc: 0.3918\n",
      "Epoch 56, Train Loss: 0.6967, Train Acc: 0.5000, Val Loss: 0.7074, Val Acc: 0.3918\n",
      "Epoch 57, Train Loss: 0.6917, Train Acc: 0.5155, Val Loss: 0.7071, Val Acc: 0.3918\n",
      "Epoch 58, Train Loss: 0.6978, Train Acc: 0.4952, Val Loss: 0.7068, Val Acc: 0.3918\n",
      "Epoch 59, Train Loss: 0.6958, Train Acc: 0.5165, Val Loss: 0.7064, Val Acc: 0.3918\n",
      "Epoch 60, Train Loss: 0.6934, Train Acc: 0.5029, Val Loss: 0.7057, Val Acc: 0.3918\n",
      "Epoch 61, Train Loss: 0.6953, Train Acc: 0.5223, Val Loss: 0.7057, Val Acc: 0.3918\n",
      "Epoch 62, Train Loss: 0.6915, Train Acc: 0.5145, Val Loss: 0.7050, Val Acc: 0.3918\n",
      "Epoch 63, Train Loss: 0.6953, Train Acc: 0.5116, Val Loss: 0.7051, Val Acc: 0.3918\n",
      "Epoch 64, Train Loss: 0.6932, Train Acc: 0.5136, Val Loss: 0.7047, Val Acc: 0.3918\n",
      "Epoch 65, Train Loss: 0.6959, Train Acc: 0.4952, Val Loss: 0.7048, Val Acc: 0.3918\n",
      "Epoch 66, Train Loss: 0.6938, Train Acc: 0.5068, Val Loss: 0.7042, Val Acc: 0.3918\n",
      "Epoch 67, Train Loss: 0.6926, Train Acc: 0.5116, Val Loss: 0.7035, Val Acc: 0.3912\n",
      "Epoch 68, Train Loss: 0.6938, Train Acc: 0.5145, Val Loss: 0.7035, Val Acc: 0.3912\n",
      "Epoch 69, Train Loss: 0.6958, Train Acc: 0.4864, Val Loss: 0.7033, Val Acc: 0.3912\n",
      "Epoch 70, Train Loss: 0.6922, Train Acc: 0.5107, Val Loss: 0.7032, Val Acc: 0.3912\n",
      "Epoch 71, Train Loss: 0.6980, Train Acc: 0.4942, Val Loss: 0.7035, Val Acc: 0.3912\n",
      "Epoch 72, Train Loss: 0.6958, Train Acc: 0.4981, Val Loss: 0.7027, Val Acc: 0.3912\n",
      "Epoch 73, Train Loss: 0.6920, Train Acc: 0.5039, Val Loss: 0.7023, Val Acc: 0.3909\n",
      "Epoch 74, Train Loss: 0.6858, Train Acc: 0.5552, Val Loss: 0.7015, Val Acc: 0.3915\n",
      "Epoch 75, Train Loss: 0.6931, Train Acc: 0.5068, Val Loss: 0.7016, Val Acc: 0.3915\n",
      "Epoch 76, Train Loss: 0.6918, Train Acc: 0.5194, Val Loss: 0.7012, Val Acc: 0.3918\n",
      "Epoch 77, Train Loss: 0.6971, Train Acc: 0.4874, Val Loss: 0.7009, Val Acc: 0.3918\n",
      "Epoch 78, Train Loss: 0.6914, Train Acc: 0.5058, Val Loss: 0.7011, Val Acc: 0.3918\n",
      "Epoch 79, Train Loss: 0.6916, Train Acc: 0.5145, Val Loss: 0.7004, Val Acc: 0.3926\n",
      "Epoch 80, Train Loss: 0.7015, Train Acc: 0.4806, Val Loss: 0.7009, Val Acc: 0.3918\n",
      "Epoch 81, Train Loss: 0.6938, Train Acc: 0.5107, Val Loss: 0.7006, Val Acc: 0.3926\n",
      "Epoch 82, Train Loss: 0.6967, Train Acc: 0.4942, Val Loss: 0.7008, Val Acc: 0.3926\n",
      "Epoch 83, Train Loss: 0.6890, Train Acc: 0.5349, Val Loss: 0.7000, Val Acc: 0.3929\n",
      "Epoch 84, Train Loss: 0.6950, Train Acc: 0.5048, Val Loss: 0.6998, Val Acc: 0.3937\n",
      "Epoch 85, Train Loss: 0.6928, Train Acc: 0.5087, Val Loss: 0.7000, Val Acc: 0.3929\n",
      "Epoch 86, Train Loss: 0.6921, Train Acc: 0.4961, Val Loss: 0.6999, Val Acc: 0.3931\n",
      "Epoch 87, Train Loss: 0.6917, Train Acc: 0.5165, Val Loss: 0.6994, Val Acc: 0.3942\n",
      "Epoch 88, Train Loss: 0.6918, Train Acc: 0.5262, Val Loss: 0.6991, Val Acc: 0.3945\n",
      "Epoch 89, Train Loss: 0.6908, Train Acc: 0.5107, Val Loss: 0.6988, Val Acc: 0.3951\n",
      "Epoch 90, Train Loss: 0.6899, Train Acc: 0.5359, Val Loss: 0.6980, Val Acc: 0.3978\n",
      "Epoch 91, Train Loss: 0.6917, Train Acc: 0.5097, Val Loss: 0.6984, Val Acc: 0.3962\n",
      "Epoch 92, Train Loss: 0.6910, Train Acc: 0.5019, Val Loss: 0.6981, Val Acc: 0.3975\n",
      "Epoch 93, Train Loss: 0.6923, Train Acc: 0.5213, Val Loss: 0.6977, Val Acc: 0.3981\n",
      "Epoch 94, Train Loss: 0.6868, Train Acc: 0.5310, Val Loss: 0.6974, Val Acc: 0.3986\n",
      "Epoch 95, Train Loss: 0.6941, Train Acc: 0.4990, Val Loss: 0.6970, Val Acc: 0.3992\n",
      "Epoch 96, Train Loss: 0.6942, Train Acc: 0.5019, Val Loss: 0.6972, Val Acc: 0.3992\n",
      "Epoch 97, Train Loss: 0.6884, Train Acc: 0.5378, Val Loss: 0.6966, Val Acc: 0.3995\n",
      "Epoch 98, Train Loss: 0.6916, Train Acc: 0.5087, Val Loss: 0.6961, Val Acc: 0.4022\n",
      "Epoch 99, Train Loss: 0.6930, Train Acc: 0.5019, Val Loss: 0.6965, Val Acc: 0.4000\n",
      "Epoch 100, Train Loss: 0.6877, Train Acc: 0.5436, Val Loss: 0.6963, Val Acc: 0.4008\n",
      "Epoch 101, Train Loss: 0.6925, Train Acc: 0.5116, Val Loss: 0.6964, Val Acc: 0.4005\n",
      "Epoch 102, Train Loss: 0.6884, Train Acc: 0.5339, Val Loss: 0.6962, Val Acc: 0.4014\n",
      "Epoch 103, Train Loss: 0.6902, Train Acc: 0.5155, Val Loss: 0.6960, Val Acc: 0.4033\n",
      "Epoch 104, Train Loss: 0.6905, Train Acc: 0.5407, Val Loss: 0.6959, Val Acc: 0.4041\n",
      "Epoch 105, Train Loss: 0.6898, Train Acc: 0.5300, Val Loss: 0.6957, Val Acc: 0.4060\n",
      "Epoch 106, Train Loss: 0.6938, Train Acc: 0.4952, Val Loss: 0.6957, Val Acc: 0.4071\n",
      "Epoch 107, Train Loss: 0.6896, Train Acc: 0.5145, Val Loss: 0.6955, Val Acc: 0.4066\n",
      "Epoch 108, Train Loss: 0.6913, Train Acc: 0.5145, Val Loss: 0.6957, Val Acc: 0.4055\n",
      "Epoch 109, Train Loss: 0.6906, Train Acc: 0.5349, Val Loss: 0.6952, Val Acc: 0.4069\n",
      "Epoch 110, Train Loss: 0.6914, Train Acc: 0.5068, Val Loss: 0.6948, Val Acc: 0.4063\n",
      "Epoch 111, Train Loss: 0.6927, Train Acc: 0.5087, Val Loss: 0.6947, Val Acc: 0.4082\n",
      "Epoch 112, Train Loss: 0.6906, Train Acc: 0.5107, Val Loss: 0.6951, Val Acc: 0.4071\n",
      "Epoch 113, Train Loss: 0.6919, Train Acc: 0.5145, Val Loss: 0.6950, Val Acc: 0.4066\n",
      "Epoch 114, Train Loss: 0.6865, Train Acc: 0.5310, Val Loss: 0.6949, Val Acc: 0.4069\n",
      "Epoch 115, Train Loss: 0.6866, Train Acc: 0.5359, Val Loss: 0.6948, Val Acc: 0.4063\n",
      "Epoch 116, Train Loss: 0.6876, Train Acc: 0.5165, Val Loss: 0.6943, Val Acc: 0.4102\n",
      "Epoch 117, Train Loss: 0.6865, Train Acc: 0.5223, Val Loss: 0.6941, Val Acc: 0.4107\n",
      "Epoch 118, Train Loss: 0.6908, Train Acc: 0.5010, Val Loss: 0.6947, Val Acc: 0.4071\n",
      "Epoch 119, Train Loss: 0.6938, Train Acc: 0.4893, Val Loss: 0.6941, Val Acc: 0.4113\n",
      "Epoch 120, Train Loss: 0.6930, Train Acc: 0.5097, Val Loss: 0.6944, Val Acc: 0.4102\n",
      "Epoch 121, Train Loss: 0.6925, Train Acc: 0.5097, Val Loss: 0.6939, Val Acc: 0.4124\n",
      "Epoch 122, Train Loss: 0.6914, Train Acc: 0.5116, Val Loss: 0.6940, Val Acc: 0.4118\n",
      "Epoch 123, Train Loss: 0.6907, Train Acc: 0.5107, Val Loss: 0.6934, Val Acc: 0.4157\n",
      "Epoch 124, Train Loss: 0.6869, Train Acc: 0.5281, Val Loss: 0.6936, Val Acc: 0.4148\n",
      "Epoch 125, Train Loss: 0.6902, Train Acc: 0.5116, Val Loss: 0.6936, Val Acc: 0.4154\n",
      "Epoch 126, Train Loss: 0.6871, Train Acc: 0.5320, Val Loss: 0.6935, Val Acc: 0.4154\n",
      "Epoch 127, Train Loss: 0.6875, Train Acc: 0.5155, Val Loss: 0.6936, Val Acc: 0.4154\n",
      "Epoch 128, Train Loss: 0.6916, Train Acc: 0.5029, Val Loss: 0.6928, Val Acc: 0.4250\n",
      "Epoch 129, Train Loss: 0.6845, Train Acc: 0.5407, Val Loss: 0.6928, Val Acc: 0.4242\n",
      "Epoch 130, Train Loss: 0.6859, Train Acc: 0.5388, Val Loss: 0.6926, Val Acc: 0.4255\n",
      "Epoch 131, Train Loss: 0.6886, Train Acc: 0.5281, Val Loss: 0.6923, Val Acc: 0.4277\n",
      "Epoch 132, Train Loss: 0.6871, Train Acc: 0.5514, Val Loss: 0.6920, Val Acc: 0.4294\n",
      "Epoch 133, Train Loss: 0.6899, Train Acc: 0.5262, Val Loss: 0.6923, Val Acc: 0.4258\n",
      "Epoch 134, Train Loss: 0.6875, Train Acc: 0.5097, Val Loss: 0.6918, Val Acc: 0.4291\n",
      "Epoch 135, Train Loss: 0.6855, Train Acc: 0.5329, Val Loss: 0.6917, Val Acc: 0.4302\n",
      "Epoch 136, Train Loss: 0.6889, Train Acc: 0.5213, Val Loss: 0.6918, Val Acc: 0.4297\n",
      "Epoch 137, Train Loss: 0.6875, Train Acc: 0.5068, Val Loss: 0.6910, Val Acc: 0.4379\n",
      "Epoch 138, Train Loss: 0.6879, Train Acc: 0.5223, Val Loss: 0.6910, Val Acc: 0.4390\n",
      "Epoch 139, Train Loss: 0.6908, Train Acc: 0.5068, Val Loss: 0.6914, Val Acc: 0.4335\n",
      "Epoch 140, Train Loss: 0.6846, Train Acc: 0.5329, Val Loss: 0.6908, Val Acc: 0.4404\n",
      "Epoch 141, Train Loss: 0.6885, Train Acc: 0.5271, Val Loss: 0.6913, Val Acc: 0.4349\n",
      "Epoch 142, Train Loss: 0.6888, Train Acc: 0.5116, Val Loss: 0.6909, Val Acc: 0.4371\n",
      "Epoch 143, Train Loss: 0.6851, Train Acc: 0.5504, Val Loss: 0.6909, Val Acc: 0.4365\n",
      "Epoch 144, Train Loss: 0.6862, Train Acc: 0.5329, Val Loss: 0.6908, Val Acc: 0.4371\n",
      "Epoch 145, Train Loss: 0.6878, Train Acc: 0.5368, Val Loss: 0.6905, Val Acc: 0.4431\n",
      "Epoch 146, Train Loss: 0.6875, Train Acc: 0.5087, Val Loss: 0.6902, Val Acc: 0.4503\n",
      "Epoch 147, Train Loss: 0.6920, Train Acc: 0.5000, Val Loss: 0.6899, Val Acc: 0.4544\n",
      "Epoch 148, Train Loss: 0.6899, Train Acc: 0.5107, Val Loss: 0.6908, Val Acc: 0.4393\n",
      "Epoch 149, Train Loss: 0.6888, Train Acc: 0.5339, Val Loss: 0.6914, Val Acc: 0.4330\n",
      "Epoch 150, Train Loss: 0.6855, Train Acc: 0.5349, Val Loss: 0.6912, Val Acc: 0.4332\n",
      "Epoch 151, Train Loss: 0.6918, Train Acc: 0.5058, Val Loss: 0.6908, Val Acc: 0.4382\n",
      "Epoch 152, Train Loss: 0.6873, Train Acc: 0.5339, Val Loss: 0.6906, Val Acc: 0.4379\n",
      "Epoch 153, Train Loss: 0.6858, Train Acc: 0.5029, Val Loss: 0.6902, Val Acc: 0.4415\n",
      "Epoch 154, Train Loss: 0.6934, Train Acc: 0.4961, Val Loss: 0.6902, Val Acc: 0.4437\n",
      "Epoch 155, Train Loss: 0.6870, Train Acc: 0.5194, Val Loss: 0.6903, Val Acc: 0.4420\n",
      "Epoch 156, Train Loss: 0.6890, Train Acc: 0.5136, Val Loss: 0.6906, Val Acc: 0.4382\n",
      "Epoch 157, Train Loss: 0.6870, Train Acc: 0.5155, Val Loss: 0.6902, Val Acc: 0.4415\n",
      "Epoch 158, Train Loss: 0.6897, Train Acc: 0.5126, Val Loss: 0.6901, Val Acc: 0.4420\n",
      "Epoch 159, Train Loss: 0.6859, Train Acc: 0.5368, Val Loss: 0.6900, Val Acc: 0.4456\n",
      "Epoch 160, Train Loss: 0.6802, Train Acc: 0.5562, Val Loss: 0.6902, Val Acc: 0.4420\n",
      "Epoch 161, Train Loss: 0.6863, Train Acc: 0.5329, Val Loss: 0.6901, Val Acc: 0.4401\n",
      "Epoch 162, Train Loss: 0.6868, Train Acc: 0.5184, Val Loss: 0.6897, Val Acc: 0.4500\n",
      "Epoch 163, Train Loss: 0.6873, Train Acc: 0.5242, Val Loss: 0.6898, Val Acc: 0.4484\n",
      "Epoch 164, Train Loss: 0.6873, Train Acc: 0.5300, Val Loss: 0.6895, Val Acc: 0.4549\n",
      "Epoch 165, Train Loss: 0.6862, Train Acc: 0.5329, Val Loss: 0.6895, Val Acc: 0.4536\n",
      "Epoch 166, Train Loss: 0.6871, Train Acc: 0.5107, Val Loss: 0.6899, Val Acc: 0.4486\n",
      "Epoch 167, Train Loss: 0.6869, Train Acc: 0.5320, Val Loss: 0.6895, Val Acc: 0.4527\n",
      "Epoch 168, Train Loss: 0.6859, Train Acc: 0.5339, Val Loss: 0.6894, Val Acc: 0.4547\n",
      "Epoch 169, Train Loss: 0.6882, Train Acc: 0.5019, Val Loss: 0.6896, Val Acc: 0.4495\n",
      "Epoch 170, Train Loss: 0.6869, Train Acc: 0.5388, Val Loss: 0.6901, Val Acc: 0.4426\n",
      "Epoch 171, Train Loss: 0.6851, Train Acc: 0.5349, Val Loss: 0.6896, Val Acc: 0.4503\n",
      "Epoch 172, Train Loss: 0.6872, Train Acc: 0.5320, Val Loss: 0.6896, Val Acc: 0.4495\n",
      "Epoch 173, Train Loss: 0.6812, Train Acc: 0.5368, Val Loss: 0.6898, Val Acc: 0.4481\n",
      "Epoch 174, Train Loss: 0.6866, Train Acc: 0.5194, Val Loss: 0.6894, Val Acc: 0.4530\n",
      "Epoch 175, Train Loss: 0.6867, Train Acc: 0.5281, Val Loss: 0.6895, Val Acc: 0.4505\n",
      "Epoch 176, Train Loss: 0.6870, Train Acc: 0.5320, Val Loss: 0.6892, Val Acc: 0.4552\n",
      "Epoch 177, Train Loss: 0.6892, Train Acc: 0.5136, Val Loss: 0.6893, Val Acc: 0.4566\n",
      "Epoch 178, Train Loss: 0.6897, Train Acc: 0.5233, Val Loss: 0.6896, Val Acc: 0.4497\n",
      "Epoch 179, Train Loss: 0.6846, Train Acc: 0.5300, Val Loss: 0.6897, Val Acc: 0.4489\n",
      "Epoch 180, Train Loss: 0.6868, Train Acc: 0.5223, Val Loss: 0.6895, Val Acc: 0.4516\n",
      "Epoch 181, Train Loss: 0.6852, Train Acc: 0.5194, Val Loss: 0.6895, Val Acc: 0.4516\n",
      "Epoch 182, Train Loss: 0.6846, Train Acc: 0.5271, Val Loss: 0.6896, Val Acc: 0.4492\n",
      "Epoch 183, Train Loss: 0.6855, Train Acc: 0.5378, Val Loss: 0.6897, Val Acc: 0.4484\n",
      "Epoch 184, Train Loss: 0.6884, Train Acc: 0.5126, Val Loss: 0.6895, Val Acc: 0.4514\n",
      "Epoch 185, Train Loss: 0.6869, Train Acc: 0.5184, Val Loss: 0.6895, Val Acc: 0.4500\n",
      "Epoch 186, Train Loss: 0.6862, Train Acc: 0.5388, Val Loss: 0.6896, Val Acc: 0.4495\n",
      "Epoch 187, Train Loss: 0.6872, Train Acc: 0.5213, Val Loss: 0.6897, Val Acc: 0.4495\n",
      "Epoch 188, Train Loss: 0.6873, Train Acc: 0.5136, Val Loss: 0.6894, Val Acc: 0.4536\n",
      "Epoch 189, Train Loss: 0.6869, Train Acc: 0.5407, Val Loss: 0.6897, Val Acc: 0.4484\n",
      "Epoch 190, Train Loss: 0.6935, Train Acc: 0.5068, Val Loss: 0.6897, Val Acc: 0.4486\n",
      "Epoch 191, Train Loss: 0.6842, Train Acc: 0.5116, Val Loss: 0.6894, Val Acc: 0.4519\n",
      "Epoch 192, Train Loss: 0.6859, Train Acc: 0.5368, Val Loss: 0.6895, Val Acc: 0.4533\n",
      "Epoch 193, Train Loss: 0.6841, Train Acc: 0.5339, Val Loss: 0.6892, Val Acc: 0.4544\n",
      "Epoch 194, Train Loss: 0.6877, Train Acc: 0.5194, Val Loss: 0.6892, Val Acc: 0.4558\n",
      "Epoch 195, Train Loss: 0.6859, Train Acc: 0.5213, Val Loss: 0.6897, Val Acc: 0.4489\n",
      "Epoch 196, Train Loss: 0.6814, Train Acc: 0.5504, Val Loss: 0.6895, Val Acc: 0.4516\n",
      "Epoch 197, Train Loss: 0.6881, Train Acc: 0.5213, Val Loss: 0.6890, Val Acc: 0.4585\n",
      "Epoch 198, Train Loss: 0.6882, Train Acc: 0.5068, Val Loss: 0.6895, Val Acc: 0.4516\n",
      "Epoch 199, Train Loss: 0.6870, Train Acc: 0.5291, Val Loss: 0.6896, Val Acc: 0.4489\n",
      "Epoch 200, Train Loss: 0.6814, Train Acc: 0.5610, Val Loss: 0.6894, Val Acc: 0.4522\n",
      "Epoch 201, Train Loss: 0.6894, Train Acc: 0.5368, Val Loss: 0.6894, Val Acc: 0.4514\n",
      "Epoch 202, Train Loss: 0.6867, Train Acc: 0.5300, Val Loss: 0.6891, Val Acc: 0.4569\n",
      "Epoch 203, Train Loss: 0.6884, Train Acc: 0.5378, Val Loss: 0.6894, Val Acc: 0.4527\n",
      "Epoch 204, Train Loss: 0.6881, Train Acc: 0.5058, Val Loss: 0.6893, Val Acc: 0.4566\n",
      "Epoch 205, Train Loss: 0.6897, Train Acc: 0.5155, Val Loss: 0.6896, Val Acc: 0.4495\n",
      "Epoch 206, Train Loss: 0.6875, Train Acc: 0.5213, Val Loss: 0.6892, Val Acc: 0.4547\n",
      "Epoch 207, Train Loss: 0.6879, Train Acc: 0.5203, Val Loss: 0.6891, Val Acc: 0.4569\n",
      "Epoch 208, Train Loss: 0.6791, Train Acc: 0.5620, Val Loss: 0.6895, Val Acc: 0.4536\n",
      "Epoch 209, Train Loss: 0.6859, Train Acc: 0.5281, Val Loss: 0.6899, Val Acc: 0.4445\n",
      "Epoch 210, Train Loss: 0.6851, Train Acc: 0.5087, Val Loss: 0.6896, Val Acc: 0.4495\n",
      "Epoch 211, Train Loss: 0.6835, Train Acc: 0.5378, Val Loss: 0.6897, Val Acc: 0.4486\n",
      "Epoch 212, Train Loss: 0.6841, Train Acc: 0.5300, Val Loss: 0.6894, Val Acc: 0.4544\n",
      "Epoch 213, Train Loss: 0.6800, Train Acc: 0.5581, Val Loss: 0.6898, Val Acc: 0.4484\n",
      "Epoch 214, Train Loss: 0.6854, Train Acc: 0.5213, Val Loss: 0.6891, Val Acc: 0.4585\n",
      "Epoch 215, Train Loss: 0.6909, Train Acc: 0.5019, Val Loss: 0.6891, Val Acc: 0.4566\n",
      "Epoch 216, Train Loss: 0.6821, Train Acc: 0.5436, Val Loss: 0.6893, Val Acc: 0.4560\n",
      "Epoch 217, Train Loss: 0.6870, Train Acc: 0.5320, Val Loss: 0.6896, Val Acc: 0.4495\n",
      "Epoch 218, Train Loss: 0.6858, Train Acc: 0.5019, Val Loss: 0.6895, Val Acc: 0.4516\n",
      "Epoch 219, Train Loss: 0.6831, Train Acc: 0.5300, Val Loss: 0.6898, Val Acc: 0.4470\n",
      "Epoch 220, Train Loss: 0.6874, Train Acc: 0.5262, Val Loss: 0.6896, Val Acc: 0.4495\n",
      "Epoch 221, Train Loss: 0.6824, Train Acc: 0.5475, Val Loss: 0.6893, Val Acc: 0.4555\n",
      "Epoch 222, Train Loss: 0.6864, Train Acc: 0.5359, Val Loss: 0.6894, Val Acc: 0.4547\n",
      "Epoch 223, Train Loss: 0.6886, Train Acc: 0.5174, Val Loss: 0.6897, Val Acc: 0.4492\n",
      "Epoch 224, Train Loss: 0.6809, Train Acc: 0.5465, Val Loss: 0.6891, Val Acc: 0.4558\n",
      "Epoch 225, Train Loss: 0.6844, Train Acc: 0.5242, Val Loss: 0.6897, Val Acc: 0.4484\n",
      "Epoch 226, Train Loss: 0.6831, Train Acc: 0.5368, Val Loss: 0.6894, Val Acc: 0.4541\n",
      "Epoch 227, Train Loss: 0.6846, Train Acc: 0.5203, Val Loss: 0.6892, Val Acc: 0.4566\n",
      "Epoch 228, Train Loss: 0.6844, Train Acc: 0.5465, Val Loss: 0.6894, Val Acc: 0.4525\n",
      "Epoch 229, Train Loss: 0.6889, Train Acc: 0.5310, Val Loss: 0.6896, Val Acc: 0.4511\n",
      "Epoch 230, Train Loss: 0.6895, Train Acc: 0.5107, Val Loss: 0.6892, Val Acc: 0.4571\n",
      "Epoch 231, Train Loss: 0.6897, Train Acc: 0.5368, Val Loss: 0.6891, Val Acc: 0.4555\n",
      "Epoch 232, Train Loss: 0.6882, Train Acc: 0.5019, Val Loss: 0.6894, Val Acc: 0.4541\n",
      "Epoch 233, Train Loss: 0.6889, Train Acc: 0.5252, Val Loss: 0.6893, Val Acc: 0.4566\n",
      "Epoch 234, Train Loss: 0.6878, Train Acc: 0.5300, Val Loss: 0.6894, Val Acc: 0.4527\n",
      "Epoch 235, Train Loss: 0.6891, Train Acc: 0.5145, Val Loss: 0.6894, Val Acc: 0.4511\n",
      "Epoch 236, Train Loss: 0.6867, Train Acc: 0.5310, Val Loss: 0.6893, Val Acc: 0.4541\n",
      "Epoch 237, Train Loss: 0.6884, Train Acc: 0.5194, Val Loss: 0.6892, Val Acc: 0.4569\n",
      "Epoch 238, Train Loss: 0.6817, Train Acc: 0.5484, Val Loss: 0.6893, Val Acc: 0.4563\n",
      "Epoch 239, Train Loss: 0.6886, Train Acc: 0.5165, Val Loss: 0.6896, Val Acc: 0.4481\n",
      "Epoch 240, Train Loss: 0.6879, Train Acc: 0.5252, Val Loss: 0.6891, Val Acc: 0.4585\n",
      "Epoch 241, Train Loss: 0.6904, Train Acc: 0.5097, Val Loss: 0.6890, Val Acc: 0.4582\n",
      "Epoch 242, Train Loss: 0.6911, Train Acc: 0.5136, Val Loss: 0.6893, Val Acc: 0.4558\n",
      "Epoch 243, Train Loss: 0.6872, Train Acc: 0.5145, Val Loss: 0.6894, Val Acc: 0.4547\n",
      "Epoch 244, Train Loss: 0.6843, Train Acc: 0.5494, Val Loss: 0.6890, Val Acc: 0.4588\n",
      "Epoch 245, Train Loss: 0.6921, Train Acc: 0.5174, Val Loss: 0.6896, Val Acc: 0.4489\n",
      "Epoch 246, Train Loss: 0.6855, Train Acc: 0.5281, Val Loss: 0.6893, Val Acc: 0.4569\n",
      "Epoch 247, Train Loss: 0.6929, Train Acc: 0.4971, Val Loss: 0.6893, Val Acc: 0.4552\n",
      "Epoch 248, Train Loss: 0.6859, Train Acc: 0.5242, Val Loss: 0.6894, Val Acc: 0.4519\n",
      "Epoch 249, Train Loss: 0.6851, Train Acc: 0.5310, Val Loss: 0.6893, Val Acc: 0.4555\n",
      "Epoch 250, Train Loss: 0.6857, Train Acc: 0.5233, Val Loss: 0.6892, Val Acc: 0.4560\n",
      "Epoch 251, Train Loss: 0.6875, Train Acc: 0.5165, Val Loss: 0.6892, Val Acc: 0.4571\n",
      "Epoch 252, Train Loss: 0.6865, Train Acc: 0.5300, Val Loss: 0.6892, Val Acc: 0.4571\n",
      "Epoch 253, Train Loss: 0.6914, Train Acc: 0.5078, Val Loss: 0.6892, Val Acc: 0.4577\n",
      "Epoch 254, Train Loss: 0.6851, Train Acc: 0.5262, Val Loss: 0.6890, Val Acc: 0.4582\n",
      "Epoch 255, Train Loss: 0.6879, Train Acc: 0.5242, Val Loss: 0.6892, Val Acc: 0.4552\n",
      "Epoch 256, Train Loss: 0.6836, Train Acc: 0.5271, Val Loss: 0.6895, Val Acc: 0.4511\n",
      "Epoch 257, Train Loss: 0.6860, Train Acc: 0.5349, Val Loss: 0.6892, Val Acc: 0.4574\n",
      "Epoch 258, Train Loss: 0.6834, Train Acc: 0.5388, Val Loss: 0.6893, Val Acc: 0.4555\n",
      "Epoch 259, Train Loss: 0.6853, Train Acc: 0.5446, Val Loss: 0.6895, Val Acc: 0.4511\n",
      "Epoch 260, Train Loss: 0.6906, Train Acc: 0.5116, Val Loss: 0.6895, Val Acc: 0.4533\n",
      "Epoch 261, Train Loss: 0.6900, Train Acc: 0.5107, Val Loss: 0.6898, Val Acc: 0.4489\n",
      "Epoch 262, Train Loss: 0.6792, Train Acc: 0.5446, Val Loss: 0.6897, Val Acc: 0.4473\n",
      "Epoch 263, Train Loss: 0.6862, Train Acc: 0.5184, Val Loss: 0.6893, Val Acc: 0.4574\n",
      "Epoch 264, Train Loss: 0.6835, Train Acc: 0.5291, Val Loss: 0.6893, Val Acc: 0.4547\n",
      "Epoch 265, Train Loss: 0.6845, Train Acc: 0.5388, Val Loss: 0.6894, Val Acc: 0.4544\n",
      "Epoch 266, Train Loss: 0.6868, Train Acc: 0.5329, Val Loss: 0.6892, Val Acc: 0.4560\n",
      "Epoch 267, Train Loss: 0.6838, Train Acc: 0.5378, Val Loss: 0.6897, Val Acc: 0.4467\n",
      "Epoch 268, Train Loss: 0.6872, Train Acc: 0.5097, Val Loss: 0.6890, Val Acc: 0.4588\n",
      "Epoch 269, Train Loss: 0.6850, Train Acc: 0.5271, Val Loss: 0.6893, Val Acc: 0.4544\n",
      "Epoch 270, Train Loss: 0.6836, Train Acc: 0.5436, Val Loss: 0.6895, Val Acc: 0.4508\n",
      "Epoch 271, Train Loss: 0.6864, Train Acc: 0.5184, Val Loss: 0.6889, Val Acc: 0.4582\n",
      "Epoch 272, Train Loss: 0.6873, Train Acc: 0.5184, Val Loss: 0.6898, Val Acc: 0.4459\n",
      "Epoch 273, Train Loss: 0.6822, Train Acc: 0.5281, Val Loss: 0.6894, Val Acc: 0.4538\n",
      "Epoch 274, Train Loss: 0.6935, Train Acc: 0.4913, Val Loss: 0.6893, Val Acc: 0.4549\n",
      "Epoch 275, Train Loss: 0.6850, Train Acc: 0.5213, Val Loss: 0.6897, Val Acc: 0.4475\n",
      "Epoch 276, Train Loss: 0.6857, Train Acc: 0.5252, Val Loss: 0.6893, Val Acc: 0.4555\n",
      "Epoch 277, Train Loss: 0.6840, Train Acc: 0.5378, Val Loss: 0.6888, Val Acc: 0.4621\n",
      "Epoch 278, Train Loss: 0.6829, Train Acc: 0.5223, Val Loss: 0.6895, Val Acc: 0.4516\n",
      "Epoch 279, Train Loss: 0.6881, Train Acc: 0.5107, Val Loss: 0.6896, Val Acc: 0.4503\n",
      "Epoch 280, Train Loss: 0.6899, Train Acc: 0.5252, Val Loss: 0.6898, Val Acc: 0.4470\n",
      "Epoch 281, Train Loss: 0.6858, Train Acc: 0.5359, Val Loss: 0.6891, Val Acc: 0.4571\n",
      "Epoch 282, Train Loss: 0.6856, Train Acc: 0.5155, Val Loss: 0.6894, Val Acc: 0.4527\n",
      "Epoch 283, Train Loss: 0.6907, Train Acc: 0.5262, Val Loss: 0.6894, Val Acc: 0.4527\n",
      "Epoch 284, Train Loss: 0.6869, Train Acc: 0.5281, Val Loss: 0.6895, Val Acc: 0.4495\n",
      "Epoch 285, Train Loss: 0.6861, Train Acc: 0.5378, Val Loss: 0.6891, Val Acc: 0.4560\n",
      "Epoch 286, Train Loss: 0.6875, Train Acc: 0.5320, Val Loss: 0.6897, Val Acc: 0.4495\n",
      "Epoch 287, Train Loss: 0.6871, Train Acc: 0.5271, Val Loss: 0.6894, Val Acc: 0.4552\n",
      "Epoch 288, Train Loss: 0.6856, Train Acc: 0.5223, Val Loss: 0.6898, Val Acc: 0.4478\n",
      "Epoch 289, Train Loss: 0.6865, Train Acc: 0.5242, Val Loss: 0.6892, Val Acc: 0.4571\n",
      "Epoch 290, Train Loss: 0.6834, Train Acc: 0.5523, Val Loss: 0.6895, Val Acc: 0.4500\n",
      "Epoch 291, Train Loss: 0.6905, Train Acc: 0.5165, Val Loss: 0.6895, Val Acc: 0.4527\n",
      "Epoch 292, Train Loss: 0.6861, Train Acc: 0.5242, Val Loss: 0.6895, Val Acc: 0.4544\n",
      "Epoch 293, Train Loss: 0.6927, Train Acc: 0.4913, Val Loss: 0.6893, Val Acc: 0.4560\n",
      "Epoch 294, Train Loss: 0.6817, Train Acc: 0.5514, Val Loss: 0.6892, Val Acc: 0.4560\n",
      "Epoch 295, Train Loss: 0.6897, Train Acc: 0.5107, Val Loss: 0.6900, Val Acc: 0.4434\n",
      "Epoch 296, Train Loss: 0.6853, Train Acc: 0.5426, Val Loss: 0.6898, Val Acc: 0.4478\n",
      "Epoch 297, Train Loss: 0.6851, Train Acc: 0.5271, Val Loss: 0.6893, Val Acc: 0.4544\n",
      "Epoch 298, Train Loss: 0.6826, Train Acc: 0.5310, Val Loss: 0.6891, Val Acc: 0.4596\n",
      "Epoch 299, Train Loss: 0.6883, Train Acc: 0.5213, Val Loss: 0.6892, Val Acc: 0.4574\n",
      "Epoch 300, Train Loss: 0.6911, Train Acc: 0.5048, Val Loss: 0.6892, Val Acc: 0.4558\n",
      "Epoch 301, Train Loss: 0.6883, Train Acc: 0.5126, Val Loss: 0.6895, Val Acc: 0.4503\n",
      "Epoch 302, Train Loss: 0.6889, Train Acc: 0.5368, Val Loss: 0.6892, Val Acc: 0.4563\n",
      "Epoch 303, Train Loss: 0.6931, Train Acc: 0.4942, Val Loss: 0.6895, Val Acc: 0.4505\n",
      "Epoch 304, Train Loss: 0.6944, Train Acc: 0.5136, Val Loss: 0.6890, Val Acc: 0.4591\n",
      "Epoch 305, Train Loss: 0.6930, Train Acc: 0.5087, Val Loss: 0.6894, Val Acc: 0.4536\n",
      "Epoch 306, Train Loss: 0.6868, Train Acc: 0.5068, Val Loss: 0.6893, Val Acc: 0.4536\n",
      "Epoch 307, Train Loss: 0.6865, Train Acc: 0.5368, Val Loss: 0.6893, Val Acc: 0.4549\n",
      "Epoch 308, Train Loss: 0.6874, Train Acc: 0.5291, Val Loss: 0.6899, Val Acc: 0.4453\n",
      "Epoch 309, Train Loss: 0.6822, Train Acc: 0.5417, Val Loss: 0.6889, Val Acc: 0.4588\n",
      "Epoch 310, Train Loss: 0.6868, Train Acc: 0.5252, Val Loss: 0.6890, Val Acc: 0.4582\n",
      "Epoch 311, Train Loss: 0.6833, Train Acc: 0.5368, Val Loss: 0.6894, Val Acc: 0.4544\n",
      "Epoch 312, Train Loss: 0.6933, Train Acc: 0.5048, Val Loss: 0.6893, Val Acc: 0.4563\n",
      "Epoch 313, Train Loss: 0.6890, Train Acc: 0.5213, Val Loss: 0.6896, Val Acc: 0.4489\n",
      "Epoch 314, Train Loss: 0.6880, Train Acc: 0.5291, Val Loss: 0.6893, Val Acc: 0.4560\n",
      "Epoch 315, Train Loss: 0.6905, Train Acc: 0.5019, Val Loss: 0.6894, Val Acc: 0.4549\n",
      "Epoch 316, Train Loss: 0.6832, Train Acc: 0.5223, Val Loss: 0.6891, Val Acc: 0.4585\n",
      "Epoch 317, Train Loss: 0.6919, Train Acc: 0.5126, Val Loss: 0.6897, Val Acc: 0.4497\n",
      "Epoch 318, Train Loss: 0.6896, Train Acc: 0.5078, Val Loss: 0.6895, Val Acc: 0.4505\n",
      "Epoch 319, Train Loss: 0.6825, Train Acc: 0.5562, Val Loss: 0.6891, Val Acc: 0.4580\n",
      "Epoch 320, Train Loss: 0.6847, Train Acc: 0.5281, Val Loss: 0.6890, Val Acc: 0.4585\n",
      "Epoch 321, Train Loss: 0.6851, Train Acc: 0.5126, Val Loss: 0.6893, Val Acc: 0.4549\n",
      "Epoch 322, Train Loss: 0.6857, Train Acc: 0.5388, Val Loss: 0.6894, Val Acc: 0.4508\n",
      "Epoch 323, Train Loss: 0.6851, Train Acc: 0.5359, Val Loss: 0.6892, Val Acc: 0.4566\n",
      "Epoch 324, Train Loss: 0.6832, Train Acc: 0.5446, Val Loss: 0.6897, Val Acc: 0.4467\n",
      "Epoch 325, Train Loss: 0.6870, Train Acc: 0.5136, Val Loss: 0.6895, Val Acc: 0.4533\n",
      "Epoch 326, Train Loss: 0.6876, Train Acc: 0.5184, Val Loss: 0.6892, Val Acc: 0.4552\n",
      "Epoch 327, Train Loss: 0.6849, Train Acc: 0.5407, Val Loss: 0.6896, Val Acc: 0.4500\n",
      "Epoch 328, Train Loss: 0.6914, Train Acc: 0.5048, Val Loss: 0.6892, Val Acc: 0.4566\n",
      "Epoch 329, Train Loss: 0.6902, Train Acc: 0.5184, Val Loss: 0.6891, Val Acc: 0.4563\n",
      "Epoch 330, Train Loss: 0.6832, Train Acc: 0.5514, Val Loss: 0.6891, Val Acc: 0.4580\n",
      "Epoch 331, Train Loss: 0.6884, Train Acc: 0.5281, Val Loss: 0.6893, Val Acc: 0.4555\n",
      "Epoch 332, Train Loss: 0.6846, Train Acc: 0.5475, Val Loss: 0.6895, Val Acc: 0.4519\n",
      "Epoch 333, Train Loss: 0.6897, Train Acc: 0.5107, Val Loss: 0.6893, Val Acc: 0.4571\n",
      "Epoch 334, Train Loss: 0.6905, Train Acc: 0.5262, Val Loss: 0.6893, Val Acc: 0.4541\n",
      "Epoch 335, Train Loss: 0.6897, Train Acc: 0.5155, Val Loss: 0.6893, Val Acc: 0.4552\n",
      "Epoch 336, Train Loss: 0.6874, Train Acc: 0.5359, Val Loss: 0.6895, Val Acc: 0.4495\n",
      "Epoch 337, Train Loss: 0.6895, Train Acc: 0.4990, Val Loss: 0.6896, Val Acc: 0.4497\n",
      "Epoch 338, Train Loss: 0.6806, Train Acc: 0.5465, Val Loss: 0.6892, Val Acc: 0.4558\n",
      "Epoch 339, Train Loss: 0.6867, Train Acc: 0.5097, Val Loss: 0.6896, Val Acc: 0.4489\n",
      "Epoch 340, Train Loss: 0.6824, Train Acc: 0.5397, Val Loss: 0.6891, Val Acc: 0.4585\n",
      "Epoch 341, Train Loss: 0.6884, Train Acc: 0.5155, Val Loss: 0.6891, Val Acc: 0.4571\n",
      "Epoch 342, Train Loss: 0.6888, Train Acc: 0.5155, Val Loss: 0.6897, Val Acc: 0.4492\n",
      "Epoch 343, Train Loss: 0.6889, Train Acc: 0.5019, Val Loss: 0.6895, Val Acc: 0.4497\n",
      "Epoch 344, Train Loss: 0.6853, Train Acc: 0.5320, Val Loss: 0.6898, Val Acc: 0.4478\n",
      "Epoch 345, Train Loss: 0.6851, Train Acc: 0.5223, Val Loss: 0.6891, Val Acc: 0.4566\n",
      "Epoch 346, Train Loss: 0.6852, Train Acc: 0.5271, Val Loss: 0.6893, Val Acc: 0.4544\n",
      "Epoch 347, Train Loss: 0.6816, Train Acc: 0.5339, Val Loss: 0.6892, Val Acc: 0.4560\n",
      "Epoch 348, Train Loss: 0.6881, Train Acc: 0.5252, Val Loss: 0.6896, Val Acc: 0.4492\n",
      "Epoch 349, Train Loss: 0.6870, Train Acc: 0.5262, Val Loss: 0.6889, Val Acc: 0.4588\n",
      "Epoch 350, Train Loss: 0.6847, Train Acc: 0.5320, Val Loss: 0.6892, Val Acc: 0.4558\n",
      "Epoch 351, Train Loss: 0.6834, Train Acc: 0.5523, Val Loss: 0.6895, Val Acc: 0.4538\n",
      "Epoch 352, Train Loss: 0.6888, Train Acc: 0.5165, Val Loss: 0.6894, Val Acc: 0.4544\n",
      "Epoch 353, Train Loss: 0.6861, Train Acc: 0.5184, Val Loss: 0.6894, Val Acc: 0.4558\n",
      "Epoch 354, Train Loss: 0.6864, Train Acc: 0.5233, Val Loss: 0.6891, Val Acc: 0.4555\n",
      "Epoch 355, Train Loss: 0.6868, Train Acc: 0.5271, Val Loss: 0.6890, Val Acc: 0.4577\n",
      "Epoch 356, Train Loss: 0.6906, Train Acc: 0.5107, Val Loss: 0.6895, Val Acc: 0.4500\n",
      "Epoch 357, Train Loss: 0.6826, Train Acc: 0.5417, Val Loss: 0.6898, Val Acc: 0.4481\n",
      "Epoch 358, Train Loss: 0.6857, Train Acc: 0.5281, Val Loss: 0.6896, Val Acc: 0.4489\n",
      "Epoch 359, Train Loss: 0.6833, Train Acc: 0.5310, Val Loss: 0.6897, Val Acc: 0.4481\n",
      "Epoch 360, Train Loss: 0.6895, Train Acc: 0.5058, Val Loss: 0.6892, Val Acc: 0.4536\n",
      "Epoch 361, Train Loss: 0.6883, Train Acc: 0.5194, Val Loss: 0.6894, Val Acc: 0.4519\n",
      "Epoch 362, Train Loss: 0.6876, Train Acc: 0.5271, Val Loss: 0.6891, Val Acc: 0.4580\n",
      "Epoch 363, Train Loss: 0.6864, Train Acc: 0.5320, Val Loss: 0.6893, Val Acc: 0.4563\n",
      "Epoch 364, Train Loss: 0.6893, Train Acc: 0.5262, Val Loss: 0.6891, Val Acc: 0.4560\n",
      "Epoch 365, Train Loss: 0.6880, Train Acc: 0.5203, Val Loss: 0.6893, Val Acc: 0.4566\n",
      "Epoch 366, Train Loss: 0.6861, Train Acc: 0.5300, Val Loss: 0.6887, Val Acc: 0.4618\n",
      "Epoch 367, Train Loss: 0.6882, Train Acc: 0.5068, Val Loss: 0.6896, Val Acc: 0.4495\n",
      "Epoch 368, Train Loss: 0.6879, Train Acc: 0.5213, Val Loss: 0.6895, Val Acc: 0.4525\n",
      "Epoch 369, Train Loss: 0.6906, Train Acc: 0.5116, Val Loss: 0.6895, Val Acc: 0.4514\n",
      "Epoch 370, Train Loss: 0.6850, Train Acc: 0.5349, Val Loss: 0.6891, Val Acc: 0.4569\n",
      "Epoch 371, Train Loss: 0.6843, Train Acc: 0.5233, Val Loss: 0.6899, Val Acc: 0.4412\n",
      "Epoch 372, Train Loss: 0.6871, Train Acc: 0.5223, Val Loss: 0.6896, Val Acc: 0.4495\n",
      "Epoch 373, Train Loss: 0.6853, Train Acc: 0.5349, Val Loss: 0.6895, Val Acc: 0.4514\n",
      "Epoch 374, Train Loss: 0.6857, Train Acc: 0.5155, Val Loss: 0.6892, Val Acc: 0.4560\n",
      "Epoch 375, Train Loss: 0.6895, Train Acc: 0.5233, Val Loss: 0.6894, Val Acc: 0.4525\n",
      "Epoch 376, Train Loss: 0.6849, Train Acc: 0.5397, Val Loss: 0.6895, Val Acc: 0.4533\n",
      "Epoch 377, Train Loss: 0.6865, Train Acc: 0.5359, Val Loss: 0.6895, Val Acc: 0.4514\n",
      "Epoch 378, Train Loss: 0.6877, Train Acc: 0.5174, Val Loss: 0.6894, Val Acc: 0.4530\n",
      "Epoch 379, Train Loss: 0.6882, Train Acc: 0.5203, Val Loss: 0.6892, Val Acc: 0.4563\n",
      "Epoch 380, Train Loss: 0.6865, Train Acc: 0.5368, Val Loss: 0.6896, Val Acc: 0.4497\n",
      "Epoch 381, Train Loss: 0.6872, Train Acc: 0.5310, Val Loss: 0.6891, Val Acc: 0.4582\n",
      "Epoch 382, Train Loss: 0.6851, Train Acc: 0.5194, Val Loss: 0.6894, Val Acc: 0.4536\n",
      "Epoch 383, Train Loss: 0.6855, Train Acc: 0.5368, Val Loss: 0.6895, Val Acc: 0.4530\n",
      "Epoch 384, Train Loss: 0.6834, Train Acc: 0.5562, Val Loss: 0.6892, Val Acc: 0.4549\n",
      "Epoch 385, Train Loss: 0.6893, Train Acc: 0.5174, Val Loss: 0.6898, Val Acc: 0.4481\n",
      "Epoch 386, Train Loss: 0.6894, Train Acc: 0.5126, Val Loss: 0.6891, Val Acc: 0.4585\n",
      "Epoch 387, Train Loss: 0.6862, Train Acc: 0.5426, Val Loss: 0.6894, Val Acc: 0.4522\n",
      "Epoch 388, Train Loss: 0.6888, Train Acc: 0.5213, Val Loss: 0.6896, Val Acc: 0.4489\n",
      "Epoch 389, Train Loss: 0.6903, Train Acc: 0.5097, Val Loss: 0.6894, Val Acc: 0.4519\n",
      "Epoch 390, Train Loss: 0.6865, Train Acc: 0.5194, Val Loss: 0.6891, Val Acc: 0.4582\n",
      "Epoch 391, Train Loss: 0.6861, Train Acc: 0.5242, Val Loss: 0.6897, Val Acc: 0.4475\n",
      "Epoch 392, Train Loss: 0.6833, Train Acc: 0.5339, Val Loss: 0.6894, Val Acc: 0.4560\n",
      "Epoch 393, Train Loss: 0.6902, Train Acc: 0.5126, Val Loss: 0.6892, Val Acc: 0.4571\n",
      "Epoch 394, Train Loss: 0.6877, Train Acc: 0.5271, Val Loss: 0.6895, Val Acc: 0.4505\n",
      "Epoch 395, Train Loss: 0.6852, Train Acc: 0.5349, Val Loss: 0.6898, Val Acc: 0.4459\n",
      "Epoch 396, Train Loss: 0.6878, Train Acc: 0.5291, Val Loss: 0.6896, Val Acc: 0.4486\n",
      "Epoch 397, Train Loss: 0.6927, Train Acc: 0.5184, Val Loss: 0.6894, Val Acc: 0.4514\n",
      "Epoch 398, Train Loss: 0.6842, Train Acc: 0.5494, Val Loss: 0.6894, Val Acc: 0.4530\n",
      "Epoch 399, Train Loss: 0.6866, Train Acc: 0.5058, Val Loss: 0.6894, Val Acc: 0.4497\n",
      "Epoch 400, Train Loss: 0.6832, Train Acc: 0.5455, Val Loss: 0.6896, Val Acc: 0.4497\n",
      "Epoch 401, Train Loss: 0.6894, Train Acc: 0.5107, Val Loss: 0.6894, Val Acc: 0.4552\n",
      "Epoch 402, Train Loss: 0.6832, Train Acc: 0.5417, Val Loss: 0.6897, Val Acc: 0.4486\n",
      "Epoch 403, Train Loss: 0.6862, Train Acc: 0.5310, Val Loss: 0.6895, Val Acc: 0.4508\n",
      "Epoch 404, Train Loss: 0.6894, Train Acc: 0.4971, Val Loss: 0.6891, Val Acc: 0.4577\n",
      "Epoch 405, Train Loss: 0.6934, Train Acc: 0.4981, Val Loss: 0.6893, Val Acc: 0.4566\n",
      "Epoch 406, Train Loss: 0.6848, Train Acc: 0.5271, Val Loss: 0.6892, Val Acc: 0.4558\n",
      "Epoch 407, Train Loss: 0.6885, Train Acc: 0.5291, Val Loss: 0.6891, Val Acc: 0.4569\n",
      "Epoch 408, Train Loss: 0.6900, Train Acc: 0.5223, Val Loss: 0.6892, Val Acc: 0.4569\n",
      "Epoch 409, Train Loss: 0.6881, Train Acc: 0.4981, Val Loss: 0.6894, Val Acc: 0.4536\n",
      "Epoch 410, Train Loss: 0.6840, Train Acc: 0.5359, Val Loss: 0.6891, Val Acc: 0.4585\n",
      "Epoch 411, Train Loss: 0.6869, Train Acc: 0.5184, Val Loss: 0.6896, Val Acc: 0.4489\n",
      "Epoch 412, Train Loss: 0.6888, Train Acc: 0.5107, Val Loss: 0.6892, Val Acc: 0.4560\n",
      "Epoch 413, Train Loss: 0.6845, Train Acc: 0.5320, Val Loss: 0.6895, Val Acc: 0.4519\n",
      "Epoch 414, Train Loss: 0.6883, Train Acc: 0.5194, Val Loss: 0.6896, Val Acc: 0.4497\n",
      "Epoch 415, Train Loss: 0.6814, Train Acc: 0.5378, Val Loss: 0.6888, Val Acc: 0.4618\n",
      "Epoch 416, Train Loss: 0.6881, Train Acc: 0.5107, Val Loss: 0.6893, Val Acc: 0.4549\n",
      "Epoch 417, Train Loss: 0.6887, Train Acc: 0.5155, Val Loss: 0.6893, Val Acc: 0.4552\n",
      "Epoch 418, Train Loss: 0.6925, Train Acc: 0.5058, Val Loss: 0.6892, Val Acc: 0.4582\n",
      "Epoch 419, Train Loss: 0.6838, Train Acc: 0.5300, Val Loss: 0.6894, Val Acc: 0.4547\n",
      "Epoch 420, Train Loss: 0.6914, Train Acc: 0.5145, Val Loss: 0.6893, Val Acc: 0.4558\n",
      "Epoch 421, Train Loss: 0.6856, Train Acc: 0.5165, Val Loss: 0.6893, Val Acc: 0.4560\n",
      "Epoch 422, Train Loss: 0.6850, Train Acc: 0.5262, Val Loss: 0.6889, Val Acc: 0.4580\n",
      "Epoch 423, Train Loss: 0.6829, Train Acc: 0.5281, Val Loss: 0.6894, Val Acc: 0.4533\n",
      "Epoch 424, Train Loss: 0.6864, Train Acc: 0.5329, Val Loss: 0.6887, Val Acc: 0.4643\n",
      "Epoch 425, Train Loss: 0.6836, Train Acc: 0.5281, Val Loss: 0.6890, Val Acc: 0.4566\n",
      "Epoch 426, Train Loss: 0.6820, Train Acc: 0.5562, Val Loss: 0.6891, Val Acc: 0.4582\n",
      "Epoch 427, Train Loss: 0.6883, Train Acc: 0.5213, Val Loss: 0.6896, Val Acc: 0.4489\n",
      "Epoch 428, Train Loss: 0.6868, Train Acc: 0.5262, Val Loss: 0.6892, Val Acc: 0.4547\n",
      "Epoch 429, Train Loss: 0.6859, Train Acc: 0.5291, Val Loss: 0.6890, Val Acc: 0.4582\n",
      "Epoch 430, Train Loss: 0.6836, Train Acc: 0.5291, Val Loss: 0.6894, Val Acc: 0.4522\n",
      "Epoch 431, Train Loss: 0.6877, Train Acc: 0.5223, Val Loss: 0.6892, Val Acc: 0.4571\n",
      "Epoch 432, Train Loss: 0.6914, Train Acc: 0.5029, Val Loss: 0.6891, Val Acc: 0.4560\n",
      "Epoch 433, Train Loss: 0.6855, Train Acc: 0.5407, Val Loss: 0.6893, Val Acc: 0.4555\n",
      "Epoch 434, Train Loss: 0.6897, Train Acc: 0.5116, Val Loss: 0.6894, Val Acc: 0.4536\n",
      "Epoch 435, Train Loss: 0.6884, Train Acc: 0.5271, Val Loss: 0.6891, Val Acc: 0.4588\n",
      "Epoch 436, Train Loss: 0.6883, Train Acc: 0.5194, Val Loss: 0.6897, Val Acc: 0.4481\n",
      "Epoch 437, Train Loss: 0.6896, Train Acc: 0.5194, Val Loss: 0.6893, Val Acc: 0.4552\n",
      "Epoch 438, Train Loss: 0.6860, Train Acc: 0.5194, Val Loss: 0.6894, Val Acc: 0.4522\n",
      "Epoch 439, Train Loss: 0.6876, Train Acc: 0.5417, Val Loss: 0.6894, Val Acc: 0.4519\n",
      "Epoch 440, Train Loss: 0.6868, Train Acc: 0.5116, Val Loss: 0.6890, Val Acc: 0.4585\n",
      "Epoch 441, Train Loss: 0.6850, Train Acc: 0.5252, Val Loss: 0.6892, Val Acc: 0.4566\n",
      "Epoch 442, Train Loss: 0.6866, Train Acc: 0.5097, Val Loss: 0.6894, Val Acc: 0.4547\n",
      "Epoch 443, Train Loss: 0.6909, Train Acc: 0.5320, Val Loss: 0.6893, Val Acc: 0.4536\n",
      "Epoch 444, Train Loss: 0.6861, Train Acc: 0.5194, Val Loss: 0.6891, Val Acc: 0.4571\n",
      "Epoch 445, Train Loss: 0.6884, Train Acc: 0.5300, Val Loss: 0.6891, Val Acc: 0.4574\n",
      "Epoch 446, Train Loss: 0.6880, Train Acc: 0.5155, Val Loss: 0.6898, Val Acc: 0.4445\n",
      "Epoch 447, Train Loss: 0.6875, Train Acc: 0.5155, Val Loss: 0.6892, Val Acc: 0.4566\n",
      "Epoch 448, Train Loss: 0.6898, Train Acc: 0.5155, Val Loss: 0.6895, Val Acc: 0.4497\n",
      "Epoch 449, Train Loss: 0.6906, Train Acc: 0.5155, Val Loss: 0.6891, Val Acc: 0.4563\n",
      "Epoch 450, Train Loss: 0.6866, Train Acc: 0.5300, Val Loss: 0.6890, Val Acc: 0.4571\n",
      "Epoch 451, Train Loss: 0.6868, Train Acc: 0.5145, Val Loss: 0.6894, Val Acc: 0.4519\n",
      "Epoch 452, Train Loss: 0.6879, Train Acc: 0.5048, Val Loss: 0.6893, Val Acc: 0.4563\n",
      "Epoch 453, Train Loss: 0.6928, Train Acc: 0.4816, Val Loss: 0.6893, Val Acc: 0.4555\n",
      "Epoch 454, Train Loss: 0.6842, Train Acc: 0.5310, Val Loss: 0.6892, Val Acc: 0.4571\n",
      "Epoch 455, Train Loss: 0.6812, Train Acc: 0.5523, Val Loss: 0.6891, Val Acc: 0.4569\n",
      "Epoch 456, Train Loss: 0.6823, Train Acc: 0.5388, Val Loss: 0.6894, Val Acc: 0.4555\n",
      "Epoch 457, Train Loss: 0.6906, Train Acc: 0.5271, Val Loss: 0.6896, Val Acc: 0.4489\n",
      "Epoch 458, Train Loss: 0.6834, Train Acc: 0.5368, Val Loss: 0.6894, Val Acc: 0.4519\n",
      "Epoch 459, Train Loss: 0.6810, Train Acc: 0.5339, Val Loss: 0.6895, Val Acc: 0.4522\n",
      "Epoch 460, Train Loss: 0.6893, Train Acc: 0.5165, Val Loss: 0.6892, Val Acc: 0.4547\n",
      "Epoch 461, Train Loss: 0.6887, Train Acc: 0.5174, Val Loss: 0.6896, Val Acc: 0.4503\n",
      "Epoch 462, Train Loss: 0.6859, Train Acc: 0.5252, Val Loss: 0.6890, Val Acc: 0.4569\n",
      "Epoch 463, Train Loss: 0.6831, Train Acc: 0.5388, Val Loss: 0.6891, Val Acc: 0.4555\n",
      "Epoch 464, Train Loss: 0.6894, Train Acc: 0.5039, Val Loss: 0.6894, Val Acc: 0.4541\n",
      "Epoch 465, Train Loss: 0.6846, Train Acc: 0.5155, Val Loss: 0.6891, Val Acc: 0.4574\n",
      "Epoch 466, Train Loss: 0.6917, Train Acc: 0.5087, Val Loss: 0.6894, Val Acc: 0.4549\n",
      "Epoch 467, Train Loss: 0.6897, Train Acc: 0.5145, Val Loss: 0.6888, Val Acc: 0.4602\n",
      "Epoch 468, Train Loss: 0.6907, Train Acc: 0.5058, Val Loss: 0.6894, Val Acc: 0.4530\n",
      "Epoch 469, Train Loss: 0.6845, Train Acc: 0.5281, Val Loss: 0.6895, Val Acc: 0.4503\n",
      "Epoch 470, Train Loss: 0.6889, Train Acc: 0.5145, Val Loss: 0.6892, Val Acc: 0.4566\n",
      "Epoch 471, Train Loss: 0.6867, Train Acc: 0.5310, Val Loss: 0.6892, Val Acc: 0.4577\n",
      "Epoch 472, Train Loss: 0.6812, Train Acc: 0.5601, Val Loss: 0.6889, Val Acc: 0.4591\n",
      "Epoch 473, Train Loss: 0.6898, Train Acc: 0.5155, Val Loss: 0.6893, Val Acc: 0.4538\n",
      "Epoch 474, Train Loss: 0.6872, Train Acc: 0.5329, Val Loss: 0.6894, Val Acc: 0.4525\n",
      "Epoch 475, Train Loss: 0.6881, Train Acc: 0.5242, Val Loss: 0.6894, Val Acc: 0.4527\n",
      "Epoch 476, Train Loss: 0.6852, Train Acc: 0.5291, Val Loss: 0.6896, Val Acc: 0.4511\n",
      "Epoch 477, Train Loss: 0.6804, Train Acc: 0.5514, Val Loss: 0.6892, Val Acc: 0.4569\n",
      "Epoch 478, Train Loss: 0.6856, Train Acc: 0.5543, Val Loss: 0.6892, Val Acc: 0.4574\n",
      "Epoch 479, Train Loss: 0.6874, Train Acc: 0.5184, Val Loss: 0.6896, Val Acc: 0.4492\n",
      "Epoch 480, Train Loss: 0.6833, Train Acc: 0.5368, Val Loss: 0.6896, Val Acc: 0.4495\n",
      "Epoch 481, Train Loss: 0.6848, Train Acc: 0.5281, Val Loss: 0.6897, Val Acc: 0.4484\n",
      "Epoch 482, Train Loss: 0.6923, Train Acc: 0.5145, Val Loss: 0.6892, Val Acc: 0.4549\n",
      "Epoch 483, Train Loss: 0.6870, Train Acc: 0.5213, Val Loss: 0.6889, Val Acc: 0.4591\n",
      "Epoch 484, Train Loss: 0.6858, Train Acc: 0.5329, Val Loss: 0.6893, Val Acc: 0.4549\n",
      "Epoch 485, Train Loss: 0.6870, Train Acc: 0.5271, Val Loss: 0.6892, Val Acc: 0.4566\n",
      "Epoch 486, Train Loss: 0.6903, Train Acc: 0.5039, Val Loss: 0.6898, Val Acc: 0.4473\n",
      "Epoch 487, Train Loss: 0.6835, Train Acc: 0.5407, Val Loss: 0.6895, Val Acc: 0.4500\n",
      "Epoch 488, Train Loss: 0.6884, Train Acc: 0.5078, Val Loss: 0.6893, Val Acc: 0.4522\n",
      "Epoch 489, Train Loss: 0.6841, Train Acc: 0.5514, Val Loss: 0.6889, Val Acc: 0.4602\n",
      "Epoch 490, Train Loss: 0.6870, Train Acc: 0.5213, Val Loss: 0.6896, Val Acc: 0.4492\n",
      "Epoch 491, Train Loss: 0.6893, Train Acc: 0.5213, Val Loss: 0.6895, Val Acc: 0.4500\n",
      "Epoch 492, Train Loss: 0.6839, Train Acc: 0.5068, Val Loss: 0.6891, Val Acc: 0.4552\n",
      "Epoch 493, Train Loss: 0.6870, Train Acc: 0.5242, Val Loss: 0.6895, Val Acc: 0.4500\n",
      "Epoch 494, Train Loss: 0.6831, Train Acc: 0.5320, Val Loss: 0.6898, Val Acc: 0.4467\n",
      "Epoch 495, Train Loss: 0.6882, Train Acc: 0.5281, Val Loss: 0.6895, Val Acc: 0.4508\n",
      "Epoch 496, Train Loss: 0.6849, Train Acc: 0.5213, Val Loss: 0.6893, Val Acc: 0.4536\n",
      "Epoch 497, Train Loss: 0.6853, Train Acc: 0.5329, Val Loss: 0.6890, Val Acc: 0.4580\n",
      "Epoch 498, Train Loss: 0.6876, Train Acc: 0.5223, Val Loss: 0.6897, Val Acc: 0.4497\n",
      "Epoch 499, Train Loss: 0.6875, Train Acc: 0.5271, Val Loss: 0.6894, Val Acc: 0.4522\n",
      "Epoch 500, Train Loss: 0.6862, Train Acc: 0.5359, Val Loss: 0.6892, Val Acc: 0.4566\n",
      "Epoch 501, Train Loss: 0.6871, Train Acc: 0.5271, Val Loss: 0.6898, Val Acc: 0.4495\n",
      "Epoch 502, Train Loss: 0.6869, Train Acc: 0.5068, Val Loss: 0.6891, Val Acc: 0.4574\n",
      "Epoch 503, Train Loss: 0.6860, Train Acc: 0.5426, Val Loss: 0.6895, Val Acc: 0.4503\n",
      "Epoch 504, Train Loss: 0.6826, Train Acc: 0.5281, Val Loss: 0.6891, Val Acc: 0.4563\n",
      "Epoch 505, Train Loss: 0.6772, Train Acc: 0.5630, Val Loss: 0.6896, Val Acc: 0.4489\n",
      "Epoch 506, Train Loss: 0.6869, Train Acc: 0.5552, Val Loss: 0.6895, Val Acc: 0.4489\n",
      "Epoch 507, Train Loss: 0.6866, Train Acc: 0.5300, Val Loss: 0.6897, Val Acc: 0.4481\n",
      "Epoch 508, Train Loss: 0.6832, Train Acc: 0.5310, Val Loss: 0.6889, Val Acc: 0.4610\n",
      "Epoch 509, Train Loss: 0.6840, Train Acc: 0.5262, Val Loss: 0.6897, Val Acc: 0.4481\n",
      "Epoch 510, Train Loss: 0.6886, Train Acc: 0.5116, Val Loss: 0.6893, Val Acc: 0.4541\n",
      "Epoch 511, Train Loss: 0.6899, Train Acc: 0.5145, Val Loss: 0.6896, Val Acc: 0.4486\n",
      "Epoch 512, Train Loss: 0.6873, Train Acc: 0.5300, Val Loss: 0.6895, Val Acc: 0.4486\n",
      "Epoch 513, Train Loss: 0.6856, Train Acc: 0.5271, Val Loss: 0.6891, Val Acc: 0.4560\n",
      "Epoch 514, Train Loss: 0.6821, Train Acc: 0.5446, Val Loss: 0.6894, Val Acc: 0.4536\n",
      "Epoch 515, Train Loss: 0.6888, Train Acc: 0.5291, Val Loss: 0.6893, Val Acc: 0.4547\n",
      "Epoch 516, Train Loss: 0.6860, Train Acc: 0.5436, Val Loss: 0.6893, Val Acc: 0.4533\n",
      "Epoch 517, Train Loss: 0.6881, Train Acc: 0.5165, Val Loss: 0.6892, Val Acc: 0.4563\n",
      "Epoch 518, Train Loss: 0.6866, Train Acc: 0.5281, Val Loss: 0.6892, Val Acc: 0.4571\n",
      "Epoch 519, Train Loss: 0.6830, Train Acc: 0.5329, Val Loss: 0.6891, Val Acc: 0.4580\n",
      "Epoch 520, Train Loss: 0.6862, Train Acc: 0.5165, Val Loss: 0.6891, Val Acc: 0.4571\n",
      "Epoch 521, Train Loss: 0.6836, Train Acc: 0.5291, Val Loss: 0.6890, Val Acc: 0.4602\n",
      "Epoch 522, Train Loss: 0.6887, Train Acc: 0.5087, Val Loss: 0.6893, Val Acc: 0.4566\n",
      "Epoch 523, Train Loss: 0.6885, Train Acc: 0.5116, Val Loss: 0.6894, Val Acc: 0.4541\n",
      "Epoch 524, Train Loss: 0.6891, Train Acc: 0.5078, Val Loss: 0.6895, Val Acc: 0.4522\n",
      "Epoch 525, Train Loss: 0.6855, Train Acc: 0.5339, Val Loss: 0.6892, Val Acc: 0.4569\n",
      "Epoch 526, Train Loss: 0.6863, Train Acc: 0.5281, Val Loss: 0.6892, Val Acc: 0.4560\n",
      "Epoch 527, Train Loss: 0.6868, Train Acc: 0.5320, Val Loss: 0.6891, Val Acc: 0.4577\n",
      "Epoch 528, Train Loss: 0.6841, Train Acc: 0.5494, Val Loss: 0.6893, Val Acc: 0.4552\n",
      "Epoch 529, Train Loss: 0.6877, Train Acc: 0.5213, Val Loss: 0.6901, Val Acc: 0.4418\n",
      "Epoch 530, Train Loss: 0.6873, Train Acc: 0.5155, Val Loss: 0.6892, Val Acc: 0.4574\n",
      "Epoch 531, Train Loss: 0.6883, Train Acc: 0.5310, Val Loss: 0.6889, Val Acc: 0.4571\n",
      "Epoch 532, Train Loss: 0.6846, Train Acc: 0.5320, Val Loss: 0.6898, Val Acc: 0.4459\n",
      "Epoch 533, Train Loss: 0.6888, Train Acc: 0.5233, Val Loss: 0.6894, Val Acc: 0.4538\n",
      "Epoch 534, Train Loss: 0.6849, Train Acc: 0.5271, Val Loss: 0.6892, Val Acc: 0.4544\n",
      "Epoch 535, Train Loss: 0.6907, Train Acc: 0.5087, Val Loss: 0.6891, Val Acc: 0.4574\n",
      "Epoch 536, Train Loss: 0.6852, Train Acc: 0.5184, Val Loss: 0.6897, Val Acc: 0.4492\n",
      "Epoch 537, Train Loss: 0.6859, Train Acc: 0.5349, Val Loss: 0.6891, Val Acc: 0.4577\n",
      "Epoch 538, Train Loss: 0.6888, Train Acc: 0.5174, Val Loss: 0.6890, Val Acc: 0.4585\n",
      "Epoch 539, Train Loss: 0.6858, Train Acc: 0.5320, Val Loss: 0.6888, Val Acc: 0.4610\n",
      "Epoch 540, Train Loss: 0.6868, Train Acc: 0.5126, Val Loss: 0.6889, Val Acc: 0.4580\n",
      "Epoch 541, Train Loss: 0.6876, Train Acc: 0.5359, Val Loss: 0.6897, Val Acc: 0.4475\n",
      "Epoch 542, Train Loss: 0.6898, Train Acc: 0.4981, Val Loss: 0.6892, Val Acc: 0.4571\n",
      "Epoch 543, Train Loss: 0.6877, Train Acc: 0.5165, Val Loss: 0.6891, Val Acc: 0.4569\n",
      "Epoch 544, Train Loss: 0.6883, Train Acc: 0.5203, Val Loss: 0.6892, Val Acc: 0.4569\n",
      "Epoch 545, Train Loss: 0.6855, Train Acc: 0.5359, Val Loss: 0.6896, Val Acc: 0.4497\n",
      "Epoch 546, Train Loss: 0.6845, Train Acc: 0.5349, Val Loss: 0.6894, Val Acc: 0.4516\n",
      "Epoch 547, Train Loss: 0.6871, Train Acc: 0.5281, Val Loss: 0.6895, Val Acc: 0.4533\n",
      "Epoch 548, Train Loss: 0.6882, Train Acc: 0.5116, Val Loss: 0.6895, Val Acc: 0.4519\n",
      "Epoch 549, Train Loss: 0.6858, Train Acc: 0.5155, Val Loss: 0.6895, Val Acc: 0.4516\n",
      "Epoch 550, Train Loss: 0.6862, Train Acc: 0.5475, Val Loss: 0.6893, Val Acc: 0.4538\n",
      "Epoch 551, Train Loss: 0.6892, Train Acc: 0.5262, Val Loss: 0.6893, Val Acc: 0.4536\n",
      "Epoch 552, Train Loss: 0.6877, Train Acc: 0.5242, Val Loss: 0.6891, Val Acc: 0.4549\n",
      "Epoch 553, Train Loss: 0.6860, Train Acc: 0.5291, Val Loss: 0.6893, Val Acc: 0.4522\n",
      "Epoch 554, Train Loss: 0.6849, Train Acc: 0.5165, Val Loss: 0.6893, Val Acc: 0.4527\n",
      "Epoch 555, Train Loss: 0.6879, Train Acc: 0.5233, Val Loss: 0.6894, Val Acc: 0.4536\n",
      "Epoch 556, Train Loss: 0.6862, Train Acc: 0.5320, Val Loss: 0.6891, Val Acc: 0.4571\n",
      "Epoch 557, Train Loss: 0.6883, Train Acc: 0.5097, Val Loss: 0.6896, Val Acc: 0.4489\n",
      "Epoch 558, Train Loss: 0.6886, Train Acc: 0.5203, Val Loss: 0.6896, Val Acc: 0.4486\n",
      "Epoch 559, Train Loss: 0.6844, Train Acc: 0.5397, Val Loss: 0.6889, Val Acc: 0.4596\n",
      "Epoch 560, Train Loss: 0.6866, Train Acc: 0.5281, Val Loss: 0.6891, Val Acc: 0.4566\n",
      "Epoch 561, Train Loss: 0.6857, Train Acc: 0.5329, Val Loss: 0.6891, Val Acc: 0.4577\n",
      "Epoch 562, Train Loss: 0.6893, Train Acc: 0.5262, Val Loss: 0.6892, Val Acc: 0.4552\n",
      "Epoch 563, Train Loss: 0.6830, Train Acc: 0.5281, Val Loss: 0.6895, Val Acc: 0.4522\n",
      "Epoch 564, Train Loss: 0.6795, Train Acc: 0.5736, Val Loss: 0.6889, Val Acc: 0.4626\n",
      "Epoch 565, Train Loss: 0.6866, Train Acc: 0.5329, Val Loss: 0.6895, Val Acc: 0.4516\n",
      "Epoch 566, Train Loss: 0.6886, Train Acc: 0.5271, Val Loss: 0.6893, Val Acc: 0.4569\n",
      "Epoch 567, Train Loss: 0.6893, Train Acc: 0.5213, Val Loss: 0.6890, Val Acc: 0.4593\n",
      "Epoch 568, Train Loss: 0.6822, Train Acc: 0.5388, Val Loss: 0.6897, Val Acc: 0.4489\n",
      "Epoch 569, Train Loss: 0.6841, Train Acc: 0.5388, Val Loss: 0.6894, Val Acc: 0.4547\n",
      "Epoch 570, Train Loss: 0.6812, Train Acc: 0.5514, Val Loss: 0.6893, Val Acc: 0.4574\n",
      "Epoch 571, Train Loss: 0.6877, Train Acc: 0.5339, Val Loss: 0.6894, Val Acc: 0.4530\n",
      "Epoch 572, Train Loss: 0.6898, Train Acc: 0.5087, Val Loss: 0.6892, Val Acc: 0.4560\n",
      "Epoch 573, Train Loss: 0.6876, Train Acc: 0.5184, Val Loss: 0.6895, Val Acc: 0.4505\n",
      "Epoch 574, Train Loss: 0.6832, Train Acc: 0.5242, Val Loss: 0.6891, Val Acc: 0.4563\n",
      "Epoch 575, Train Loss: 0.6868, Train Acc: 0.5271, Val Loss: 0.6896, Val Acc: 0.4486\n",
      "Epoch 576, Train Loss: 0.6862, Train Acc: 0.5465, Val Loss: 0.6897, Val Acc: 0.4486\n",
      "Epoch 577, Train Loss: 0.6860, Train Acc: 0.5291, Val Loss: 0.6896, Val Acc: 0.4489\n",
      "Epoch 578, Train Loss: 0.6840, Train Acc: 0.5310, Val Loss: 0.6894, Val Acc: 0.4503\n",
      "Epoch 579, Train Loss: 0.6888, Train Acc: 0.5087, Val Loss: 0.6891, Val Acc: 0.4574\n",
      "Epoch 580, Train Loss: 0.6872, Train Acc: 0.5252, Val Loss: 0.6893, Val Acc: 0.4571\n",
      "Epoch 581, Train Loss: 0.6906, Train Acc: 0.5068, Val Loss: 0.6896, Val Acc: 0.4503\n",
      "Epoch 582, Train Loss: 0.6871, Train Acc: 0.5194, Val Loss: 0.6888, Val Acc: 0.4618\n",
      "Epoch 583, Train Loss: 0.6849, Train Acc: 0.5300, Val Loss: 0.6893, Val Acc: 0.4555\n",
      "Epoch 584, Train Loss: 0.6822, Train Acc: 0.5388, Val Loss: 0.6894, Val Acc: 0.4505\n",
      "Epoch 585, Train Loss: 0.6878, Train Acc: 0.5252, Val Loss: 0.6894, Val Acc: 0.4547\n",
      "Epoch 586, Train Loss: 0.6859, Train Acc: 0.5291, Val Loss: 0.6892, Val Acc: 0.4566\n",
      "Epoch 587, Train Loss: 0.6897, Train Acc: 0.5203, Val Loss: 0.6897, Val Acc: 0.4489\n",
      "Epoch 588, Train Loss: 0.6870, Train Acc: 0.5310, Val Loss: 0.6894, Val Acc: 0.4519\n",
      "Epoch 589, Train Loss: 0.6871, Train Acc: 0.5378, Val Loss: 0.6894, Val Acc: 0.4538\n",
      "Epoch 590, Train Loss: 0.6866, Train Acc: 0.5194, Val Loss: 0.6892, Val Acc: 0.4569\n",
      "Epoch 591, Train Loss: 0.6916, Train Acc: 0.5097, Val Loss: 0.6890, Val Acc: 0.4588\n",
      "Epoch 592, Train Loss: 0.6820, Train Acc: 0.5523, Val Loss: 0.6893, Val Acc: 0.4544\n",
      "Epoch 593, Train Loss: 0.6857, Train Acc: 0.5426, Val Loss: 0.6890, Val Acc: 0.4580\n",
      "Epoch 594, Train Loss: 0.6857, Train Acc: 0.5368, Val Loss: 0.6894, Val Acc: 0.4527\n",
      "Epoch 595, Train Loss: 0.6852, Train Acc: 0.5300, Val Loss: 0.6892, Val Acc: 0.4563\n",
      "Epoch 596, Train Loss: 0.6882, Train Acc: 0.5359, Val Loss: 0.6889, Val Acc: 0.4596\n",
      "Epoch 597, Train Loss: 0.6858, Train Acc: 0.5252, Val Loss: 0.6891, Val Acc: 0.4566\n",
      "Epoch 598, Train Loss: 0.6868, Train Acc: 0.5320, Val Loss: 0.6897, Val Acc: 0.4492\n",
      "Epoch 599, Train Loss: 0.6879, Train Acc: 0.5155, Val Loss: 0.6888, Val Acc: 0.4582\n",
      "Epoch 600, Train Loss: 0.6842, Train Acc: 0.5533, Val Loss: 0.6895, Val Acc: 0.4495\n",
      "Epoch 601, Train Loss: 0.6823, Train Acc: 0.5533, Val Loss: 0.6891, Val Acc: 0.4563\n",
      "Epoch 602, Train Loss: 0.6886, Train Acc: 0.5281, Val Loss: 0.6890, Val Acc: 0.4574\n",
      "Epoch 603, Train Loss: 0.6850, Train Acc: 0.5126, Val Loss: 0.6893, Val Acc: 0.4538\n",
      "Epoch 604, Train Loss: 0.6826, Train Acc: 0.5378, Val Loss: 0.6891, Val Acc: 0.4563\n",
      "Epoch 605, Train Loss: 0.6881, Train Acc: 0.5155, Val Loss: 0.6898, Val Acc: 0.4462\n",
      "Epoch 606, Train Loss: 0.6881, Train Acc: 0.5174, Val Loss: 0.6894, Val Acc: 0.4544\n",
      "Epoch 607, Train Loss: 0.6923, Train Acc: 0.4932, Val Loss: 0.6893, Val Acc: 0.4569\n",
      "Epoch 608, Train Loss: 0.6910, Train Acc: 0.5029, Val Loss: 0.6894, Val Acc: 0.4519\n",
      "Epoch 609, Train Loss: 0.6829, Train Acc: 0.5300, Val Loss: 0.6891, Val Acc: 0.4566\n",
      "Epoch 610, Train Loss: 0.6834, Train Acc: 0.5388, Val Loss: 0.6890, Val Acc: 0.4566\n",
      "Epoch 611, Train Loss: 0.6828, Train Acc: 0.5262, Val Loss: 0.6895, Val Acc: 0.4489\n",
      "Epoch 612, Train Loss: 0.6863, Train Acc: 0.5039, Val Loss: 0.6891, Val Acc: 0.4569\n",
      "Epoch 613, Train Loss: 0.6864, Train Acc: 0.5329, Val Loss: 0.6893, Val Acc: 0.4549\n",
      "Epoch 614, Train Loss: 0.6858, Train Acc: 0.5339, Val Loss: 0.6893, Val Acc: 0.4533\n",
      "Epoch 615, Train Loss: 0.6808, Train Acc: 0.5533, Val Loss: 0.6892, Val Acc: 0.4544\n",
      "Epoch 616, Train Loss: 0.6868, Train Acc: 0.5203, Val Loss: 0.6894, Val Acc: 0.4538\n",
      "Epoch 617, Train Loss: 0.6828, Train Acc: 0.5281, Val Loss: 0.6890, Val Acc: 0.4582\n",
      "Epoch 618, Train Loss: 0.6847, Train Acc: 0.5281, Val Loss: 0.6896, Val Acc: 0.4484\n",
      "Epoch 619, Train Loss: 0.6873, Train Acc: 0.5320, Val Loss: 0.6894, Val Acc: 0.4533\n",
      "Epoch 620, Train Loss: 0.6907, Train Acc: 0.5155, Val Loss: 0.6891, Val Acc: 0.4580\n",
      "Epoch 621, Train Loss: 0.6840, Train Acc: 0.5320, Val Loss: 0.6895, Val Acc: 0.4503\n",
      "Epoch 622, Train Loss: 0.6820, Train Acc: 0.5446, Val Loss: 0.6898, Val Acc: 0.4484\n",
      "Epoch 623, Train Loss: 0.6858, Train Acc: 0.5378, Val Loss: 0.6893, Val Acc: 0.4530\n",
      "Epoch 624, Train Loss: 0.6869, Train Acc: 0.5310, Val Loss: 0.6890, Val Acc: 0.4569\n",
      "Epoch 625, Train Loss: 0.6911, Train Acc: 0.4961, Val Loss: 0.6895, Val Acc: 0.4525\n",
      "Epoch 626, Train Loss: 0.6867, Train Acc: 0.5252, Val Loss: 0.6893, Val Acc: 0.4558\n",
      "Epoch 627, Train Loss: 0.6901, Train Acc: 0.5407, Val Loss: 0.6893, Val Acc: 0.4536\n",
      "Epoch 628, Train Loss: 0.6895, Train Acc: 0.5126, Val Loss: 0.6894, Val Acc: 0.4536\n",
      "Epoch 629, Train Loss: 0.6863, Train Acc: 0.5388, Val Loss: 0.6889, Val Acc: 0.4607\n",
      "Epoch 630, Train Loss: 0.6890, Train Acc: 0.5310, Val Loss: 0.6892, Val Acc: 0.4580\n",
      "Epoch 631, Train Loss: 0.6838, Train Acc: 0.5233, Val Loss: 0.6890, Val Acc: 0.4585\n",
      "Epoch 632, Train Loss: 0.6845, Train Acc: 0.5310, Val Loss: 0.6895, Val Acc: 0.4533\n",
      "Epoch 633, Train Loss: 0.6846, Train Acc: 0.5349, Val Loss: 0.6893, Val Acc: 0.4552\n",
      "Epoch 634, Train Loss: 0.6818, Train Acc: 0.5543, Val Loss: 0.6895, Val Acc: 0.4541\n",
      "Epoch 635, Train Loss: 0.6866, Train Acc: 0.5262, Val Loss: 0.6893, Val Acc: 0.4544\n",
      "Epoch 636, Train Loss: 0.6940, Train Acc: 0.4700, Val Loss: 0.6893, Val Acc: 0.4555\n",
      "Epoch 637, Train Loss: 0.6862, Train Acc: 0.5281, Val Loss: 0.6888, Val Acc: 0.4640\n",
      "Epoch 638, Train Loss: 0.6868, Train Acc: 0.5329, Val Loss: 0.6892, Val Acc: 0.4560\n",
      "Epoch 639, Train Loss: 0.6864, Train Acc: 0.5223, Val Loss: 0.6894, Val Acc: 0.4519\n",
      "Epoch 640, Train Loss: 0.6903, Train Acc: 0.5155, Val Loss: 0.6894, Val Acc: 0.4538\n",
      "Epoch 641, Train Loss: 0.6863, Train Acc: 0.5126, Val Loss: 0.6893, Val Acc: 0.4566\n",
      "Epoch 642, Train Loss: 0.6836, Train Acc: 0.5368, Val Loss: 0.6892, Val Acc: 0.4577\n",
      "Epoch 643, Train Loss: 0.6911, Train Acc: 0.5058, Val Loss: 0.6895, Val Acc: 0.4522\n",
      "Epoch 644, Train Loss: 0.6878, Train Acc: 0.5281, Val Loss: 0.6894, Val Acc: 0.4503\n",
      "Epoch 645, Train Loss: 0.6862, Train Acc: 0.5174, Val Loss: 0.6895, Val Acc: 0.4533\n",
      "Epoch 646, Train Loss: 0.6895, Train Acc: 0.5136, Val Loss: 0.6888, Val Acc: 0.4621\n",
      "Epoch 647, Train Loss: 0.6868, Train Acc: 0.5097, Val Loss: 0.6893, Val Acc: 0.4569\n",
      "Epoch 648, Train Loss: 0.6924, Train Acc: 0.5039, Val Loss: 0.6895, Val Acc: 0.4508\n",
      "Epoch 649, Train Loss: 0.6827, Train Acc: 0.5436, Val Loss: 0.6890, Val Acc: 0.4569\n",
      "Epoch 650, Train Loss: 0.6884, Train Acc: 0.5262, Val Loss: 0.6895, Val Acc: 0.4533\n",
      "Epoch 651, Train Loss: 0.6858, Train Acc: 0.5116, Val Loss: 0.6891, Val Acc: 0.4574\n",
      "Epoch 652, Train Loss: 0.6882, Train Acc: 0.5155, Val Loss: 0.6893, Val Acc: 0.4525\n",
      "Epoch 653, Train Loss: 0.6890, Train Acc: 0.5184, Val Loss: 0.6893, Val Acc: 0.4558\n",
      "Epoch 654, Train Loss: 0.6856, Train Acc: 0.5562, Val Loss: 0.6894, Val Acc: 0.4547\n",
      "Epoch 655, Train Loss: 0.6900, Train Acc: 0.5000, Val Loss: 0.6892, Val Acc: 0.4574\n",
      "Epoch 656, Train Loss: 0.6865, Train Acc: 0.5368, Val Loss: 0.6895, Val Acc: 0.4503\n",
      "Epoch 657, Train Loss: 0.6881, Train Acc: 0.5097, Val Loss: 0.6888, Val Acc: 0.4613\n",
      "Epoch 658, Train Loss: 0.6894, Train Acc: 0.5339, Val Loss: 0.6894, Val Acc: 0.4538\n",
      "Epoch 659, Train Loss: 0.6850, Train Acc: 0.5252, Val Loss: 0.6896, Val Acc: 0.4486\n",
      "Epoch 660, Train Loss: 0.6881, Train Acc: 0.5126, Val Loss: 0.6891, Val Acc: 0.4574\n",
      "Epoch 661, Train Loss: 0.6849, Train Acc: 0.5271, Val Loss: 0.6892, Val Acc: 0.4558\n",
      "Epoch 662, Train Loss: 0.6864, Train Acc: 0.5203, Val Loss: 0.6895, Val Acc: 0.4525\n",
      "Epoch 663, Train Loss: 0.6844, Train Acc: 0.5310, Val Loss: 0.6894, Val Acc: 0.4530\n",
      "Epoch 664, Train Loss: 0.6893, Train Acc: 0.5262, Val Loss: 0.6889, Val Acc: 0.4599\n",
      "Epoch 665, Train Loss: 0.6909, Train Acc: 0.5039, Val Loss: 0.6892, Val Acc: 0.4566\n",
      "Epoch 666, Train Loss: 0.6911, Train Acc: 0.5242, Val Loss: 0.6893, Val Acc: 0.4541\n",
      "Epoch 667, Train Loss: 0.6894, Train Acc: 0.5233, Val Loss: 0.6893, Val Acc: 0.4541\n",
      "Epoch 668, Train Loss: 0.6856, Train Acc: 0.5271, Val Loss: 0.6897, Val Acc: 0.4492\n",
      "Epoch 669, Train Loss: 0.6858, Train Acc: 0.5455, Val Loss: 0.6894, Val Acc: 0.4508\n",
      "Epoch 670, Train Loss: 0.6911, Train Acc: 0.5087, Val Loss: 0.6892, Val Acc: 0.4541\n",
      "Epoch 671, Train Loss: 0.6849, Train Acc: 0.5320, Val Loss: 0.6895, Val Acc: 0.4538\n",
      "Epoch 672, Train Loss: 0.6883, Train Acc: 0.5329, Val Loss: 0.6891, Val Acc: 0.4560\n",
      "Epoch 673, Train Loss: 0.6795, Train Acc: 0.5630, Val Loss: 0.6895, Val Acc: 0.4497\n",
      "Epoch 674, Train Loss: 0.6859, Train Acc: 0.5281, Val Loss: 0.6892, Val Acc: 0.4558\n",
      "Epoch 675, Train Loss: 0.6924, Train Acc: 0.4758, Val Loss: 0.6892, Val Acc: 0.4558\n",
      "Epoch 676, Train Loss: 0.6901, Train Acc: 0.4990, Val Loss: 0.6896, Val Acc: 0.4495\n",
      "Epoch 677, Train Loss: 0.6831, Train Acc: 0.5349, Val Loss: 0.6894, Val Acc: 0.4541\n",
      "Epoch 678, Train Loss: 0.6836, Train Acc: 0.5388, Val Loss: 0.6895, Val Acc: 0.4530\n",
      "Epoch 679, Train Loss: 0.6877, Train Acc: 0.5165, Val Loss: 0.6890, Val Acc: 0.4593\n",
      "Epoch 680, Train Loss: 0.6882, Train Acc: 0.5300, Val Loss: 0.6894, Val Acc: 0.4514\n",
      "Epoch 681, Train Loss: 0.6886, Train Acc: 0.5291, Val Loss: 0.6895, Val Acc: 0.4516\n",
      "Epoch 682, Train Loss: 0.6825, Train Acc: 0.5271, Val Loss: 0.6890, Val Acc: 0.4582\n",
      "Epoch 683, Train Loss: 0.6901, Train Acc: 0.5194, Val Loss: 0.6892, Val Acc: 0.4560\n",
      "Epoch 684, Train Loss: 0.6869, Train Acc: 0.5329, Val Loss: 0.6892, Val Acc: 0.4558\n",
      "Epoch 685, Train Loss: 0.6872, Train Acc: 0.5349, Val Loss: 0.6891, Val Acc: 0.4585\n",
      "Epoch 686, Train Loss: 0.6890, Train Acc: 0.5417, Val Loss: 0.6894, Val Acc: 0.4525\n",
      "Epoch 687, Train Loss: 0.6890, Train Acc: 0.5436, Val Loss: 0.6893, Val Acc: 0.4563\n",
      "Epoch 688, Train Loss: 0.6859, Train Acc: 0.5426, Val Loss: 0.6891, Val Acc: 0.4588\n",
      "Epoch 689, Train Loss: 0.6839, Train Acc: 0.5494, Val Loss: 0.6890, Val Acc: 0.4569\n",
      "Epoch 690, Train Loss: 0.6861, Train Acc: 0.5281, Val Loss: 0.6894, Val Acc: 0.4522\n",
      "Epoch 691, Train Loss: 0.6916, Train Acc: 0.5165, Val Loss: 0.6895, Val Acc: 0.4500\n",
      "Epoch 692, Train Loss: 0.6839, Train Acc: 0.5446, Val Loss: 0.6893, Val Acc: 0.4533\n",
      "Epoch 693, Train Loss: 0.6888, Train Acc: 0.5097, Val Loss: 0.6896, Val Acc: 0.4486\n",
      "Epoch 694, Train Loss: 0.6852, Train Acc: 0.5426, Val Loss: 0.6892, Val Acc: 0.4541\n",
      "Epoch 695, Train Loss: 0.6829, Train Acc: 0.5388, Val Loss: 0.6894, Val Acc: 0.4533\n",
      "Epoch 696, Train Loss: 0.6829, Train Acc: 0.5388, Val Loss: 0.6893, Val Acc: 0.4560\n",
      "Epoch 697, Train Loss: 0.6808, Train Acc: 0.5494, Val Loss: 0.6893, Val Acc: 0.4571\n",
      "Epoch 698, Train Loss: 0.6863, Train Acc: 0.5310, Val Loss: 0.6892, Val Acc: 0.4571\n",
      "Epoch 699, Train Loss: 0.6904, Train Acc: 0.5068, Val Loss: 0.6893, Val Acc: 0.4560\n",
      "Epoch 700, Train Loss: 0.6898, Train Acc: 0.4913, Val Loss: 0.6892, Val Acc: 0.4560\n",
      "Epoch 701, Train Loss: 0.6834, Train Acc: 0.5262, Val Loss: 0.6891, Val Acc: 0.4577\n",
      "Epoch 702, Train Loss: 0.6829, Train Acc: 0.5426, Val Loss: 0.6891, Val Acc: 0.4582\n",
      "Epoch 703, Train Loss: 0.6886, Train Acc: 0.5271, Val Loss: 0.6893, Val Acc: 0.4574\n",
      "Epoch 704, Train Loss: 0.6868, Train Acc: 0.5281, Val Loss: 0.6894, Val Acc: 0.4497\n",
      "Epoch 705, Train Loss: 0.6817, Train Acc: 0.5494, Val Loss: 0.6891, Val Acc: 0.4571\n",
      "Epoch 706, Train Loss: 0.6846, Train Acc: 0.5465, Val Loss: 0.6889, Val Acc: 0.4599\n",
      "Epoch 707, Train Loss: 0.6830, Train Acc: 0.5349, Val Loss: 0.6890, Val Acc: 0.4560\n",
      "Epoch 708, Train Loss: 0.6925, Train Acc: 0.5039, Val Loss: 0.6890, Val Acc: 0.4588\n",
      "Epoch 709, Train Loss: 0.6872, Train Acc: 0.5145, Val Loss: 0.6888, Val Acc: 0.4632\n",
      "Epoch 710, Train Loss: 0.6859, Train Acc: 0.5262, Val Loss: 0.6896, Val Acc: 0.4486\n",
      "Epoch 711, Train Loss: 0.6881, Train Acc: 0.5397, Val Loss: 0.6897, Val Acc: 0.4453\n",
      "Epoch 712, Train Loss: 0.6906, Train Acc: 0.4932, Val Loss: 0.6889, Val Acc: 0.4588\n",
      "Epoch 713, Train Loss: 0.6921, Train Acc: 0.4971, Val Loss: 0.6889, Val Acc: 0.4602\n",
      "Epoch 714, Train Loss: 0.6856, Train Acc: 0.5349, Val Loss: 0.6897, Val Acc: 0.4497\n",
      "Epoch 715, Train Loss: 0.6836, Train Acc: 0.5087, Val Loss: 0.6892, Val Acc: 0.4577\n",
      "Epoch 716, Train Loss: 0.6855, Train Acc: 0.5174, Val Loss: 0.6892, Val Acc: 0.4552\n",
      "Epoch 717, Train Loss: 0.6885, Train Acc: 0.5048, Val Loss: 0.6892, Val Acc: 0.4566\n",
      "Epoch 718, Train Loss: 0.6887, Train Acc: 0.5281, Val Loss: 0.6890, Val Acc: 0.4591\n",
      "Epoch 719, Train Loss: 0.6881, Train Acc: 0.5262, Val Loss: 0.6894, Val Acc: 0.4516\n",
      "Epoch 720, Train Loss: 0.6838, Train Acc: 0.5514, Val Loss: 0.6892, Val Acc: 0.4558\n",
      "Epoch 721, Train Loss: 0.6885, Train Acc: 0.5194, Val Loss: 0.6893, Val Acc: 0.4566\n",
      "Epoch 722, Train Loss: 0.6852, Train Acc: 0.5029, Val Loss: 0.6893, Val Acc: 0.4547\n",
      "Epoch 723, Train Loss: 0.6879, Train Acc: 0.5068, Val Loss: 0.6890, Val Acc: 0.4560\n",
      "Epoch 724, Train Loss: 0.6874, Train Acc: 0.5184, Val Loss: 0.6890, Val Acc: 0.4574\n",
      "Epoch 725, Train Loss: 0.6846, Train Acc: 0.5262, Val Loss: 0.6894, Val Acc: 0.4533\n",
      "Epoch 726, Train Loss: 0.6886, Train Acc: 0.5271, Val Loss: 0.6889, Val Acc: 0.4593\n",
      "Epoch 727, Train Loss: 0.6876, Train Acc: 0.5136, Val Loss: 0.6892, Val Acc: 0.4566\n",
      "Epoch 728, Train Loss: 0.6835, Train Acc: 0.5388, Val Loss: 0.6895, Val Acc: 0.4514\n",
      "Epoch 729, Train Loss: 0.6893, Train Acc: 0.5019, Val Loss: 0.6888, Val Acc: 0.4591\n",
      "Epoch 730, Train Loss: 0.6864, Train Acc: 0.5339, Val Loss: 0.6890, Val Acc: 0.4574\n",
      "Epoch 731, Train Loss: 0.6854, Train Acc: 0.5388, Val Loss: 0.6893, Val Acc: 0.4569\n",
      "Epoch 732, Train Loss: 0.6878, Train Acc: 0.5213, Val Loss: 0.6889, Val Acc: 0.4574\n",
      "Epoch 733, Train Loss: 0.6901, Train Acc: 0.4981, Val Loss: 0.6890, Val Acc: 0.4582\n",
      "Epoch 734, Train Loss: 0.6926, Train Acc: 0.5252, Val Loss: 0.6894, Val Acc: 0.4530\n",
      "Epoch 735, Train Loss: 0.6854, Train Acc: 0.5320, Val Loss: 0.6891, Val Acc: 0.4569\n",
      "Epoch 736, Train Loss: 0.6928, Train Acc: 0.4816, Val Loss: 0.6896, Val Acc: 0.4473\n",
      "Epoch 737, Train Loss: 0.6832, Train Acc: 0.5271, Val Loss: 0.6891, Val Acc: 0.4560\n",
      "Epoch 738, Train Loss: 0.6860, Train Acc: 0.5329, Val Loss: 0.6890, Val Acc: 0.4582\n",
      "Epoch 739, Train Loss: 0.6863, Train Acc: 0.5194, Val Loss: 0.6895, Val Acc: 0.4503\n",
      "Epoch 740, Train Loss: 0.6885, Train Acc: 0.5116, Val Loss: 0.6894, Val Acc: 0.4503\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>model_eval</td><td>ContextFusionCNN(<br>  ...</td></tr><tr><td>train_acc</td><td>0.68682</td></tr><tr><td>train_loss</td><td>1.20786</td></tr><tr><td>val_acc</td><td>0.65004</td></tr><tr><td>val_loss</td><td>1.22477</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ContextFusion w/o deltas on perfectly balanced train</strong> at: <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/a599d50b' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src/runs/a599d50b</a><br> View project at: <a href='https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src' target=\"_blank\">https://wandb.ai/fischbach-kamil-pg/Mandarin_Pronunciation_Recognition_Project-src</a><br>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_172046-7c3zmj0x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model variables definition.\n",
    "pth = \"SimplifiedLightweightCNN.pth\"\n",
    "lr = 1e-4  # Reduce from 1e-3\n",
    "epochs = 140\n",
    "model = model.to(device)\n",
    "reload_function(train)\n",
    "reload_function(evaluate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)  # Add L2 regularization\n",
    "# Add learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # name of the run\n",
    "    name=\"ContextFusion w/o deltas on perfectly balanced train\",\n",
    "    config={\n",
    "        \"Name\": 'SimplifiedLightweightCNN',\n",
    "        \"learning_rate\": lr,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"criterion\": \"BCELoss\",\n",
    "        \"architecture\": \"SimplifiedLightweightCNN\",\n",
    "        \"architecture_details\": str(model),\n",
    "        \"dataset\": \"Stage-I\",\n",
    "        \"train_val_test(%)\": f'{TRAIN_SPLIT}-{VAL_SPLIT}-{TEST_SPLIT}',\n",
    "        \"epochs\": epochs,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    # Logging the metadata for each epoch so that the charts can be generated on the dashboard\n",
    "    run.log({\"train_acc\": train_acc, \"train_loss\": train_loss, \"val_acc\": val_acc, \"val_loss\": val_loss, })\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "run.log({\"model_eval\": model.eval()})\n",
    "# Saving the model to pth and adding it to the artifacts of the run, there is 5GB of memory on wandb, so we should be fine.\n",
    "torch.save(model.state_dict(), os.path.join(RESULT_DIRECTORY, pth))\n",
    "artifact = wandb.Artifact(\"SimplifiedLightweightCNN-model\", type=\"model\")\n",
    "artifact.add_file(os.path.join(RESULT_DIRECTORY, pth))\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "# Finish the run so it gets sent to the remote. You can discover the run right after that on the dashboard.\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408b6df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mandarin_Pronunciation_Recognition_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
